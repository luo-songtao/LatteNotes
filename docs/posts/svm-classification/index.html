<!DOCTYPE html>













<html lang="en"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Allow having a localized datetime different from the appearance language -->
  

  

    

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="SVM for classification" />
<meta name="author" content="luo-songtao" />
<meta property="og:locale" content="en" />
<meta name="description" content="SVM for classification Linearly Separable case The Largest Margin Principle In the two-classclassification problem using linear models of the form [\begin{aligned} y(\boldsymbol{x}) = \boldsymbol{w}^T\phi(\boldsymbol{x}) + b \end{aligned}] where $\phi(\boldsymbol{x})$ denotes a fixed feature-space transformation, and we have made the bias parameter b explicit. We shall assume for the moment that the training data set is linearly separable in feature space, so that by definition there exists at least one choice of the parameters $\boldsymbol{w}$ and $b$ such that this function satisfies $y(\boldsymbol{x}_n)&gt; 0$ for points having $t_n = +1$ and $y(\boldsymbol{x}_n) &lt; 0$ for points having $t_n = -1$, so that $t_ny(\boldsymbol{x}_n) &gt; 0$ for all training data points. Margin: The support vector machine approaches this problem through the concept of the margin, which is defined to be the smallest distance between the decision boundary and any of the samples. Maximum Margin: In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. The distance of a point xn to the decision surface is given by [\begin{aligned} d = \frac {t_n y(\boldsymbol{x}_n)}{|\boldsymbol{w}|} = \frac {t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b)}{|\boldsymbol{w}|} \end{aligned}] Thus the maximum margin solution is found by solving [\begin{aligned} \arg \max_{\boldsymbol{w}, b} \left { \frac 1{|\boldsymbol{w}|} \min_n[t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b)] \right} \end{aligned}] We can see, that if let $\boldsymbol{w}=k\boldsymbol{w},b=kb$, then the distance $d$ is unchanged. So we can use this freedom to set: \[\begin{aligned} t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) = 1 \end{aligned}\] for the point that is closest to the surface. And then, all data points will statisfy the constraints \[\begin{aligned} t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1 \end{aligned}\] The optimization problem then simply requires that we maximize $|\boldsymbol{w}|^{-1}$, which is equivalent to minimizing $|\boldsymbol{w}|^2$, and so we have to solve the optimization problem [\begin{aligned} \arg \min_{\boldsymbol{w}, b}&amp; \qquad \frac 12 |\boldsymbol{w}|^2 \ \text{S.t.}&amp; \qquad t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1 \end{aligned}] 应用拉格朗日乘子法定义拉格朗日函数： [\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{u}) = -\frac 12 |\boldsymbol{w}|^2 - \sum_{n=1}^N u_n [t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) - 1] \end{aligned}] Note: 这是一个二次规划问题 Setting the derivatives of $L$ with respect to $\boldsymbol{w}, b$ equal to zero, we obtain the following two conditions: [\begin{aligned} \boldsymbol{w} &amp;= \sum_{n=1}^N a_nt_n\phi(\boldsymbol{x}n) \ 0 &amp;= \sum{n=1}^N a_n t_n \end{aligned}] Dual Representation and Kernel Function Eliminating $\boldsymbol{w}, b$ from $L(\boldsymbol{w}, b, \boldsymbol{a})$ 得到 Dual Representaion of the Maximum margin problem in which we maximize: \[\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = \sum_{n=1}^N a_n - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}_n, \boldsymbol{x}_m) \\ \text{S.t.}&amp;\qquad a_n \ge 0 \\ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}\] Here $k(\boldsymbol{x}_n, \boldsymbol{x}_m) = \phi(\boldsymbol{x}_n)^T\phi(\boldsymbol{x}_m)$ Classify new data points: \[\begin{aligned} y(\boldsymbol{x}) = \sum_{n=1}^N a_n t_n k(\boldsymbol{x}, \boldsymbol{x}_n) + b \end{aligned}\] This constrained optimization problem statisfies the Karush-Kuhn-Tucker (KKT) conditions, that is : [\begin{aligned} a_n &amp;\ge 0 \ t_ny(\boldsymbol{x}_n) - 1 &amp; \ge 0 \ a_n{t_ny(\boldsymbol{x}_n) - 1} &amp;= 0 \end{aligned}] Thus for every data point, either $a_n=0$ or $t_ny(\boldsymbol{x}_n) = 1$ Support Vectors: satisfy $t_ny(\boldsymbol{x}_n) = 1$ 解决上述的二次规划问题后，得到$\hat{\boldsymbol{a}}$, 那么对于所有的$a_n \gt 0$的项及对应的$x_n$将构成支持向量集合$\mathcal{S}$ 根据支持向量集合$\mathcal{S}$计算$b$: using $t_n^2 = 1$ [\begin{aligned} t_n\left(\sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) +b \right) &amp;= 1 \ t_n^2\left(\sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) +b \right) &amp;= t_n \end{aligned}] we can obtain [\begin{aligned} b = t_n - \sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) \end{aligned}] a numerically more stable solution is obtained by averaging: [\begin{aligned} b = \frac 1{N_{\mathcal{S}}} \sum_{m\in \mathcal{S}} \left(t_n - \sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) \right)\end{aligned}] Error functoin view For later comparison with alternative models, we can express the maximummargin classifier in terms of the minimization of an error function, with a simple quadratic regularizer, in the form [\begin{aligned} \sum_{n=1}^N E_{\infty}(y(\boldsymbol{x_n})t_n - 1) + \lambda |\boldsymbol{w}|^2 \end{aligned}] $E_{\infty}(z)$ is a function that is zero if $z\ge 0$ else $\infty$ and ensures that $t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1$ are statisfied. Nonlinear Separable case Slack Variables Introduce slack variables $\xi_n \ge 0$ $\xi = 0$: points are on or inside the correct margin boundary $\xi \in (0,1)$: between correct margin boundary and decision boundary $\xi = 1$: on the decision boundary $\xi &gt; 1$: points are missclassified New Classification Constraints: Soft Margin Constraints [\begin{aligned} t_n y(\boldsymbol{x}_n) \ge 1 - \xi_n \end{aligned}] Note: contrast to Hard Margin Constraints $t_n y(\boldsymbol{x}_n) \ge 1$ Original Problem Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary. [\begin{aligned} \min&amp;\qquad C\sum_{n=1}^N \xi_n + \frac 12 |\boldsymbol{w}|^2 \ \text{S.t.} &amp; \qquad t_n y(\boldsymbol{x}_n) \ge 1 - \xi_n \ &amp;\qquad C &gt; 0 \ &amp; \qquad \xi_n \ge 0 \end{aligned}] where the parameter $C &gt; 0$ controls the trade-off between the slack variable penalty and the margin. Because any point that is misclassified has $\xi_n&gt; 1$ , it follows that $\sum_n \xi_n$ is an upper bound on the number of misclassified points. The parameter $C$ is therefore analogous to (the inverse of) a regularization coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit $C \rightarrow \infty$, we will recover the earlier support vector machine for separable data. Lagrangian function: [\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{a}) = \frac 12 |\boldsymbol{w}|^2 + C\sum_{n=1}^N \xi_n -\sum_{n=1}^N a_n(t_n y(\boldsymbol{x}_n) - 1 + \xi_n) -\sum+{n=1}^n \mu_n \xi_n \end{aligned}] With KKT conditions: \[\begin{aligned} a_n &amp;\ge 0 \\ t_n y(\boldsymbol{x}_n) - 1 + \xi_n &amp;\ge 0 \\ a_n(t_n y(\boldsymbol{x}_n) - 1 + \xi_n) &amp;= 0 \\ \mu_n &amp;\ge 0\\ \xi_n &amp;\ge 0\\ \mu_n\xi_n &amp;= 0 \end{aligned}\] optimize out $\boldsymbol{w}$, $b$, and ${\xi_n}$ \[\begin{aligned} \frac {\partial L}{\partial \boldsymbol{w}} = 0 &amp;\Rightarrow \boldsymbol{w} = \sum_{n=1}^N a_nt_n\phi_n \\ \frac {\partial L}{\partial b} = 0 &amp;\Rightarrow \sum_{n=1}^N a_nt_n = 0 \\ \frac {\partial L}{\partial \xi_n} = 0 &amp;\Rightarrow a_n = C-\mu_n \\ &amp;\Rightarrow a_n \le C \end{aligned}\] Dual Problem eliminate $\boldsymbol{w}$, $b$, and ${\xi_n}$ from $L$ [\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = \sum_{n=1}^N a_n - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}n, \boldsymbol{x}_m) \ \text{S.t.}&amp;\qquad 0 \le a_n \le C \ &amp;\qquad \sum{n=1}^N a_nt_n = 0 \end{aligned}] WOW, which is identical to the separable case, except that $a_n$ has a box constraints as $0 \le a_n \le C$. And also, 同样是一个二次规划问题 Support Vectors: 在所有的$a_n&gt;0$中，判断： $a_n &lt; C \Longrightarrow \mu&gt;0 \Longrightarrow \xi_n=0$: on the margin boundary $a_n = C \Longrightarrow \mu=0 \Longrightarrow \xi_n &gt; 0$: $\xi \le 1$: correctly clssified $\xi_n &gt; 1$: misclassified 换句话说，我只对$0&lt; a_n &lt; C$所对应的$\boldsymbol{x}_n$感兴趣…… 所有总结如下： 先求对偶问题-二次规划解，得到$\hat{\boldsymbol{a}}$ 对满足$0&lt; a_n &lt; C$条件的，用来计算$\boldsymbol{w}，b$，和前面一样 其中同样用核函数，来表示$\boldsymbol{w}$所对应的结果，也和前面一样 与数据线性可分情况相比，唯一区别在于，限制了$a_n$的最大值，从而引入误分类点和边界内的点对决策边界的影响。如果$C$非常大，那么将等于没有任何影响，等价于解决数据线性可分情况，换句话说，此时忽略了所有误分类点和边界内的点。 $\nu$-SVM [\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}n, \boldsymbol{x}_m) \ \text{S.t.}&amp;\qquad 0 \le a_n \le \frac 1N \ &amp;\qquad \sum{n=1}^N a_n \ge \nu \ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}] This approach has the advantage that the parameter $\nu$, which replaces $C$, can be interpreted as both an upper bound on the fraction of margin errors(points for which $\xi_n &gt; 0$ and hence which lie on the wrong side of the margin boundary and which may or may not be misclassified) and a lower bound on the fraction of support vectors" />
<meta property="og:description" content="SVM for classification Linearly Separable case The Largest Margin Principle In the two-classclassification problem using linear models of the form [\begin{aligned} y(\boldsymbol{x}) = \boldsymbol{w}^T\phi(\boldsymbol{x}) + b \end{aligned}] where $\phi(\boldsymbol{x})$ denotes a fixed feature-space transformation, and we have made the bias parameter b explicit. We shall assume for the moment that the training data set is linearly separable in feature space, so that by definition there exists at least one choice of the parameters $\boldsymbol{w}$ and $b$ such that this function satisfies $y(\boldsymbol{x}_n)&gt; 0$ for points having $t_n = +1$ and $y(\boldsymbol{x}_n) &lt; 0$ for points having $t_n = -1$, so that $t_ny(\boldsymbol{x}_n) &gt; 0$ for all training data points. Margin: The support vector machine approaches this problem through the concept of the margin, which is defined to be the smallest distance between the decision boundary and any of the samples. Maximum Margin: In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. The distance of a point xn to the decision surface is given by [\begin{aligned} d = \frac {t_n y(\boldsymbol{x}_n)}{|\boldsymbol{w}|} = \frac {t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b)}{|\boldsymbol{w}|} \end{aligned}] Thus the maximum margin solution is found by solving [\begin{aligned} \arg \max_{\boldsymbol{w}, b} \left { \frac 1{|\boldsymbol{w}|} \min_n[t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b)] \right} \end{aligned}] We can see, that if let $\boldsymbol{w}=k\boldsymbol{w},b=kb$, then the distance $d$ is unchanged. So we can use this freedom to set: \[\begin{aligned} t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) = 1 \end{aligned}\] for the point that is closest to the surface. And then, all data points will statisfy the constraints \[\begin{aligned} t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1 \end{aligned}\] The optimization problem then simply requires that we maximize $|\boldsymbol{w}|^{-1}$, which is equivalent to minimizing $|\boldsymbol{w}|^2$, and so we have to solve the optimization problem [\begin{aligned} \arg \min_{\boldsymbol{w}, b}&amp; \qquad \frac 12 |\boldsymbol{w}|^2 \ \text{S.t.}&amp; \qquad t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1 \end{aligned}] 应用拉格朗日乘子法定义拉格朗日函数： [\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{u}) = -\frac 12 |\boldsymbol{w}|^2 - \sum_{n=1}^N u_n [t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) - 1] \end{aligned}] Note: 这是一个二次规划问题 Setting the derivatives of $L$ with respect to $\boldsymbol{w}, b$ equal to zero, we obtain the following two conditions: [\begin{aligned} \boldsymbol{w} &amp;= \sum_{n=1}^N a_nt_n\phi(\boldsymbol{x}n) \ 0 &amp;= \sum{n=1}^N a_n t_n \end{aligned}] Dual Representation and Kernel Function Eliminating $\boldsymbol{w}, b$ from $L(\boldsymbol{w}, b, \boldsymbol{a})$ 得到 Dual Representaion of the Maximum margin problem in which we maximize: \[\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = \sum_{n=1}^N a_n - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}_n, \boldsymbol{x}_m) \\ \text{S.t.}&amp;\qquad a_n \ge 0 \\ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}\] Here $k(\boldsymbol{x}_n, \boldsymbol{x}_m) = \phi(\boldsymbol{x}_n)^T\phi(\boldsymbol{x}_m)$ Classify new data points: \[\begin{aligned} y(\boldsymbol{x}) = \sum_{n=1}^N a_n t_n k(\boldsymbol{x}, \boldsymbol{x}_n) + b \end{aligned}\] This constrained optimization problem statisfies the Karush-Kuhn-Tucker (KKT) conditions, that is : [\begin{aligned} a_n &amp;\ge 0 \ t_ny(\boldsymbol{x}_n) - 1 &amp; \ge 0 \ a_n{t_ny(\boldsymbol{x}_n) - 1} &amp;= 0 \end{aligned}] Thus for every data point, either $a_n=0$ or $t_ny(\boldsymbol{x}_n) = 1$ Support Vectors: satisfy $t_ny(\boldsymbol{x}_n) = 1$ 解决上述的二次规划问题后，得到$\hat{\boldsymbol{a}}$, 那么对于所有的$a_n \gt 0$的项及对应的$x_n$将构成支持向量集合$\mathcal{S}$ 根据支持向量集合$\mathcal{S}$计算$b$: using $t_n^2 = 1$ [\begin{aligned} t_n\left(\sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) +b \right) &amp;= 1 \ t_n^2\left(\sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) +b \right) &amp;= t_n \end{aligned}] we can obtain [\begin{aligned} b = t_n - \sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) \end{aligned}] a numerically more stable solution is obtained by averaging: [\begin{aligned} b = \frac 1{N_{\mathcal{S}}} \sum_{m\in \mathcal{S}} \left(t_n - \sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) \right)\end{aligned}] Error functoin view For later comparison with alternative models, we can express the maximummargin classifier in terms of the minimization of an error function, with a simple quadratic regularizer, in the form [\begin{aligned} \sum_{n=1}^N E_{\infty}(y(\boldsymbol{x_n})t_n - 1) + \lambda |\boldsymbol{w}|^2 \end{aligned}] $E_{\infty}(z)$ is a function that is zero if $z\ge 0$ else $\infty$ and ensures that $t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1$ are statisfied. Nonlinear Separable case Slack Variables Introduce slack variables $\xi_n \ge 0$ $\xi = 0$: points are on or inside the correct margin boundary $\xi \in (0,1)$: between correct margin boundary and decision boundary $\xi = 1$: on the decision boundary $\xi &gt; 1$: points are missclassified New Classification Constraints: Soft Margin Constraints [\begin{aligned} t_n y(\boldsymbol{x}_n) \ge 1 - \xi_n \end{aligned}] Note: contrast to Hard Margin Constraints $t_n y(\boldsymbol{x}_n) \ge 1$ Original Problem Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary. [\begin{aligned} \min&amp;\qquad C\sum_{n=1}^N \xi_n + \frac 12 |\boldsymbol{w}|^2 \ \text{S.t.} &amp; \qquad t_n y(\boldsymbol{x}_n) \ge 1 - \xi_n \ &amp;\qquad C &gt; 0 \ &amp; \qquad \xi_n \ge 0 \end{aligned}] where the parameter $C &gt; 0$ controls the trade-off between the slack variable penalty and the margin. Because any point that is misclassified has $\xi_n&gt; 1$ , it follows that $\sum_n \xi_n$ is an upper bound on the number of misclassified points. The parameter $C$ is therefore analogous to (the inverse of) a regularization coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit $C \rightarrow \infty$, we will recover the earlier support vector machine for separable data. Lagrangian function: [\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{a}) = \frac 12 |\boldsymbol{w}|^2 + C\sum_{n=1}^N \xi_n -\sum_{n=1}^N a_n(t_n y(\boldsymbol{x}_n) - 1 + \xi_n) -\sum+{n=1}^n \mu_n \xi_n \end{aligned}] With KKT conditions: \[\begin{aligned} a_n &amp;\ge 0 \\ t_n y(\boldsymbol{x}_n) - 1 + \xi_n &amp;\ge 0 \\ a_n(t_n y(\boldsymbol{x}_n) - 1 + \xi_n) &amp;= 0 \\ \mu_n &amp;\ge 0\\ \xi_n &amp;\ge 0\\ \mu_n\xi_n &amp;= 0 \end{aligned}\] optimize out $\boldsymbol{w}$, $b$, and ${\xi_n}$ \[\begin{aligned} \frac {\partial L}{\partial \boldsymbol{w}} = 0 &amp;\Rightarrow \boldsymbol{w} = \sum_{n=1}^N a_nt_n\phi_n \\ \frac {\partial L}{\partial b} = 0 &amp;\Rightarrow \sum_{n=1}^N a_nt_n = 0 \\ \frac {\partial L}{\partial \xi_n} = 0 &amp;\Rightarrow a_n = C-\mu_n \\ &amp;\Rightarrow a_n \le C \end{aligned}\] Dual Problem eliminate $\boldsymbol{w}$, $b$, and ${\xi_n}$ from $L$ [\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = \sum_{n=1}^N a_n - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}n, \boldsymbol{x}_m) \ \text{S.t.}&amp;\qquad 0 \le a_n \le C \ &amp;\qquad \sum{n=1}^N a_nt_n = 0 \end{aligned}] WOW, which is identical to the separable case, except that $a_n$ has a box constraints as $0 \le a_n \le C$. And also, 同样是一个二次规划问题 Support Vectors: 在所有的$a_n&gt;0$中，判断： $a_n &lt; C \Longrightarrow \mu&gt;0 \Longrightarrow \xi_n=0$: on the margin boundary $a_n = C \Longrightarrow \mu=0 \Longrightarrow \xi_n &gt; 0$: $\xi \le 1$: correctly clssified $\xi_n &gt; 1$: misclassified 换句话说，我只对$0&lt; a_n &lt; C$所对应的$\boldsymbol{x}_n$感兴趣…… 所有总结如下： 先求对偶问题-二次规划解，得到$\hat{\boldsymbol{a}}$ 对满足$0&lt; a_n &lt; C$条件的，用来计算$\boldsymbol{w}，b$，和前面一样 其中同样用核函数，来表示$\boldsymbol{w}$所对应的结果，也和前面一样 与数据线性可分情况相比，唯一区别在于，限制了$a_n$的最大值，从而引入误分类点和边界内的点对决策边界的影响。如果$C$非常大，那么将等于没有任何影响，等价于解决数据线性可分情况，换句话说，此时忽略了所有误分类点和边界内的点。 $\nu$-SVM [\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}n, \boldsymbol{x}_m) \ \text{S.t.}&amp;\qquad 0 \le a_n \le \frac 1N \ &amp;\qquad \sum{n=1}^N a_n \ge \nu \ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}] This approach has the advantage that the parameter $\nu$, which replaces $C$, can be interpreted as both an upper bound on the fraction of margin errors(points for which $\xi_n &gt; 0$ and hence which lie on the wrong side of the margin boundary and which may or may not be misclassified) and a lower bound on the fraction of support vectors" />
<link rel="canonical" href="http://0.0.0.0:4000/posts/svm-classification/" />
<meta property="og:url" content="http://0.0.0.0:4000/posts/svm-classification/" />
<meta property="og:site_name" content="Candy Note" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-20T23:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="SVM for classification" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@luo-songtao" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"luo-songtao"},"dateModified":"2022-02-20T23:00:00+00:00","datePublished":"2022-02-20T23:00:00+00:00","description":"SVM for classification Linearly Separable case The Largest Margin Principle In the two-classclassification problem using linear models of the form [\\begin{aligned} y(\\boldsymbol{x}) = \\boldsymbol{w}^T\\phi(\\boldsymbol{x}) + b \\end{aligned}] where $\\phi(\\boldsymbol{x})$ denotes a fixed feature-space transformation, and we have made the bias parameter b explicit. We shall assume for the moment that the training data set is linearly separable in feature space, so that by definition there exists at least one choice of the parameters $\\boldsymbol{w}$ and $b$ such that this function satisfies $y(\\boldsymbol{x}_n)&gt; 0$ for points having $t_n = +1$ and $y(\\boldsymbol{x}_n) &lt; 0$ for points having $t_n = -1$, so that $t_ny(\\boldsymbol{x}_n) &gt; 0$ for all training data points. Margin: The support vector machine approaches this problem through the concept of the margin, which is defined to be the smallest distance between the decision boundary and any of the samples. Maximum Margin: In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. The distance of a point xn to the decision surface is given by [\\begin{aligned} d = \\frac {t_n y(\\boldsymbol{x}_n)}{|\\boldsymbol{w}|} = \\frac {t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b)}{|\\boldsymbol{w}|} \\end{aligned}] Thus the maximum margin solution is found by solving [\\begin{aligned} \\arg \\max_{\\boldsymbol{w}, b} \\left { \\frac 1{|\\boldsymbol{w}|} \\min_n[t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b)] \\right} \\end{aligned}] We can see, that if let $\\boldsymbol{w}=k\\boldsymbol{w},b=kb$, then the distance $d$ is unchanged. So we can use this freedom to set: \\[\\begin{aligned} t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b) = 1 \\end{aligned}\\] for the point that is closest to the surface. And then, all data points will statisfy the constraints \\[\\begin{aligned} t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b) \\ge 1 \\end{aligned}\\] The optimization problem then simply requires that we maximize $|\\boldsymbol{w}|^{-1}$, which is equivalent to minimizing $|\\boldsymbol{w}|^2$, and so we have to solve the optimization problem [\\begin{aligned} \\arg \\min_{\\boldsymbol{w}, b}&amp; \\qquad \\frac 12 |\\boldsymbol{w}|^2 \\ \\text{S.t.}&amp; \\qquad t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b) \\ge 1 \\end{aligned}] 应用拉格朗日乘子法定义拉格朗日函数： [\\begin{aligned} L(\\boldsymbol{w}, b, \\boldsymbol{u}) = -\\frac 12 |\\boldsymbol{w}|^2 - \\sum_{n=1}^N u_n [t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b) - 1] \\end{aligned}] Note: 这是一个二次规划问题 Setting the derivatives of $L$ with respect to $\\boldsymbol{w}, b$ equal to zero, we obtain the following two conditions: [\\begin{aligned} \\boldsymbol{w} &amp;= \\sum_{n=1}^N a_nt_n\\phi(\\boldsymbol{x}n) \\ 0 &amp;= \\sum{n=1}^N a_n t_n \\end{aligned}] Dual Representation and Kernel Function Eliminating $\\boldsymbol{w}, b$ from $L(\\boldsymbol{w}, b, \\boldsymbol{a})$ 得到 Dual Representaion of the Maximum margin problem in which we maximize: \\[\\begin{aligned} \\max&amp; \\qquad\\tilde{L}(\\boldsymbol{a}) = \\sum_{n=1}^N a_n - \\frac 12 \\sum_{n=1}^N\\sum_{m=1}^N a_na_mt_nt_m k(\\boldsymbol{x}_n, \\boldsymbol{x}_m) \\\\ \\text{S.t.}&amp;\\qquad a_n \\ge 0 \\\\ &amp;\\qquad \\sum_{n=1}^N a_nt_n = 0 \\end{aligned}\\] Here $k(\\boldsymbol{x}_n, \\boldsymbol{x}_m) = \\phi(\\boldsymbol{x}_n)^T\\phi(\\boldsymbol{x}_m)$ Classify new data points: \\[\\begin{aligned} y(\\boldsymbol{x}) = \\sum_{n=1}^N a_n t_n k(\\boldsymbol{x}, \\boldsymbol{x}_n) + b \\end{aligned}\\] This constrained optimization problem statisfies the Karush-Kuhn-Tucker (KKT) conditions, that is : [\\begin{aligned} a_n &amp;\\ge 0 \\ t_ny(\\boldsymbol{x}_n) - 1 &amp; \\ge 0 \\ a_n{t_ny(\\boldsymbol{x}_n) - 1} &amp;= 0 \\end{aligned}] Thus for every data point, either $a_n=0$ or $t_ny(\\boldsymbol{x}_n) = 1$ Support Vectors: satisfy $t_ny(\\boldsymbol{x}_n) = 1$ 解决上述的二次规划问题后，得到$\\hat{\\boldsymbol{a}}$, 那么对于所有的$a_n \\gt 0$的项及对应的$x_n$将构成支持向量集合$\\mathcal{S}$ 根据支持向量集合$\\mathcal{S}$计算$b$: using $t_n^2 = 1$ [\\begin{aligned} t_n\\left(\\sum_{m\\in \\mathcal{S}} a_m t_m k(x_n, x_m) +b \\right) &amp;= 1 \\ t_n^2\\left(\\sum_{m\\in \\mathcal{S}} a_m t_m k(x_n, x_m) +b \\right) &amp;= t_n \\end{aligned}] we can obtain [\\begin{aligned} b = t_n - \\sum_{m\\in \\mathcal{S}} a_m t_m k(x_n, x_m) \\end{aligned}] a numerically more stable solution is obtained by averaging: [\\begin{aligned} b = \\frac 1{N_{\\mathcal{S}}} \\sum_{m\\in \\mathcal{S}} \\left(t_n - \\sum_{m\\in \\mathcal{S}} a_m t_m k(x_n, x_m) \\right)\\end{aligned}] Error functoin view For later comparison with alternative models, we can express the maximummargin classifier in terms of the minimization of an error function, with a simple quadratic regularizer, in the form [\\begin{aligned} \\sum_{n=1}^N E_{\\infty}(y(\\boldsymbol{x_n})t_n - 1) + \\lambda |\\boldsymbol{w}|^2 \\end{aligned}] $E_{\\infty}(z)$ is a function that is zero if $z\\ge 0$ else $\\infty$ and ensures that $t_n(\\boldsymbol{w}^T\\phi(\\boldsymbol{x}_n) + b) \\ge 1$ are statisfied. Nonlinear Separable case Slack Variables Introduce slack variables $\\xi_n \\ge 0$ $\\xi = 0$: points are on or inside the correct margin boundary $\\xi \\in (0,1)$: between correct margin boundary and decision boundary $\\xi = 1$: on the decision boundary $\\xi &gt; 1$: points are missclassified New Classification Constraints: Soft Margin Constraints [\\begin{aligned} t_n y(\\boldsymbol{x}_n) \\ge 1 - \\xi_n \\end{aligned}] Note: contrast to Hard Margin Constraints $t_n y(\\boldsymbol{x}_n) \\ge 1$ Original Problem Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary. [\\begin{aligned} \\min&amp;\\qquad C\\sum_{n=1}^N \\xi_n + \\frac 12 |\\boldsymbol{w}|^2 \\ \\text{S.t.} &amp; \\qquad t_n y(\\boldsymbol{x}_n) \\ge 1 - \\xi_n \\ &amp;\\qquad C &gt; 0 \\ &amp; \\qquad \\xi_n \\ge 0 \\end{aligned}] where the parameter $C &gt; 0$ controls the trade-off between the slack variable penalty and the margin. Because any point that is misclassified has $\\xi_n&gt; 1$ , it follows that $\\sum_n \\xi_n$ is an upper bound on the number of misclassified points. The parameter $C$ is therefore analogous to (the inverse of) a regularization coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit $C \\rightarrow \\infty$, we will recover the earlier support vector machine for separable data. Lagrangian function: [\\begin{aligned} L(\\boldsymbol{w}, b, \\boldsymbol{a}) = \\frac 12 |\\boldsymbol{w}|^2 + C\\sum_{n=1}^N \\xi_n -\\sum_{n=1}^N a_n(t_n y(\\boldsymbol{x}_n) - 1 + \\xi_n) -\\sum+{n=1}^n \\mu_n \\xi_n \\end{aligned}] With KKT conditions: \\[\\begin{aligned} a_n &amp;\\ge 0 \\\\ t_n y(\\boldsymbol{x}_n) - 1 + \\xi_n &amp;\\ge 0 \\\\ a_n(t_n y(\\boldsymbol{x}_n) - 1 + \\xi_n) &amp;= 0 \\\\ \\mu_n &amp;\\ge 0\\\\ \\xi_n &amp;\\ge 0\\\\ \\mu_n\\xi_n &amp;= 0 \\end{aligned}\\] optimize out $\\boldsymbol{w}$, $b$, and ${\\xi_n}$ \\[\\begin{aligned} \\frac {\\partial L}{\\partial \\boldsymbol{w}} = 0 &amp;\\Rightarrow \\boldsymbol{w} = \\sum_{n=1}^N a_nt_n\\phi_n \\\\ \\frac {\\partial L}{\\partial b} = 0 &amp;\\Rightarrow \\sum_{n=1}^N a_nt_n = 0 \\\\ \\frac {\\partial L}{\\partial \\xi_n} = 0 &amp;\\Rightarrow a_n = C-\\mu_n \\\\ &amp;\\Rightarrow a_n \\le C \\end{aligned}\\] Dual Problem eliminate $\\boldsymbol{w}$, $b$, and ${\\xi_n}$ from $L$ [\\begin{aligned} \\max&amp; \\qquad\\tilde{L}(\\boldsymbol{a}) = \\sum_{n=1}^N a_n - \\frac 12 \\sum_{n=1}^N\\sum_{m=1}^N a_na_mt_nt_m k(\\boldsymbol{x}n, \\boldsymbol{x}_m) \\ \\text{S.t.}&amp;\\qquad 0 \\le a_n \\le C \\ &amp;\\qquad \\sum{n=1}^N a_nt_n = 0 \\end{aligned}] WOW, which is identical to the separable case, except that $a_n$ has a box constraints as $0 \\le a_n \\le C$. And also, 同样是一个二次规划问题 Support Vectors: 在所有的$a_n&gt;0$中，判断： $a_n &lt; C \\Longrightarrow \\mu&gt;0 \\Longrightarrow \\xi_n=0$: on the margin boundary $a_n = C \\Longrightarrow \\mu=0 \\Longrightarrow \\xi_n &gt; 0$: $\\xi \\le 1$: correctly clssified $\\xi_n &gt; 1$: misclassified 换句话说，我只对$0&lt; a_n &lt; C$所对应的$\\boldsymbol{x}_n$感兴趣…… 所有总结如下： 先求对偶问题-二次规划解，得到$\\hat{\\boldsymbol{a}}$ 对满足$0&lt; a_n &lt; C$条件的，用来计算$\\boldsymbol{w}，b$，和前面一样 其中同样用核函数，来表示$\\boldsymbol{w}$所对应的结果，也和前面一样 与数据线性可分情况相比，唯一区别在于，限制了$a_n$的最大值，从而引入误分类点和边界内的点对决策边界的影响。如果$C$非常大，那么将等于没有任何影响，等价于解决数据线性可分情况，换句话说，此时忽略了所有误分类点和边界内的点。 $\\nu$-SVM [\\begin{aligned} \\max&amp; \\qquad\\tilde{L}(\\boldsymbol{a}) = - \\frac 12 \\sum_{n=1}^N\\sum_{m=1}^N a_na_mt_nt_m k(\\boldsymbol{x}n, \\boldsymbol{x}_m) \\ \\text{S.t.}&amp;\\qquad 0 \\le a_n \\le \\frac 1N \\ &amp;\\qquad \\sum{n=1}^N a_n \\ge \\nu \\ &amp;\\qquad \\sum_{n=1}^N a_nt_n = 0 \\end{aligned}] This approach has the advantage that the parameter $\\nu$, which replaces $C$, can be interpreted as both an upper bound on the fraction of margin errors(points for which $\\xi_n &gt; 0$ and hence which lie on the wrong side of the margin boundary and which may or may not be misclassified) and a lower bound on the fraction of support vectors","headline":"SVM for classification","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/posts/svm-classification/"},"url":"http://0.0.0.0:4000/posts/svm-classification/"}</script>
<!-- End Jekyll SEO tag -->


  <title>SVM for classification | Candy Note
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Candy Note">
<meta name="application-name" content="Candy Note">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  

    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  
    <!--
  Switch the mode between dark and light.
-->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get MODE_ATTR() { return "data-mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    static get ID() { return "mode-toggle"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener("change", () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();

      });

    } /* constructor() */

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage({
        direction: ModeToggle.ID,
        message: this.modeStatus
      }, "*");
    }

  } /* ModeToggle */

  const toggle = new ModeToggle();

  function flipMode() {
    if (toggle.hasMode) {
      if (toggle.isSysDarkPrefer) {
        if (toggle.isLightMode) {
          toggle.clearMode();
        } else {
          toggle.setLight();
        }

      } else {
        if (toggle.isDarkMode) {
          toggle.clearMode();
        } else {
          toggle.setDark();
        }
      }

    } else {
      if (toggle.isSysDarkPrefer) {
        toggle.setLight();
      } else {
        toggle.setDark();
      }
    }

    toggle.notify();

  } /* flipMode() */

</script>

  

  <body data-spy="scroll" data-target="#toc" data-topbar-visible="true">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
          
          <img src="
            
              /assets/img/head.jpg
            
          " alt="avatar" onerror="this.style.display='none'">
        
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Candy Note</a>
    </div>
    <div class="site-subtitle font-italic">Personal Notes</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>CATEGORIES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>TAGS</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ARCHIVES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center">

    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
      <a href="https://github.com/github_username" aria-label="github"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/twitter_username" aria-label="twitter"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['ryomawithlst','gmail.com'].join('@')" aria-label="email"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          <span>
            <a href="/">
              Home
            </a>
          </span>

        

      

        

      

        

          
            <span>SVM for classification</span>
          

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        









<div class="row">

  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-8">
    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    

    
      
      
        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->



<!-- images -->





<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  




<!-- Wrap prompt element of blockquote with the <div> tag -->







<!-- return -->







<h1 data-toc-skip>SVM for classification</h1>

<div class="post-meta text-muted">

  <!-- author -->
  <div>
    
    

    

    By
    <em>
      
        <a href="https://github.com/luo-songtao">luo-songtao</a>
      
    </em>
  </div>

  <div class="d-flex">
    <div>
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago"
    data-ts="1645398000"
    
      data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll"
    
    >
  2022-02-20
</em>

      </span>

      <!-- lastmod date -->
      

      <!-- read time -->
      <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom"
  title="1401 words">
  <em>7 min</em> read</span>


      <!-- page views -->
      
    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <h2 id="svm-for-classification"><span class="mr-2">SVM for classification</span><a href="#svm-for-classification" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2>

<h3 id="linearly-separable-case"><span class="mr-2">Linearly Separable case</span><a href="#linearly-separable-case" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3>

<h4 id="the-largest-margin-principle"><span class="mr-2">The Largest Margin Principle</span><a href="#the-largest-margin-principle" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<p>In the two-classclassification problem using linear models of the form</p>

\[\begin{aligned} y(\boldsymbol{x}) = \boldsymbol{w}^T\phi(\boldsymbol{x}) + b \end{aligned}\]

<p>where $\phi(\boldsymbol{x})$ denotes a fixed feature-space transformation, and we have made the bias parameter b explicit.</p>

<p>We shall assume for the moment that the training data set is linearly separable in feature space, so that by definition there exists at least one choice of the parameters $\boldsymbol{w}$ and $b$ such that this function satisfies $y(\boldsymbol{x}_n)&gt; 0$ for points having $t_n = +1$ and $y(\boldsymbol{x}_n) &lt; 0$ for points having $t_n = -1$, so that $t_ny(\boldsymbol{x}_n) &gt; 0$ for all training data points.</p>

<ul>
  <li>Margin: The support vector machine approaches this problem through the concept of the margin, which is defined to be the smallest distance between the decision boundary and any of the samples.</li>
  <li>Maximum Margin: In support vector machines the decision boundary is chosen to be the one for
which the margin is maximized.</li>
</ul>

<p>The distance of a point xn to the decision surface is given by</p>

\[\begin{aligned} d = \frac {t_n y(\boldsymbol{x}_n)}{\|\boldsymbol{w}\|} = \frac {t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b)}{\|\boldsymbol{w}\|} \end{aligned}\]

<p>Thus the maximum margin solution is found by solving</p>

\[\begin{aligned} \arg \max_{\boldsymbol{w}, b} \left \{ \frac 1{\|\boldsymbol{w}\|} \min_n[t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b)] \right\} \end{aligned}\]

<ul>
  <li>
    <p>We can see, that if let $\boldsymbol{w}=k\boldsymbol{w},b=kb$, then the distance $d$ is unchanged. So we can use this freedom to set:</p>

\[\begin{aligned} t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) = 1 \end{aligned}\]

    <p>for the point that is closest to the surface.</p>

    <ul>
      <li>
        <p>And then, all data points will statisfy the constraints</p>

\[\begin{aligned} t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1 \end{aligned}\]
      </li>
    </ul>
  </li>
</ul>

<p>The optimization problem then simply requires that we maximize $|\boldsymbol{w}|^{-1}$, which is equivalent to minimizing $|\boldsymbol{w}|^2$, and so we have to solve the optimization problem</p>

\[\begin{aligned} \arg \min_{\boldsymbol{w}, b}&amp; \qquad \frac 12 \|\boldsymbol{w}\|^2 \\ \text{S.t.}&amp; \qquad t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1 \end{aligned}\]

<p>应用拉格朗日乘子法定义拉格朗日函数：</p>

\[\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{u}) = -\frac 12 \|\boldsymbol{w}\|^2 - \sum_{n=1}^N u_n [t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) - 1]  \end{aligned}\]

<ul>
  <li>Note: 这是一个二次规划问题</li>
</ul>

<p>Setting the derivatives of $L$ with respect to $\boldsymbol{w}, b$ equal to zero, we obtain the following two conditions:</p>

\[\begin{aligned} \boldsymbol{w} &amp;= \sum_{n=1}^N a_nt_n\phi(\boldsymbol{x}_n) \\ 0 &amp;= \sum_{n=1}^N a_n t_n \end{aligned}\]

<p>Dual Representation and Kernel Function</p>

<ul>
  <li>Eliminating $\boldsymbol{w}, b$ from $L(\boldsymbol{w}, b, \boldsymbol{a})$</li>
  <li>
    <p>得到 Dual Representaion of the Maximum margin problem in which we maximize:</p>

\[\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = \sum_{n=1}^N a_n - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}_n, \boldsymbol{x}_m) \\ \text{S.t.}&amp;\qquad a_n \ge 0 \\ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}\]

    <ul>
      <li>Here $k(\boldsymbol{x}_n, \boldsymbol{x}_m) = \phi(\boldsymbol{x}_n)^T\phi(\boldsymbol{x}_m)$</li>
    </ul>
  </li>
  <li>
    <p>Classify new data points:</p>

\[\begin{aligned} y(\boldsymbol{x}) = \sum_{n=1}^N a_n t_n k(\boldsymbol{x}, \boldsymbol{x}_n) + b \end{aligned}\]
  </li>
</ul>

<p>This constrained optimization problem statisfies the Karush-Kuhn-Tucker (KKT) conditions, that is :</p>

\[\begin{aligned} a_n &amp;\ge 0 \\ t_ny(\boldsymbol{x}_n) - 1 &amp; \ge 0 \\ a_n\{t_ny(\boldsymbol{x}_n) - 1\} &amp;= 0 \end{aligned}\]

<ul>
  <li>
    <p>Thus for every data point, either $a_n=0$ or $t_ny(\boldsymbol{x}_n) = 1$</p>
  </li>
  <li>
    <p>Support Vectors: satisfy $t_ny(\boldsymbol{x}_n) = 1$</p>
  </li>
</ul>

<p>解决上述的二次规划问题后，得到$\hat{\boldsymbol{a}}$, 那么对于所有的$a_n \gt 0$的项及对应的$x_n$将构成支持向量集合$\mathcal{S}$</p>

<p>根据支持向量集合$\mathcal{S}$计算$b$: using $t_n^2 = 1$</p>

\[\begin{aligned} t_n\left(\sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) +b \right) &amp;= 1 \\ t_n^2\left(\sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) +b \right) &amp;= t_n \end{aligned}\]

<ul>
  <li>we can obtain</li>
</ul>

\[\begin{aligned} b = t_n - \sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) \end{aligned}\]

<ul>
  <li>a numerically more stable solution is obtained by averaging:</li>
</ul>

\[\begin{aligned} b = \frac 1{N_{\mathcal{S}}} \sum_{m\in \mathcal{S}} \left(t_n - \sum_{m\in \mathcal{S}} a_m t_m k(x_n, x_m) \right)\end{aligned}\]

<h4 id="error-functoin-view"><span class="mr-2">Error functoin view</span><a href="#error-functoin-view" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<p>For later comparison with alternative models, we can express the maximummargin classifier in terms of the minimization of an error function, with a simple quadratic regularizer, in the form</p>

\[\begin{aligned} \sum_{n=1}^N E_{\infty}(y(\boldsymbol{x_n})t_n - 1) + \lambda \|\boldsymbol{w}\|^2 \end{aligned}\]

<ul>
  <li>$E_{\infty}(z)$ is a function that is zero if $z\ge 0$ else $\infty$ and ensures that $t_n(\boldsymbol{w}^T\phi(\boldsymbol{x}_n) + b) \ge 1$ are statisfied.</li>
</ul>

<h3 id="nonlinear-separable-case"><span class="mr-2">Nonlinear Separable case</span><a href="#nonlinear-separable-case" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3>

<h4 id="slack-variables"><span class="mr-2">Slack Variables</span><a href="#slack-variables" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<p>Introduce slack variables $\xi_n \ge 0$</p>

<ul>
  <li>$\xi = 0$: points are on or inside the correct margin boundary</li>
  <li>$\xi \in (0,1)$: between correct margin boundary and decision boundary</li>
  <li>$\xi = 1$: on the decision boundary</li>
  <li>$\xi &gt; 1$: points are missclassified</li>
</ul>

<p>New Classification Constraints: Soft Margin Constraints</p>

\[\begin{aligned} t_n y(\boldsymbol{x}_n) \ge 1 - \xi_n \end{aligned}\]

<ul>
  <li>Note: contrast to Hard Margin Constraints  $t_n y(\boldsymbol{x}_n) \ge 1$</li>
</ul>

<h4 id="original-problem"><span class="mr-2">Original Problem</span><a href="#original-problem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<p>Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary.</p>

\[\begin{aligned} \min&amp;\qquad  C\sum_{n=1}^N \xi_n  + \frac 12 \|\boldsymbol{w}\|^2 \\ \text{S.t.} &amp; \qquad t_n y(\boldsymbol{x}_n) \ge 1 - \xi_n \\ &amp;\qquad C &gt; 0 \\ &amp; \qquad \xi_n \ge 0 \end{aligned}\]

<p>where the parameter $C &gt; 0$ controls the trade-off between the slack variable penalty and the margin.</p>

<ul>
  <li>Because any point that is misclassified has $\xi_n&gt; 1$ , it follows that $\sum_n \xi_n$ is an upper bound on the number of misclassified points. The parameter $C$ is therefore analogous to (the inverse of) a regularization  coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit $C \rightarrow \infty$, we will recover the earlier support vector machine for separable data.</li>
</ul>

<p>Lagrangian function:</p>

\[\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{a}) = \frac 12 \|\boldsymbol{w}\|^2 + C\sum_{n=1}^N \xi_n -\sum_{n=1}^N a_n(t_n y(\boldsymbol{x}_n) - 1 + \xi_n) -\sum+{n=1}^n \mu_n \xi_n \end{aligned}\]

<ul>
  <li>
    <p>With KKT conditions:</p>

\[\begin{aligned} a_n &amp;\ge 0 \\ t_n y(\boldsymbol{x}_n) - 1 + \xi_n &amp;\ge 0 \\ a_n(t_n y(\boldsymbol{x}_n) - 1 + \xi_n) &amp;= 0 \\ \mu_n &amp;\ge 0\\ \xi_n &amp;\ge 0\\ \mu_n\xi_n &amp;= 0 \end{aligned}\]
  </li>
  <li>
    <p>optimize out $\boldsymbol{w}$, $b$, and ${\xi_n}$</p>

\[\begin{aligned} \frac {\partial L}{\partial \boldsymbol{w}} = 0 &amp;\Rightarrow  \boldsymbol{w} = \sum_{n=1}^N a_nt_n\phi_n \\ \frac {\partial L}{\partial b} = 0 &amp;\Rightarrow \sum_{n=1}^N a_nt_n = 0 \\ \frac {\partial L}{\partial \xi_n} = 0 &amp;\Rightarrow a_n = C-\mu_n \\ &amp;\Rightarrow a_n \le C \end{aligned}\]
  </li>
</ul>

<h4 id="dual-problem"><span class="mr-2">Dual Problem</span><a href="#dual-problem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<p>eliminate $\boldsymbol{w}$, $b$, and ${\xi_n}$ from $L$</p>

\[\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = \sum_{n=1}^N a_n - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}_n, \boldsymbol{x}_m) \\ \text{S.t.}&amp;\qquad 0 \le a_n \le C \\ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}\]

<ul>
  <li>WOW, which is identical to the separable case, except that $a_n$ has a box constraints as $0 \le a_n \le C$.</li>
</ul>

<p>And also, 同样是一个二次规划问题</p>

<p>Support Vectors: 在所有的$a_n&gt;0$中，判断：</p>
<ul>
  <li>$a_n &lt; C \Longrightarrow \mu&gt;0 \Longrightarrow \xi_n=0$: on the margin boundary</li>
  <li>$a_n = C \Longrightarrow \mu=0 \Longrightarrow \xi_n &gt; 0$:
    <ul>
      <li>$\xi \le 1$: correctly clssified</li>
      <li>$\xi_n &gt; 1$: misclassified</li>
    </ul>
  </li>
  <li>换句话说，我只对$0&lt; a_n &lt; C$所对应的$\boldsymbol{x}_n$感兴趣……</li>
</ul>

<p>所有总结如下：</p>
<ul>
  <li>先求对偶问题-二次规划解，得到$\hat{\boldsymbol{a}}$</li>
  <li>对满足$0&lt; a_n &lt; C$条件的，用来计算$\boldsymbol{w}，b$，和前面一样</li>
  <li>其中同样用核函数，来表示$\boldsymbol{w}$所对应的结果，也和前面一样</li>
</ul>

<p>与数据线性可分情况相比，唯一区别在于，限制了$a_n$的最大值，从而引入误分类点和边界内的点对决策边界的影响。如果$C$非常大，那么将等于没有任何影响，等价于解决数据线性可分情况，换句话说，此时忽略了所有误分类点和边界内的点。</p>

<h3 id="nu-svm"><span class="mr-2">$\nu$-SVM</span><a href="#nu-svm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3>

\[\begin{aligned} \max&amp; \qquad\tilde{L}(\boldsymbol{a}) = - \frac 12 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_m k(\boldsymbol{x}_n, \boldsymbol{x}_m) \\ \text{S.t.}&amp;\qquad 0 \le a_n \le \frac 1N \\ &amp;\qquad \sum_{n=1}^N a_n \ge \nu \\ &amp;\qquad \sum_{n=1}^N a_nt_n = 0 \end{aligned}\]

<p>This approach has the advantage that the parameter $\nu$, which replaces $C$, can be interpreted as both an upper bound on the fraction of margin errors(points for which $\xi_n &gt; 0$ and hence which lie on the wrong side of the margin boundary and which may or may not be misclassified) and a lower bound on the fraction of support vectors</p>


</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw mr-1"></i>
    
      <a href='/categories/machine-learning/'>Machine Learning</a>,
      <a href='/categories/svm/'>SVM</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw mr-1"></i>
      
      <a href="/tags/svm/"
          class="post-tag no-text-decoration" >SVM</a>
      
      <a href="/tags/classification/"
          class="post-tag no-text-decoration" >classification</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.

      
    </div>

    <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=SVM for classification - Candy Note&amp;url=http://0.0.0.0:4000/posts/svm-classification/" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=SVM for classification - Candy Note&amp;u=http://0.0.0.0:4000/posts/svm-classification/" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://t.me/share/url?url=http://0.0.0.0:4000/posts/svm-classification/&amp;text=SVM for classification - Candy Note" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i id="copy-link" class="fa-fw fas fa-link small"
        data-toggle="tooltip" data-placement="top"
        title="Copy link"
        data-title-succeed="Link copied successfully!">
    </i>

  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
    

    </div>
  </div> <!-- #core-wrapper -->

  <!-- pannel -->
  <div id="panel-wrapper" class="col-xl-3 pl-2 text-muted">

    <div class="access">
      















      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/gaussian-distribution/">Gaussian Distribution</a>
    
      
      <a class="post-tag" href="/tags/pca/">PCA</a>
    
      
      <a class="post-tag" href="/tags/em/">EM</a>
    
      
      <a class="post-tag" href="/tags/bayesian/">Bayesian</a>
    
      
      <a class="post-tag" href="/tags/markov-chain/">Markov Chain</a>
    
      
      <a class="post-tag" href="/tags/bernoulli/">Bernoulli</a>
    
      
      <a class="post-tag" href="/tags/binomial/">Binomial</a>
    
      
      <a class="post-tag" href="/tags/gamma-distribution/">Gamma Distribution</a>
    
      
      <a class="post-tag" href="/tags/gmm/">GMM</a>
    
      
      <a class="post-tag" href="/tags/kernel/">Kernel</a>
    

    </div>
  </div>


    </div>

    
      
      



<!-- BS-toc.js will be loaded at medium priority -->
<script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>

<div id="toc-wrapper" class="pl-0 pr-4 mb-5">
  <div class="panel-heading pl-3 pt-2 mb-2">Contents</div>
  <nav id="toc" data-toggle="toc"></nav>
</div>


    
  </div>

</div>

<!-- tail -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
      
        
        <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->






  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/posts/svm-regression/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1645398000"
    
    >
  2022-02-20
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>SVM for Regression</h3>
            <div class="text-muted small">
              <p>
                





                SVM for Regression

Using $\epsilon$-insensitive error function

$\epsilon$-insensitive can lead to sparse solutions

\[\begin{aligned} \min &amp;amp;\qquad C\sum_{n=1}^N (\xi_n + \widehat{\xi}_n) + \f...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/HMM/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1645401600"
    
    >
  2022-02-21
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Hidden Markov Model</h3>
            <div class="text-muted small">
              <p>
                





                Hidden Markov Model

Assume that all latent variables form a Markov chain, giving rise to the graphical structure, this is known as a state space model.

If the latent variables are discrete, then ...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/markov_chain/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1645401600"
    
    >
  2022-02-21
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Markov Chain</h3>
            <div class="text-muted small">
              <p>
                





                Markov Chain

A first-Order Markov Chain

\[\begin{aligned} p(z^{(m+1)}\vert z^{(m)}, ...,z^{(1)} )  = p(z^{(m+1)}\vert z^{(m)})\end{aligned}\]

then given

  initial variable: $p(z^{(0)})$
  trans...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->


      
        
        <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/posts/probabilistic_generative_models/" class="btn btn-outline-primary"
    prompt="Older">
    <p>Probabilistic Generative Model</p>
  </a>
  

  
  <a href="/posts/svm-regression/" class="btn btn-outline-primary"
    prompt="Newer">
    <p>SVM for Regression</p>
  </a>
  

</div>

      
        
        <!--  The comments switcher -->


      
    </div>
  </div>
</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center text-muted">
    <div class="footer-left">
      <p class="mb-0">
        © 2022
        <a href="https://twitter.com/username">luo-songtao</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/gaussian-distribution/">Gaussian Distribution</a>
    
      
      <a class="post-tag" href="/tags/pca/">PCA</a>
    
      
      <a class="post-tag" href="/tags/em/">EM</a>
    
      
      <a class="post-tag" href="/tags/bayesian/">Bayesian</a>
    
      
      <a class="post-tag" href="/tags/markov-chain/">Markov Chain</a>
    
      
      <a class="post-tag" href="/tags/bernoulli/">Bernoulli</a>
    
      
      <a class="post-tag" href="/tags/binomial/">Binomial</a>
    
      
      <a class="post-tag" href="/tags/gamma-distribution/">Gamma Distribution</a>
    
      
      <a class="post-tag" href="/tags/gmm/">GMM</a>
    
      
      <a class="post-tag" href="/tags/kernel/">Kernel</a>
    

    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    
      <!--
  mermaid-js loader
-->

<script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script>

<script>
  $(function() {
    function updateMermaid(event) {
      if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {

        const mode = event.data.message;

        if (typeof mermaid === "undefined") {
          return;
        }

        let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    let initTheme = "default";

    if ($("html[data-mode=dark]").length > 0
      || ($("html[data-mode]").length == 0
        && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) {
      initTheme = "dark";
    }

    let mermaidConf = {
      theme: initTheme  /* <default|dark|forest|neutral> */
    };

    /* Markdown converts to HTML */
    $("pre").has("code.language-mermaid").each(function() {
      let svgCode = $(this).children().html();
      $(this).addClass("unloaded");
      $(this).after(`<div class=\"mermaid\">${svgCode}</div>`);
    });

    mermaid.initialize(mermaidConf);

    window.addEventListener("message", updateMermaid);
  });
</script>

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup & clipboard -->
  

  







  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script>







  

  

  







  
    

    

  



  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script>








<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
  /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
  MathJax = {
    tex: {
      inlineMath: [              /* start/end delimiter pairs for in-line math */
        ['$','$'],
        ['\\(','\\)']
      ],
      displayMath: [             /* start/end delimiter pairs for display math */
        ['$$', '$$'],
        ['\\[', '\\]']
      ]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>


<!-- commons -->

<script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script>




  </body>

</html>

