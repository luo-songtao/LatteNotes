<!DOCTYPE html>













<html lang="en"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Allow having a localized datetime different from the appearance language -->
  

  

    

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Beta Distribution" />
<meta name="author" content="luo-songtao" />
<meta property="og:locale" content="en" />
<meta name="description" content="Beta Distribution New we introduce a prior distribution of $p(\mu)$ over the parameter $\mu$ in the Bernouli and Binomial distribution The beta distribution as the prior distribution will have a simple interpretation as well as some useful analytical properties. conjugacy: In this case, if we let the posterior distribution is priportional to the product of the prior and the likelihood function, then it will have the same functional form as the prior. Then we will see the posterior distribution is also a beta distribution. This property is called conjugacy. \(\begin{aligned} \mathrm{Beta}(\mu \vert a, b) = \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \end{aligned}\) where $\Gamma(x)$ is the Gamma function defined by: \[\begin{aligned}\Gamma(x) = \int_0^{\infty} u^{x-1}e^{-u}du \end{aligned}\] $\Gamma(x+1) = x\Gamma(x)$, and then $\Gamma(x+1) = x!$ when $x$ is an integer. $$\begin{aligned} \Gamma(x+1) &amp;= \int_0^{\infty} u^{x}e^{-u}du \&amp;= \int_0^{\infty} -u^{x}d(e^{-u}) \&amp;=-u^{x}e^{-u} - \int_0^\infty x u^{x-1} e^{-u} du \qquad\qquad (using: \space \int u(x)d[v(x)] = u(x)v(x) \int v(x)d[u(x)]) \&amp;= -u^{x}e^{-u} + x\Gamma(x) \end{aligned}$$ according $u \in [0,+\infty)$, if $u=0$, $-u^x e^{-u}=0$; and if $u\rightarrow +\infty$: \[\lim_{u\rightarrow +\infty} -\frac {u^x}{e^u} = \lim_{u\rightarrow +\infty} -\frac {x!}{e^u} = 0 \qquad\qquad (using:\space \mathrm{L&#39;H么pital&#39;s \space rule})\] then we have $\Gamma(x+1) = x\Gamma(x)$. The coefficient ensures that the beta distribution is normalized. \[\begin{aligned} \int_0^1 \mathrm{Beta}(\mu \vert a, b) du = 1 \end{aligned}\] Mean \[\begin{aligned} \mathbb{E}[\mu] &amp;= \int_0^1 \mu \mathrm{Beta}(\mu \vert a, b) d\mu \\&amp;= \int_0^1 \mu \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}d\mu \\&amp;= \frac {a\Gamma(a+1+b)}{(a+b)\Gamma(a+1)\Gamma(b)} \int_0^1 \mu^{a}(1-\mu)^{b-1}d\mu \\ &amp;= \frac {a}{a+b} \int_0^1 \frac {\Gamma(c+b)}{\Gamma(c)\Gamma(b)} \mu^{c-1}(1-\mu)^{b-1}d\mu \\ &amp;= \frac {a}{a+b}\end{aligned}\] Variance \[\begin{aligned} \mathrm{var}[\mu] &amp;= \mathbb{E}[(\mu - \frac {a}{a+b})^2] \\&amp;= \int_0^1 (\mu - \frac {a}{a+b})^2 \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} d\mu \\&amp;= \frac {a}{a+b} \left(\int_0^1 \mu \frac {\Gamma(a+b+1)}{\Gamma(a+1)\Gamma(b)} \mu^{a}(1-\mu)^{b-1}\right) - 2\frac {a}{a+b} * \frac {a}{a+b} + \left(\frac {a}{a+b}\right)^2 \\&amp;= \frac {a}{a+b} \frac {a+1}{a+b+1} - \left(\frac {a}{a+b}\right)^2 \\ &amp;= \frac {ab}{(a+b)^2(a+b+1)} \end{aligned}\] The Posterior Distribution \[\begin{aligned} p(\mu \vert D) &amp;\propto p(D\vert \mu)p(\mu) \\ p(\mu \vert m, N, a, b) &amp;\propto \mu^{m+a-1}(1-\mu)^{N-m+b-1} \end{aligned}\] Then we have: \[\begin{aligned} p(\mu \vert m, N, a, b) = \frac {\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(N-m+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1} \end{aligned}\] The Predictive Distribution \[\begin{aligned} p(x\vert D) &amp;= \int_0^1 p(x\vert \mu) p(\mu\vert D)d\mu \\ &amp;=\left\{\begin{aligned}&amp;\int_0^1 \mu p(\mu\vert D)d\mu \qquad \qquad &amp;x=1 \\ &amp; \int_0^1 (1-\mu) p(\mu\vert D)d\mu \qquad \qquad &amp;x=0 \end{aligned} \right. \\ &amp;= \left\{\begin{aligned}&amp;\mathbb{E}_{\mu}[\mu \vert D] \qquad \qquad &amp;x=1 \\ &amp; 1-\mathbb{E}_{\mu}[\mu \vert D] \qquad \qquad &amp;x=0 \end{aligned} \right. \end{aligned}\] Using the posterior distribution result, we obtain: \[\begin{aligned} p(x=1 \vert D) &amp;= \int_0^1 \mu \frac {\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(N-m+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1} d\mu &amp;= \frac {m+a}{N+a+b} \\ p(x=0\vert D) &amp;= 1 - p(x=1 \vert D) &amp;= \frac {N-m+b}{N+a+b} \end{aligned}\] Then we can see, in the limit of an infinitely large data set $m,N\rightarrow \infty$, this result reduces to the maximum likelihood result $\frac mN$. It is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set. For a finite data set, the posterior distribution mean for $\mu$ always lie between prior mean and the maximum likelihood estimate for $\mu$ corresponding to the relative frequencies of events. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Beta Distributions import numpy as np import matplotlib.pyplot as plt from scipy.stats import beta parameters = [(0.1,0.3), (1,1), (8,4), (200,150)] fig, axs = plt.subplots(1,4, figsize=(24,6)) for i in range(4): a, b = parameters[i] ax = axs[i] # mu = np.linspace(beta.ppf(0, a, b), beta.ppf(1, a, b), 100) mu = np.linspace(0,1, 100) ax.plot(mu, beta.pdf(mu, a, b), &#39;r&#39;, lw=3, alpha=0.8, label=&#39;beta pdf&#39;) ax.set_xlim((0,1)) ax.set_title(&quot;a={}, b={}&quot;.format(a,b)) ax.set_xlabel(&quot;$\mu$&quot;) plt.show() We can see that as $a\rightarrow \infty$, $b\rightarrow \infty$, the beta distribution becomes more sharply peaked, because the variance goes to zero for $a\rightarrow \infty$ or $b\rightarrow \infty$. Also in the posterior distribution of $\mu$, as the number of observations increases, the posterior distribution becomes more sharply peaked. And the variance of posterior distribution decreases, that means the uncertainty represented by the posterior distribution will also decrease. But is that a general property of Bayesian learning, we can see in the following: To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold. Consider a gengeral Bayesian inference problem for a parameter $\theta$ for which we have observed a data set $D$, described by the joint distribution $p(\theta, D)$. Firstly: we can see that the posterior mean of $\theta$, averaged over the distribution generation the data, is equal to the prior mean of $\theta$. \[\begin{aligned} \mathbb{E}_\theta[\theta] &amp;= \mathbb{E}_D[\mathbb{E}_\theta[\theta\vert D]] \\ Proof: \qquad&amp; \\ \mathbb{E}_D[\mathbb{E}_\theta[\theta\vert D]] &amp;= \int \left[ \int \theta p(\theta\vert D)d\theta \right]p(D) dD \\ &amp;= \int \theta p(\theta) d\theta \\ &amp;= \mathbb{E}_\theta[\theta] \end{aligned}\] And then, we can see the prior variance of $\theta$ is equal to the average posterior variance of $\theta$ plus to the variance of posterior mean of $\theta$. \[\begin{aligned} \mathrm{var}_{\theta}[\theta] &amp;= \mathbb{E}_{D}[\mathrm{var}_\theta[\theta\vert D]] + \mathrm{var}_{D}[\mathbb{E}_\theta[\theta\vert D]] \\ &amp;\ge \mathbb{E}_{D}[\mathrm{var}_\theta[\theta\vert D]] \end{aligned}\] It means that, on average, the variance of posterior distribution is smaller than the prior variance. And the reduction is greater if the variance in the posterior mean is greater. Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance." />
<meta property="og:description" content="Beta Distribution New we introduce a prior distribution of $p(\mu)$ over the parameter $\mu$ in the Bernouli and Binomial distribution The beta distribution as the prior distribution will have a simple interpretation as well as some useful analytical properties. conjugacy: In this case, if we let the posterior distribution is priportional to the product of the prior and the likelihood function, then it will have the same functional form as the prior. Then we will see the posterior distribution is also a beta distribution. This property is called conjugacy. \(\begin{aligned} \mathrm{Beta}(\mu \vert a, b) = \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \end{aligned}\) where $\Gamma(x)$ is the Gamma function defined by: \[\begin{aligned}\Gamma(x) = \int_0^{\infty} u^{x-1}e^{-u}du \end{aligned}\] $\Gamma(x+1) = x\Gamma(x)$, and then $\Gamma(x+1) = x!$ when $x$ is an integer. $$\begin{aligned} \Gamma(x+1) &amp;= \int_0^{\infty} u^{x}e^{-u}du \&amp;= \int_0^{\infty} -u^{x}d(e^{-u}) \&amp;=-u^{x}e^{-u} - \int_0^\infty x u^{x-1} e^{-u} du \qquad\qquad (using: \space \int u(x)d[v(x)] = u(x)v(x) \int v(x)d[u(x)]) \&amp;= -u^{x}e^{-u} + x\Gamma(x) \end{aligned}$$ according $u \in [0,+\infty)$, if $u=0$, $-u^x e^{-u}=0$; and if $u\rightarrow +\infty$: \[\lim_{u\rightarrow +\infty} -\frac {u^x}{e^u} = \lim_{u\rightarrow +\infty} -\frac {x!}{e^u} = 0 \qquad\qquad (using:\space \mathrm{L&#39;H么pital&#39;s \space rule})\] then we have $\Gamma(x+1) = x\Gamma(x)$. The coefficient ensures that the beta distribution is normalized. \[\begin{aligned} \int_0^1 \mathrm{Beta}(\mu \vert a, b) du = 1 \end{aligned}\] Mean \[\begin{aligned} \mathbb{E}[\mu] &amp;= \int_0^1 \mu \mathrm{Beta}(\mu \vert a, b) d\mu \\&amp;= \int_0^1 \mu \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}d\mu \\&amp;= \frac {a\Gamma(a+1+b)}{(a+b)\Gamma(a+1)\Gamma(b)} \int_0^1 \mu^{a}(1-\mu)^{b-1}d\mu \\ &amp;= \frac {a}{a+b} \int_0^1 \frac {\Gamma(c+b)}{\Gamma(c)\Gamma(b)} \mu^{c-1}(1-\mu)^{b-1}d\mu \\ &amp;= \frac {a}{a+b}\end{aligned}\] Variance \[\begin{aligned} \mathrm{var}[\mu] &amp;= \mathbb{E}[(\mu - \frac {a}{a+b})^2] \\&amp;= \int_0^1 (\mu - \frac {a}{a+b})^2 \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} d\mu \\&amp;= \frac {a}{a+b} \left(\int_0^1 \mu \frac {\Gamma(a+b+1)}{\Gamma(a+1)\Gamma(b)} \mu^{a}(1-\mu)^{b-1}\right) - 2\frac {a}{a+b} * \frac {a}{a+b} + \left(\frac {a}{a+b}\right)^2 \\&amp;= \frac {a}{a+b} \frac {a+1}{a+b+1} - \left(\frac {a}{a+b}\right)^2 \\ &amp;= \frac {ab}{(a+b)^2(a+b+1)} \end{aligned}\] The Posterior Distribution \[\begin{aligned} p(\mu \vert D) &amp;\propto p(D\vert \mu)p(\mu) \\ p(\mu \vert m, N, a, b) &amp;\propto \mu^{m+a-1}(1-\mu)^{N-m+b-1} \end{aligned}\] Then we have: \[\begin{aligned} p(\mu \vert m, N, a, b) = \frac {\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(N-m+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1} \end{aligned}\] The Predictive Distribution \[\begin{aligned} p(x\vert D) &amp;= \int_0^1 p(x\vert \mu) p(\mu\vert D)d\mu \\ &amp;=\left\{\begin{aligned}&amp;\int_0^1 \mu p(\mu\vert D)d\mu \qquad \qquad &amp;x=1 \\ &amp; \int_0^1 (1-\mu) p(\mu\vert D)d\mu \qquad \qquad &amp;x=0 \end{aligned} \right. \\ &amp;= \left\{\begin{aligned}&amp;\mathbb{E}_{\mu}[\mu \vert D] \qquad \qquad &amp;x=1 \\ &amp; 1-\mathbb{E}_{\mu}[\mu \vert D] \qquad \qquad &amp;x=0 \end{aligned} \right. \end{aligned}\] Using the posterior distribution result, we obtain: \[\begin{aligned} p(x=1 \vert D) &amp;= \int_0^1 \mu \frac {\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(N-m+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1} d\mu &amp;= \frac {m+a}{N+a+b} \\ p(x=0\vert D) &amp;= 1 - p(x=1 \vert D) &amp;= \frac {N-m+b}{N+a+b} \end{aligned}\] Then we can see, in the limit of an infinitely large data set $m,N\rightarrow \infty$, this result reduces to the maximum likelihood result $\frac mN$. It is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set. For a finite data set, the posterior distribution mean for $\mu$ always lie between prior mean and the maximum likelihood estimate for $\mu$ corresponding to the relative frequencies of events. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Beta Distributions import numpy as np import matplotlib.pyplot as plt from scipy.stats import beta parameters = [(0.1,0.3), (1,1), (8,4), (200,150)] fig, axs = plt.subplots(1,4, figsize=(24,6)) for i in range(4): a, b = parameters[i] ax = axs[i] # mu = np.linspace(beta.ppf(0, a, b), beta.ppf(1, a, b), 100) mu = np.linspace(0,1, 100) ax.plot(mu, beta.pdf(mu, a, b), &#39;r&#39;, lw=3, alpha=0.8, label=&#39;beta pdf&#39;) ax.set_xlim((0,1)) ax.set_title(&quot;a={}, b={}&quot;.format(a,b)) ax.set_xlabel(&quot;$\mu$&quot;) plt.show() We can see that as $a\rightarrow \infty$, $b\rightarrow \infty$, the beta distribution becomes more sharply peaked, because the variance goes to zero for $a\rightarrow \infty$ or $b\rightarrow \infty$. Also in the posterior distribution of $\mu$, as the number of observations increases, the posterior distribution becomes more sharply peaked. And the variance of posterior distribution decreases, that means the uncertainty represented by the posterior distribution will also decrease. But is that a general property of Bayesian learning, we can see in the following: To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold. Consider a gengeral Bayesian inference problem for a parameter $\theta$ for which we have observed a data set $D$, described by the joint distribution $p(\theta, D)$. Firstly: we can see that the posterior mean of $\theta$, averaged over the distribution generation the data, is equal to the prior mean of $\theta$. \[\begin{aligned} \mathbb{E}_\theta[\theta] &amp;= \mathbb{E}_D[\mathbb{E}_\theta[\theta\vert D]] \\ Proof: \qquad&amp; \\ \mathbb{E}_D[\mathbb{E}_\theta[\theta\vert D]] &amp;= \int \left[ \int \theta p(\theta\vert D)d\theta \right]p(D) dD \\ &amp;= \int \theta p(\theta) d\theta \\ &amp;= \mathbb{E}_\theta[\theta] \end{aligned}\] And then, we can see the prior variance of $\theta$ is equal to the average posterior variance of $\theta$ plus to the variance of posterior mean of $\theta$. \[\begin{aligned} \mathrm{var}_{\theta}[\theta] &amp;= \mathbb{E}_{D}[\mathrm{var}_\theta[\theta\vert D]] + \mathrm{var}_{D}[\mathbb{E}_\theta[\theta\vert D]] \\ &amp;\ge \mathbb{E}_{D}[\mathrm{var}_\theta[\theta\vert D]] \end{aligned}\] It means that, on average, the variance of posterior distribution is smaller than the prior variance. And the reduction is greater if the variance in the posterior mean is greater. Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance." />
<link rel="canonical" href="http://0.0.0.0:4000/posts/beta_distribution/" />
<meta property="og:url" content="http://0.0.0.0:4000/posts/beta_distribution/" />
<meta property="og:site_name" content="Candy Note" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-20T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Beta Distribution" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@luo-songtao" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"luo-songtao"},"dateModified":"2021-02-20T00:00:00+00:00","datePublished":"2021-02-20T00:00:00+00:00","description":"Beta Distribution New we introduce a prior distribution of $p(\\mu)$ over the parameter $\\mu$ in the Bernouli and Binomial distribution The beta distribution as the prior distribution will have a simple interpretation as well as some useful analytical properties. conjugacy: In this case, if we let the posterior distribution is priportional to the product of the prior and the likelihood function, then it will have the same functional form as the prior. Then we will see the posterior distribution is also a beta distribution. This property is called conjugacy. \\(\\begin{aligned} \\mathrm{Beta}(\\mu \\vert a, b) = \\frac {\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\mu^{a-1}(1-\\mu)^{b-1} \\end{aligned}\\) where $\\Gamma(x)$ is the Gamma function defined by: \\[\\begin{aligned}\\Gamma(x) = \\int_0^{\\infty} u^{x-1}e^{-u}du \\end{aligned}\\] $\\Gamma(x+1) = x\\Gamma(x)$, and then $\\Gamma(x+1) = x!$ when $x$ is an integer. $$\\begin{aligned} \\Gamma(x+1) &amp;= \\int_0^{\\infty} u^{x}e^{-u}du \\&amp;= \\int_0^{\\infty} -u^{x}d(e^{-u}) \\&amp;=-u^{x}e^{-u} - \\int_0^\\infty x u^{x-1} e^{-u} du \\qquad\\qquad (using: \\space \\int u(x)d[v(x)] = u(x)v(x) \\int v(x)d[u(x)]) \\&amp;= -u^{x}e^{-u} + x\\Gamma(x) \\end{aligned}$$ according $u \\in [0,+\\infty)$, if $u=0$, $-u^x e^{-u}=0$; and if $u\\rightarrow +\\infty$: \\[\\lim_{u\\rightarrow +\\infty} -\\frac {u^x}{e^u} = \\lim_{u\\rightarrow +\\infty} -\\frac {x!}{e^u} = 0 \\qquad\\qquad (using:\\space \\mathrm{L&#39;H么pital&#39;s \\space rule})\\] then we have $\\Gamma(x+1) = x\\Gamma(x)$. The coefficient ensures that the beta distribution is normalized. \\[\\begin{aligned} \\int_0^1 \\mathrm{Beta}(\\mu \\vert a, b) du = 1 \\end{aligned}\\] Mean \\[\\begin{aligned} \\mathbb{E}[\\mu] &amp;= \\int_0^1 \\mu \\mathrm{Beta}(\\mu \\vert a, b) d\\mu \\\\&amp;= \\int_0^1 \\mu \\frac {\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\mu^{a-1}(1-\\mu)^{b-1}d\\mu \\\\&amp;= \\frac {a\\Gamma(a+1+b)}{(a+b)\\Gamma(a+1)\\Gamma(b)} \\int_0^1 \\mu^{a}(1-\\mu)^{b-1}d\\mu \\\\ &amp;= \\frac {a}{a+b} \\int_0^1 \\frac {\\Gamma(c+b)}{\\Gamma(c)\\Gamma(b)} \\mu^{c-1}(1-\\mu)^{b-1}d\\mu \\\\ &amp;= \\frac {a}{a+b}\\end{aligned}\\] Variance \\[\\begin{aligned} \\mathrm{var}[\\mu] &amp;= \\mathbb{E}[(\\mu - \\frac {a}{a+b})^2] \\\\&amp;= \\int_0^1 (\\mu - \\frac {a}{a+b})^2 \\frac {\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\mu^{a-1}(1-\\mu)^{b-1} d\\mu \\\\&amp;= \\frac {a}{a+b} \\left(\\int_0^1 \\mu \\frac {\\Gamma(a+b+1)}{\\Gamma(a+1)\\Gamma(b)} \\mu^{a}(1-\\mu)^{b-1}\\right) - 2\\frac {a}{a+b} * \\frac {a}{a+b} + \\left(\\frac {a}{a+b}\\right)^2 \\\\&amp;= \\frac {a}{a+b} \\frac {a+1}{a+b+1} - \\left(\\frac {a}{a+b}\\right)^2 \\\\ &amp;= \\frac {ab}{(a+b)^2(a+b+1)} \\end{aligned}\\] The Posterior Distribution \\[\\begin{aligned} p(\\mu \\vert D) &amp;\\propto p(D\\vert \\mu)p(\\mu) \\\\ p(\\mu \\vert m, N, a, b) &amp;\\propto \\mu^{m+a-1}(1-\\mu)^{N-m+b-1} \\end{aligned}\\] Then we have: \\[\\begin{aligned} p(\\mu \\vert m, N, a, b) = \\frac {\\Gamma(N+a+b)}{\\Gamma(m+a)\\Gamma(N-m+b)} \\mu^{m+a-1}(1-\\mu)^{N-m+b-1} \\end{aligned}\\] The Predictive Distribution \\[\\begin{aligned} p(x\\vert D) &amp;= \\int_0^1 p(x\\vert \\mu) p(\\mu\\vert D)d\\mu \\\\ &amp;=\\left\\{\\begin{aligned}&amp;\\int_0^1 \\mu p(\\mu\\vert D)d\\mu \\qquad \\qquad &amp;x=1 \\\\ &amp; \\int_0^1 (1-\\mu) p(\\mu\\vert D)d\\mu \\qquad \\qquad &amp;x=0 \\end{aligned} \\right. \\\\ &amp;= \\left\\{\\begin{aligned}&amp;\\mathbb{E}_{\\mu}[\\mu \\vert D] \\qquad \\qquad &amp;x=1 \\\\ &amp; 1-\\mathbb{E}_{\\mu}[\\mu \\vert D] \\qquad \\qquad &amp;x=0 \\end{aligned} \\right. \\end{aligned}\\] Using the posterior distribution result, we obtain: \\[\\begin{aligned} p(x=1 \\vert D) &amp;= \\int_0^1 \\mu \\frac {\\Gamma(N+a+b)}{\\Gamma(m+a)\\Gamma(N-m+b)} \\mu^{m+a-1}(1-\\mu)^{N-m+b-1} d\\mu &amp;= \\frac {m+a}{N+a+b} \\\\ p(x=0\\vert D) &amp;= 1 - p(x=1 \\vert D) &amp;= \\frac {N-m+b}{N+a+b} \\end{aligned}\\] Then we can see, in the limit of an infinitely large data set $m,N\\rightarrow \\infty$, this result reduces to the maximum likelihood result $\\frac mN$. It is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set. For a finite data set, the posterior distribution mean for $\\mu$ always lie between prior mean and the maximum likelihood estimate for $\\mu$ corresponding to the relative frequencies of events. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Beta Distributions import numpy as np import matplotlib.pyplot as plt from scipy.stats import beta parameters = [(0.1,0.3), (1,1), (8,4), (200,150)] fig, axs = plt.subplots(1,4, figsize=(24,6)) for i in range(4): a, b = parameters[i] ax = axs[i] # mu = np.linspace(beta.ppf(0, a, b), beta.ppf(1, a, b), 100) mu = np.linspace(0,1, 100) ax.plot(mu, beta.pdf(mu, a, b), &#39;r&#39;, lw=3, alpha=0.8, label=&#39;beta pdf&#39;) ax.set_xlim((0,1)) ax.set_title(&quot;a={}, b={}&quot;.format(a,b)) ax.set_xlabel(&quot;$\\mu$&quot;) plt.show() We can see that as $a\\rightarrow \\infty$, $b\\rightarrow \\infty$, the beta distribution becomes more sharply peaked, because the variance goes to zero for $a\\rightarrow \\infty$ or $b\\rightarrow \\infty$. Also in the posterior distribution of $\\mu$, as the number of observations increases, the posterior distribution becomes more sharply peaked. And the variance of posterior distribution decreases, that means the uncertainty represented by the posterior distribution will also decrease. But is that a general property of Bayesian learning, we can see in the following: To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold. Consider a gengeral Bayesian inference problem for a parameter $\\theta$ for which we have observed a data set $D$, described by the joint distribution $p(\\theta, D)$. Firstly: we can see that the posterior mean of $\\theta$, averaged over the distribution generation the data, is equal to the prior mean of $\\theta$. \\[\\begin{aligned} \\mathbb{E}_\\theta[\\theta] &amp;= \\mathbb{E}_D[\\mathbb{E}_\\theta[\\theta\\vert D]] \\\\ Proof: \\qquad&amp; \\\\ \\mathbb{E}_D[\\mathbb{E}_\\theta[\\theta\\vert D]] &amp;= \\int \\left[ \\int \\theta p(\\theta\\vert D)d\\theta \\right]p(D) dD \\\\ &amp;= \\int \\theta p(\\theta) d\\theta \\\\ &amp;= \\mathbb{E}_\\theta[\\theta] \\end{aligned}\\] And then, we can see the prior variance of $\\theta$ is equal to the average posterior variance of $\\theta$ plus to the variance of posterior mean of $\\theta$. \\[\\begin{aligned} \\mathrm{var}_{\\theta}[\\theta] &amp;= \\mathbb{E}_{D}[\\mathrm{var}_\\theta[\\theta\\vert D]] + \\mathrm{var}_{D}[\\mathbb{E}_\\theta[\\theta\\vert D]] \\\\ &amp;\\ge \\mathbb{E}_{D}[\\mathrm{var}_\\theta[\\theta\\vert D]] \\end{aligned}\\] It means that, on average, the variance of posterior distribution is smaller than the prior variance. And the reduction is greater if the variance in the posterior mean is greater. Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance.","headline":"Beta Distribution","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/posts/beta_distribution/"},"url":"http://0.0.0.0:4000/posts/beta_distribution/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Beta Distribution | Candy Note
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Candy Note">
<meta name="application-name" content="Candy Note">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  

    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  
    <!--
  Switch the mode between dark and light.
-->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get MODE_ATTR() { return "data-mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    static get ID() { return "mode-toggle"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener("change", () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();

      });

    } /* constructor() */

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage({
        direction: ModeToggle.ID,
        message: this.modeStatus
      }, "*");
    }

  } /* ModeToggle */

  const toggle = new ModeToggle();

  function flipMode() {
    if (toggle.hasMode) {
      if (toggle.isSysDarkPrefer) {
        if (toggle.isLightMode) {
          toggle.clearMode();
        } else {
          toggle.setLight();
        }

      } else {
        if (toggle.isDarkMode) {
          toggle.clearMode();
        } else {
          toggle.setDark();
        }
      }

    } else {
      if (toggle.isSysDarkPrefer) {
        toggle.setLight();
      } else {
        toggle.setDark();
      }
    }

    toggle.notify();

  } /* flipMode() */

</script>

  

  <body data-spy="scroll" data-target="#toc" data-topbar-visible="true">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
          
          <img src="
            
              /assets/img/head.jpg
            
          " alt="avatar" onerror="this.style.display='none'">
        
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Candy Note</a>
    </div>
    <div class="site-subtitle font-italic">Personal Notes</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>CATEGORIES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>TAGS</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ARCHIVES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center">

    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
      <a href="https://github.com/github_username" aria-label="github"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/twitter_username" aria-label="twitter"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['ryomawithlst','gmail.com'].join('@')" aria-label="email"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          <span>
            <a href="/">
              Home
            </a>
          </span>

        

      

        

      

        

          
            <span>Beta Distribution</span>
          

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        









<div class="row">

  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-8">
    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    

    
      
      
        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->



<!-- images -->




  
  

  
    
      
      
    

    
    
    

    
    

    
    
    

    
      
      

      
      

      

      
    
      
      

      
      

      

      
    

    
      

        <!-- Add CDN URL -->
        

        <!-- Add image path -->
        

        
        

      

      <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

      

    

    <!-- Add SVG placehoder to prevent layout reflow -->

    

    <!-- Bypass the HTML-proofer test -->
    

    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    

    

  

  
  

  

  
  

  

  
  

  




<!-- Wrap prompt element of blockquote with the <div> tag -->







<!-- return -->







<h1 data-toc-skip>Beta Distribution</h1>

<div class="post-meta text-muted">

  <!-- author -->
  <div>
    
    

    

    By
    <em>
      
        <a href="https://github.com/luo-songtao">luo-songtao</a>
      
    </em>
  </div>

  <div class="d-flex">
    <div>
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago"
    data-ts="1613779200"
    
      data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll"
    
    >
  2021-02-20
</em>

      </span>

      <!-- lastmod date -->
      

      <!-- read time -->
      <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom"
  title="862 words">
  <em>4 min</em> read</span>


      <!-- page views -->
      
    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <h2 id="beta-distribution"><span class="mr-2">Beta Distribution</span><a href="#beta-distribution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2>

<ul>
  <li>
    <p>New we introduce a prior distribution of $p(\mu)$ over the parameter $\mu$ in the Bernouli and Binomial distribution</p>
  </li>
  <li>
    <p>The beta distribution as the prior distribution will have a simple interpretation as well as some useful analytical properties.</p>
  </li>
  <li>
    <p><strong>conjugacy</strong>: In this case, if we let the posterior distribution is priportional to the product of the prior and the likelihood function, then it will have the same functional form as the prior. Then we will see the posterior distribution is also a beta distribution. This property is called <strong>conjugacy</strong>.</p>

    <p>\(\begin{aligned} \mathrm{Beta}(\mu \vert a, b) = \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \end{aligned}\)</p>
    <ul>
      <li>
        <p>where $\Gamma(x)$ is the Gamma function defined by:</p>

\[\begin{aligned}\Gamma(x) = \int_0^{\infty} u^{x-1}e^{-u}du \end{aligned}\]

        <ul>
          <li>
            <p>$\Gamma(x+1) = x\Gamma(x)$, and then $\Gamma(x+1) = x!$ when $x$ is an integer.</p>

            <p>$$\begin{aligned} \Gamma(x+1) &amp;= \int_0^{\infty} u^{x}e^{-u}du \&amp;= \int_0^{\infty} -u^{x}d(e^{-u}) \&amp;=-u^{x}e^{-u} - \int_0^\infty x u^{x-1} e^{-u} du \qquad\qquad (using: \space \int u(x)d[v(x)] = u(x)v(x)</p>

            <ul>
              <li>\int v(x)d[u(x)]) \&amp;= -u^{x}e^{-u} + x\Gamma(x)  \end{aligned}$$</li>
              <li>
                <p>according $u \in [0,+\infty)$, if $u=0$, $-u^x e^{-u}=0$; and if $u\rightarrow +\infty$:</p>

\[\lim_{u\rightarrow +\infty} -\frac {u^x}{e^u} = \lim_{u\rightarrow +\infty} -\frac {x!}{e^u} = 0 \qquad\qquad (using:\space \mathrm{L'H么pital's \space rule})\]
              </li>
              <li>then we have $\Gamma(x+1) = x\Gamma(x)$.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>The coefficient ensures that the beta distribution is normalized.</p>

\[\begin{aligned} \int_0^1 \mathrm{Beta}(\mu \vert a, b) du = 1 \end{aligned}\]
      </li>
    </ul>
  </li>
  <li>
    <p>Mean</p>

\[\begin{aligned} \mathbb{E}[\mu] &amp;= \int_0^1 \mu \mathrm{Beta}(\mu \vert a, b) d\mu \\&amp;= \int_0^1 \mu \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}d\mu \\&amp;= \frac {a\Gamma(a+1+b)}{(a+b)\Gamma(a+1)\Gamma(b)} \int_0^1  \mu^{a}(1-\mu)^{b-1}d\mu \\ &amp;= \frac {a}{a+b} \int_0^1 \frac {\Gamma(c+b)}{\Gamma(c)\Gamma(b)} \mu^{c-1}(1-\mu)^{b-1}d\mu \\ &amp;= \frac {a}{a+b}\end{aligned}\]
  </li>
  <li>
    <p>Variance</p>

\[\begin{aligned} \mathrm{var}[\mu]  &amp;= \mathbb{E}[(\mu - \frac {a}{a+b})^2] \\&amp;= \int_0^1 (\mu - \frac {a}{a+b})^2 \frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}  d\mu \\&amp;= \frac {a}{a+b} \left(\int_0^1 \mu \frac {\Gamma(a+b+1)}{\Gamma(a+1)\Gamma(b)} \mu^{a}(1-\mu)^{b-1}\right) - 2\frac {a}{a+b} * \frac {a}{a+b} + \left(\frac {a}{a+b}\right)^2 \\&amp;= \frac {a}{a+b} \frac {a+1}{a+b+1} - \left(\frac {a}{a+b}\right)^2 \\ &amp;= \frac {ab}{(a+b)^2(a+b+1)} \end{aligned}\]
  </li>
  <li>
    <p>The Posterior Distribution</p>

\[\begin{aligned} p(\mu \vert D) &amp;\propto p(D\vert \mu)p(\mu) \\ p(\mu \vert m, N, a, b) &amp;\propto \mu^{m+a-1}(1-\mu)^{N-m+b-1} \end{aligned}\]

    <ul>
      <li>
        <p>Then we have:</p>

\[\begin{aligned} p(\mu \vert m, N, a, b) = \frac {\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(N-m+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1} \end{aligned}\]
      </li>
    </ul>
  </li>
  <li>
    <p>The Predictive Distribution</p>

\[\begin{aligned} p(x\vert D) &amp;= \int_0^1 p(x\vert \mu) p(\mu\vert D)d\mu \\ &amp;=\left\{\begin{aligned}&amp;\int_0^1 \mu p(\mu\vert D)d\mu \qquad \qquad &amp;x=1 \\ &amp; \int_0^1 (1-\mu) p(\mu\vert D)d\mu \qquad \qquad &amp;x=0 \end{aligned} \right. \\ &amp;= \left\{\begin{aligned}&amp;\mathbb{E}_{\mu}[\mu \vert D] \qquad \qquad &amp;x=1 \\ &amp; 1-\mathbb{E}_{\mu}[\mu \vert D] \qquad \qquad &amp;x=0 \end{aligned} \right.  \end{aligned}\]

    <ul>
      <li>
        <p>Using the posterior distribution result, we obtain:</p>

\[\begin{aligned} p(x=1 \vert D) &amp;= \int_0^1 \mu \frac {\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(N-m+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1} d\mu &amp;= \frac {m+a}{N+a+b} \\ p(x=0\vert D) &amp;= 1 - p(x=1 \vert D) &amp;= \frac {N-m+b}{N+a+b}  \end{aligned}\]
      </li>
      <li>
        <p>Then we can see, in the limit of an infinitely large data set $m,N\rightarrow \infty$, this result reduces to the maximum likelihood result $\frac mN$.</p>
      </li>
      <li>
        <p>It is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set.</p>
      </li>
      <li>
        <p>For a finite data set, the posterior distribution mean for $\mu$ always lie between prior mean and the maximum likelihood estimate for $\mu$ corresponding to the relative frequencies of events.</p>
      </li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="c1"># Beta Distributions
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">150</span><span class="p">)]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="c1"># mu = np.linspace(beta.ppf(0, a, b), beta.ppf(1, a, b), 100)
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">beta</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span>
           <span class="s">'r'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'beta pdf'</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"a={}, b={}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$\mu$"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>
<p><img data-src="/assets/images/probability/show_beta.png" alt="show_beta" data-proofer-ignore></p>

<ul>
  <li>
    <p>We can see that as $a\rightarrow \infty$, $b\rightarrow \infty$, the beta distribution becomes more sharply peaked, because the variance goes to zero for $a\rightarrow \infty$ or $b\rightarrow \infty$.</p>
  </li>
  <li>
    <p>Also in the posterior distribution of $\mu$, as the number of observations increases, the posterior distribution becomes more sharply peaked. And the variance of posterior distribution decreases, that means the uncertainty represented by the posterior distribution will also decrease.</p>
  </li>
  <li>
    <p>But is that a general property of Bayesian learning, we can see in the following:</p>
    <ul>
      <li>
        <p>To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold.</p>
      </li>
      <li>Consider a gengeral Bayesian inference problem for a parameter $\theta$ for which we have observed a data set $D$, described by the joint distribution $p(\theta, D)$.</li>
      <li>
        <p>Firstly: we can see that the posterior mean of $\theta$, averaged over the distribution generation the data, is equal to the prior mean of $\theta$.</p>

\[\begin{aligned} \mathbb{E}_\theta[\theta] &amp;= \mathbb{E}_D[\mathbb{E}_\theta[\theta\vert D]] \\ Proof: \qquad&amp;  \\ \mathbb{E}_D[\mathbb{E}_\theta[\theta\vert D]] &amp;= \int \left[ \int \theta p(\theta\vert D)d\theta \right]p(D) dD \\ &amp;= \int \theta p(\theta) d\theta \\ &amp;= \mathbb{E}_\theta[\theta] \end{aligned}\]
      </li>
      <li>
        <p>And then, we can see the prior variance of $\theta$ is equal to the average posterior variance of $\theta$ plus to the variance of posterior mean of $\theta$.</p>

\[\begin{aligned} \mathrm{var}_{\theta}[\theta] &amp;= \mathbb{E}_{D}[\mathrm{var}_\theta[\theta\vert D]] + \mathrm{var}_{D}[\mathbb{E}_\theta[\theta\vert D]] \\ &amp;\ge \mathbb{E}_{D}[\mathrm{var}_\theta[\theta\vert D]] \end{aligned}\]
      </li>
      <li>
        <p>It means that, on average, the variance of posterior distribution is smaller than the prior variance. And the reduction is greater if the variance in the posterior mean is greater.</p>
      </li>
      <li>Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance.</li>
    </ul>
  </li>
</ul>


</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw mr-1"></i>
    
      <a href='/categories/probability/'>Probability</a>,
      <a href='/categories/distribution/'>Distribution</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw mr-1"></i>
      
      <a href="/tags/beta-distribution/"
          class="post-tag no-text-decoration" >Beta Distribution</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.

      
    </div>

    <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=Beta Distribution - Candy Note&amp;url=http://0.0.0.0:4000/posts/beta_distribution/" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=Beta Distribution - Candy Note&amp;u=http://0.0.0.0:4000/posts/beta_distribution/" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://t.me/share/url?url=http://0.0.0.0:4000/posts/beta_distribution/&amp;text=Beta Distribution - Candy Note" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i id="copy-link" class="fa-fw fas fa-link small"
        data-toggle="tooltip" data-placement="top"
        title="Copy link"
        data-title-succeed="Link copied successfully!">
    </i>

  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
    

    </div>
  </div> <!-- #core-wrapper -->

  <!-- pannel -->
  <div id="panel-wrapper" class="col-xl-3 pl-2 text-muted">

    <div class="access">
      















      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/gaussian-distribution/">Gaussian Distribution</a>
    
      
      <a class="post-tag" href="/tags/pca/">PCA</a>
    
      
      <a class="post-tag" href="/tags/em/">EM</a>
    
      
      <a class="post-tag" href="/tags/bayesian/">Bayesian</a>
    
      
      <a class="post-tag" href="/tags/markov-chain/">Markov Chain</a>
    
      
      <a class="post-tag" href="/tags/bernoulli/">Bernoulli</a>
    
      
      <a class="post-tag" href="/tags/binomial/">Binomial</a>
    
      
      <a class="post-tag" href="/tags/gamma-distribution/">Gamma Distribution</a>
    
      
      <a class="post-tag" href="/tags/gmm/">GMM</a>
    
      
      <a class="post-tag" href="/tags/kernel/">Kernel</a>
    

    </div>
  </div>


    </div>

    
      
      



<!-- BS-toc.js will be loaded at medium priority -->
<script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>

<div id="toc-wrapper" class="pl-0 pr-4 mb-5">
  <div class="panel-heading pl-3 pt-2 mb-2">Contents</div>
  <nav id="toc" data-toggle="toc"></nav>
</div>


    
  </div>

</div>

<!-- tail -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
      
        
        <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->






  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/posts/quick_look_continuous_distributions/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1613779200"
    
    >
  2021-02-20
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>A Quick Look of Continuous Distribution</h3>
            <div class="text-muted small">
              <p>
                





                Uniform distribution


  
    $x\sim \mathcal{U}(x\vert a,b)$, $x\in [a,b]$

\[\begin{aligned} p(x) = \mathcal{U}(x\vert a,b) = \frac {1}{b-a} \end{aligned}\]
  


Univariate Gaussian distribution
...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/quick_look_discrete_distributions/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1613779200"
    
    >
  2021-02-20
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>A Quick Look of Discrete Distribution</h3>
            <div class="text-muted small">
              <p>
                





                Bernoulli distribution


  
    $x\sim Bern(x\vert \mu)$

\[\begin{aligned}  p(x) = Bern(x\vert \mu) &amp;amp;= \mu^x(1-\mu)^{1-x} \\ &amp;amp;= \mu^{\mathbb{I}(x=1)}(1-\mu)^{\mathbb{I}(x=0)} \\ \\ \mathbb...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/bernoulli_and_binomial_distribution/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1613779200"
    
    >
  2021-02-20
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bernoulli and Binomial Distribution</h3>
            <div class="text-muted small">
              <p>
                





                Bernoulli Distributions


  
    Bernoulli Distributions is the discrete probability distribution of a random variable which takes the value 1 with probability $\mu$ and the value 0 with probabilit...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->


      
        
        <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/posts/bernoulli_and_binomial_distribution/" class="btn btn-outline-primary"
    prompt="Older">
    <p>Bernoulli and Binomial Distribution</p>
  </a>
  

  
  <a href="/posts/dirichlet_distribution/" class="btn btn-outline-primary"
    prompt="Newer">
    <p>Dirichlet Distribution</p>
  </a>
  

</div>

      
        
        <!--  The comments switcher -->


      
    </div>
  </div>
</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center text-muted">
    <div class="footer-left">
      <p class="mb-0">
        漏 2022
        <a href="https://twitter.com/username">luo-songtao</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/gaussian-distribution/">Gaussian Distribution</a>
    
      
      <a class="post-tag" href="/tags/pca/">PCA</a>
    
      
      <a class="post-tag" href="/tags/em/">EM</a>
    
      
      <a class="post-tag" href="/tags/bayesian/">Bayesian</a>
    
      
      <a class="post-tag" href="/tags/markov-chain/">Markov Chain</a>
    
      
      <a class="post-tag" href="/tags/bernoulli/">Bernoulli</a>
    
      
      <a class="post-tag" href="/tags/binomial/">Binomial</a>
    
      
      <a class="post-tag" href="/tags/gamma-distribution/">Gamma Distribution</a>
    
      
      <a class="post-tag" href="/tags/gmm/">GMM</a>
    
      
      <a class="post-tag" href="/tags/kernel/">Kernel</a>
    

    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    
      <!--
  mermaid-js loader
-->

<script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script>

<script>
  $(function() {
    function updateMermaid(event) {
      if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {

        const mode = event.data.message;

        if (typeof mermaid === "undefined") {
          return;
        }

        let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* Re-render the SVG  <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    let initTheme = "default";

    if ($("html[data-mode=dark]").length > 0
      || ($("html[data-mode]").length == 0
        && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) {
      initTheme = "dark";
    }

    let mermaidConf = {
      theme: initTheme  /* <default|dark|forest|neutral> */
    };

    /* Markdown converts to HTML */
    $("pre").has("code.language-mermaid").each(function() {
      let svgCode = $(this).children().html();
      $(this).addClass("unloaded");
      $(this).after(`<div class=\"mermaid\">${svgCode}</div>`);
    });

    mermaid.initialize(mermaidConf);

    window.addEventListener("message", updateMermaid);
  });
</script>

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup & clipboard -->
  

  







  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script>







  

  

  







  
    

    

  



  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script>








<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
  /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
  MathJax = {
    tex: {
      inlineMath: [              /* start/end delimiter pairs for in-line math */
        ['$','$'],
        ['\\(','\\)']
      ],
      displayMath: [             /* start/end delimiter pairs for display math */
        ['$$', '$$'],
        ['\\[', '\\]']
      ]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>


<!-- commons -->

<script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script>




  </body>

</html>

