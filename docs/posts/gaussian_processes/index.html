<!DOCTYPE html>













<html lang="en"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Allow having a localized datetime different from the appearance language -->
  

  

    

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Gaussian Processes" />
<meta name="author" content="luo-songtao" />
<meta property="og:locale" content="en" />
<meta name="description" content="Gaussian Processes In the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly. At first sight, it might seem difficult to work with a distribution over the uncountably infinite space of functions. However, as we shall see, for a finite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points, and so in practice we can work in a finite space. Definition In general, a Gaussian process is defined as a probability distribution over functions $y(\boldsymbol{x})$ such that the set of values of $y(\boldsymbol{x})$ evaluated at an arbitrary set of points $\boldsymbol{x}_1, . . . , \boldsymbol{x}_N$ jointly have a Gaussian distribution. More generally, a stochastic process $y(\boldsymbol{x})$ is specified by giving the joint probability distribution for any finite set of values $y(\boldsymbol{x}_1), . . . , y(\boldsymbol{x}_N)$ in a consistent manner. A key point about Gaussian stochastic processes is that the joint distribution over $N $ variables $y_1, . . . , y_N$ is specified completely by the second-order statistics, namely the mean and the covariance. Mean: In most applications, we will not have any prior knowledge about the mean of $y(\boldsymbol{x})$ and so by symmetry we take it to be zero. This is equivalent to choosing the mean of the prior over weight values $p(w|α)$ to be zero in the basis function viewpoint. Covariance The specification of the Gaussian process is then completed by giving the covariance of $y(\boldsymbol{x})$ evaluated at any two values of $\boldsymbol{x}$, which is given by the kernel function. \[\begin{aligned} \text{cov} = \mathbb{E}[y(\boldsymbol{x}_n)y(\boldsymbol{x}_m)] = k(y(\boldsymbol{x}_n, y(\boldsymbol{x}_m)) \end{aligned}\] Gaussian Processes for Regression Let the prior on the regression function be a Gaussian Processes, denoted by: \(\begin{aligned} f(\boldsymbol{x}) \sim \text{GP}(\mu(\boldsymbol{x}), k(\boldsymbol{x}, \boldsymbol{x}&#39;)) \end{aligned}\) where $\mu(\boldsymbol{x})$ is the mean function and $k(\boldsymbol{x}, \boldsymbol{x}’)$ is the kernel or covariance function, i.e., \[\begin{aligned} \mu(\boldsymbol{x}) &amp;= \mathbb{E}[f(\boldsymbol{x})] \\ k(\boldsymbol{x}, \boldsymbol{x}&#39;) &amp;= \mathbb{E}\left[(f(\boldsymbol{x})-\mu(\boldsymbol{x}))(f(\boldsymbol{x}&#39;)-\mu(\boldsymbol{x}&#39;))^T \right] \end{aligned}\] Note: We obviously require that $k()$ be a positive definite kernel. And then for any finite set of points, this process defines a joint Gaussian: [\begin{aligned} p(\mathbf{f} \vert \mathbf{X}) = \mathcal{N}(\mathbf{f} \vert \boldsymbol{\mu}, \mathbf{K}) \end{aligned}] where $\mathbf{K}_{ij} = k(\boldsymbol{x}_i, \boldsymbol{x}_j)$ and $\boldsymbol{\mu} = (\mu(\boldsymbol{x}_1), . . . , \mu(\boldsymbol{x}_N))$. Note that it is common to use a mean function of $\mu(\boldsymbol{x})$ = 0, since the GP is flexible enough to model the mean arbitrarily well. Predictions using noise-free observations Suppose we observe a training set $\mathbf{X}_N = {\boldsymbol{x}_1, …,\boldsymbol{x}_N }$, $\mathbf{t}_N = {f(\boldsymbol{x}_1),…,f(\boldsymbol{x}_N)}$, and this is the noise-free observation of the function evaluated at $\boldsymbol{x}_n$ [\begin{aligned} p(\boldsymbol{t}) &amp;= p(\boldsymbol{f}) \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{K}) \end{aligned}] The predictino problem \(\begin{aligned} \binom{\mathbf{t}_N}{t_{N+1}} \sim \mathcal{N}\left(\binom{\boldsymbol{\mu}_N}{\mu_{N+1}}, \binom{\mathbf{K}_N\qquad \mathbf{a}}{\mathbf{a}^T \qquad k_{N+1}} \right) \end{aligned}\) where we have defined: \[\begin{aligned} \boldsymbol{\mu}_N &amp;= [\mu_1,...,\mu_N]^T \\ \mu_{N+1} &amp;= \mu(\boldsymbol{x}_{N+1}) = \mathbb{E}[f(\boldsymbol{x}_{N+1})] \\ \mathbf{a} &amp;= [k(\boldsymbol{x}_1,\boldsymbol{x}_{N+1}),...,k(\boldsymbol{x}_N,\boldsymbol{x}_{N+1})]^T \\ k_{N+1} &amp;= k(\boldsymbol{x}_{N+1}, \boldsymbol{x}_{N+1}) \end{aligned}\] then the posterior distribution: [\begin{aligned} p(t_{N+1} \vert \mathbf{t}N) &amp;= \mathcal{N}(t{N+1} \vert m, \sigma^2) \ \ m &amp;= \mu_{N+1} + \mathbf{a}^T \mathbf{K}N^{-1}(\mathbf{t}_N - \boldsymbol{\mu}_N) \ \sigma^2 &amp;= k{N+1} - \mathbf{a}^T\mathbf{K}_N^{-1}\mathbf{a} \end{aligned}] Predictions using noise observations [\begin{aligned} t_{n} = f(\boldsymbol{x}_n) + \varepsilon_n \end{aligned}] here $\varepsilon_n$ is a random noisy variable, and we this is a noisy processes that have a Gaussian distribution $\varepsilon_n \sim \mathcal{N}(0, \beta^{-1}))$. then: [\begin{aligned} p(\boldsymbol{f}) &amp;= \mathcal{N}(\boldsymbol{f}\vert 0, \mathbf{K}) \ p(\boldsymbol{t} \vert \boldsymbol{f}) &amp;= \mathcal{N}(\boldsymbol{t} \vert \boldsymbol{f}, \beta^{-1}I) \ \ p(\boldsymbol{t}) &amp;= \mathcal{N}(\boldsymbol{t} \vert 0, \mathbf{K}+\beta^{-1}I) \end{aligned}] then the posterior distribution: [\begin{aligned} \mathbf{K}N &amp;= \mathbf{K}_N + \beta^{-1}I\ \ p(t{N+1} \vert \mathbf{t}N) &amp;= \mathcal{N}(t{N+1} \vert m, \sigma^2) \ \ m &amp;= \mathbf{a}^T \mathbf{K}N^{-1}\mathbf{t}_N \ \sigma^2 &amp;= k{N+1} - \mathbf{a}^T\mathbf{K}_N^{-1}\mathbf{a} \end{aligned}] Some typical Kernel Functions used for Gaussian Processes Linear kernel \[\begin{aligned} k(x,x&#39;) = x^Tx&#39; \end{aligned}\] Note this kernel does not correspond to a stationary process Gaussian Kernel or Squared Exponential Kernel \[\begin{aligned} k(x,x&#39;) = \exp\left( -\frac {1}{2h^2}\|x-x&#39;\|^2 \right) \end{aligned}\] where $h$ determines the length scale of process. The smaller the value of $h$, the larger the “statistical” similarity (stronger correlation) of two points having a distance $d=|x-x’|^2$ apart Ornstein-Uhlenbeck Kernel \[\begin{aligned} k(x,x&#39;) = \exp\left( -\frac {1}{h}\|x-x&#39;\| \right) \end{aligned}\] Rational Quadratic Kernel \[\begin{aligned} k(x,x&#39;) = ( 1+\|x-x&#39;\|^2)^{-\alpha}, \qquad \alpha \ge 0 \end{aligned}\] Parameterized Kernel Functions \[\begin{aligned} k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x&#39;\|^2 \right) \\ k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x&#39;\|^2 \right) + \theta_3 \\ k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x&#39;\|^2 \right) + \theta_3 + \theta_4 x^Tx&#39; \\ k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {1}{2}(x-x&#39;)^TM(x-x&#39;) \right) \\ &amp;\space\space\vdots \end{aligned}\] For Parameterized Kernel Functions, we can use MLE to find the parameters given a data set. Also Bayesian inference can be used to find the posterior distribution of parameters, and then MAP estimation. Another topics Multiple kernel learning Semi-parametric Computational and numerical issues …… Gaussian Processes for Classification Binary Classification Assume $t_n \in (0,1)$, and let $a_n = f(\boldsymbol{x}_n)$ then we define: \[\begin{aligned} p(t_n\vert a_n) = \sigma(a)^t(1-\sigma)^{1-t} \end{aligned}\] $a_n$ is a Gaussian Processes \[\begin{aligned} p(\boldsymbol{a}) = \mathcal{N}(\boldsymbol{a}\vert \boldsymbol{\mu}, \mathbf{K}) \end{aligned}\] But note that ${t_n}$ is not a gaussian processes And we can introduce a noise-like term by a parameter $\nu$ that ensures that the covariance matrix $\mathbf{K}$ is positive definite. \[\begin{aligned} K_{nm} = k(\boldsymbol{x_n}, \boldsymbol{x}_m) + \nu\delta_{nm} \end{aligned}\] Unnormalized posterior: \[\begin{aligned} \tilde{p}(\boldsymbol{t}, \boldsymbol{a}) = \mathcal{N}(\boldsymbol{a}\vert \boldsymbol{\mu}, \mathbf{K})\prod_{n=1}^N p(t_n\vert a_n) \end{aligned}\] wighting for update…" />
<meta property="og:description" content="Gaussian Processes In the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly. At first sight, it might seem difficult to work with a distribution over the uncountably infinite space of functions. However, as we shall see, for a finite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points, and so in practice we can work in a finite space. Definition In general, a Gaussian process is defined as a probability distribution over functions $y(\boldsymbol{x})$ such that the set of values of $y(\boldsymbol{x})$ evaluated at an arbitrary set of points $\boldsymbol{x}_1, . . . , \boldsymbol{x}_N$ jointly have a Gaussian distribution. More generally, a stochastic process $y(\boldsymbol{x})$ is specified by giving the joint probability distribution for any finite set of values $y(\boldsymbol{x}_1), . . . , y(\boldsymbol{x}_N)$ in a consistent manner. A key point about Gaussian stochastic processes is that the joint distribution over $N $ variables $y_1, . . . , y_N$ is specified completely by the second-order statistics, namely the mean and the covariance. Mean: In most applications, we will not have any prior knowledge about the mean of $y(\boldsymbol{x})$ and so by symmetry we take it to be zero. This is equivalent to choosing the mean of the prior over weight values $p(w|α)$ to be zero in the basis function viewpoint. Covariance The specification of the Gaussian process is then completed by giving the covariance of $y(\boldsymbol{x})$ evaluated at any two values of $\boldsymbol{x}$, which is given by the kernel function. \[\begin{aligned} \text{cov} = \mathbb{E}[y(\boldsymbol{x}_n)y(\boldsymbol{x}_m)] = k(y(\boldsymbol{x}_n, y(\boldsymbol{x}_m)) \end{aligned}\] Gaussian Processes for Regression Let the prior on the regression function be a Gaussian Processes, denoted by: \(\begin{aligned} f(\boldsymbol{x}) \sim \text{GP}(\mu(\boldsymbol{x}), k(\boldsymbol{x}, \boldsymbol{x}&#39;)) \end{aligned}\) where $\mu(\boldsymbol{x})$ is the mean function and $k(\boldsymbol{x}, \boldsymbol{x}’)$ is the kernel or covariance function, i.e., \[\begin{aligned} \mu(\boldsymbol{x}) &amp;= \mathbb{E}[f(\boldsymbol{x})] \\ k(\boldsymbol{x}, \boldsymbol{x}&#39;) &amp;= \mathbb{E}\left[(f(\boldsymbol{x})-\mu(\boldsymbol{x}))(f(\boldsymbol{x}&#39;)-\mu(\boldsymbol{x}&#39;))^T \right] \end{aligned}\] Note: We obviously require that $k()$ be a positive definite kernel. And then for any finite set of points, this process defines a joint Gaussian: [\begin{aligned} p(\mathbf{f} \vert \mathbf{X}) = \mathcal{N}(\mathbf{f} \vert \boldsymbol{\mu}, \mathbf{K}) \end{aligned}] where $\mathbf{K}_{ij} = k(\boldsymbol{x}_i, \boldsymbol{x}_j)$ and $\boldsymbol{\mu} = (\mu(\boldsymbol{x}_1), . . . , \mu(\boldsymbol{x}_N))$. Note that it is common to use a mean function of $\mu(\boldsymbol{x})$ = 0, since the GP is flexible enough to model the mean arbitrarily well. Predictions using noise-free observations Suppose we observe a training set $\mathbf{X}_N = {\boldsymbol{x}_1, …,\boldsymbol{x}_N }$, $\mathbf{t}_N = {f(\boldsymbol{x}_1),…,f(\boldsymbol{x}_N)}$, and this is the noise-free observation of the function evaluated at $\boldsymbol{x}_n$ [\begin{aligned} p(\boldsymbol{t}) &amp;= p(\boldsymbol{f}) \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{K}) \end{aligned}] The predictino problem \(\begin{aligned} \binom{\mathbf{t}_N}{t_{N+1}} \sim \mathcal{N}\left(\binom{\boldsymbol{\mu}_N}{\mu_{N+1}}, \binom{\mathbf{K}_N\qquad \mathbf{a}}{\mathbf{a}^T \qquad k_{N+1}} \right) \end{aligned}\) where we have defined: \[\begin{aligned} \boldsymbol{\mu}_N &amp;= [\mu_1,...,\mu_N]^T \\ \mu_{N+1} &amp;= \mu(\boldsymbol{x}_{N+1}) = \mathbb{E}[f(\boldsymbol{x}_{N+1})] \\ \mathbf{a} &amp;= [k(\boldsymbol{x}_1,\boldsymbol{x}_{N+1}),...,k(\boldsymbol{x}_N,\boldsymbol{x}_{N+1})]^T \\ k_{N+1} &amp;= k(\boldsymbol{x}_{N+1}, \boldsymbol{x}_{N+1}) \end{aligned}\] then the posterior distribution: [\begin{aligned} p(t_{N+1} \vert \mathbf{t}N) &amp;= \mathcal{N}(t{N+1} \vert m, \sigma^2) \ \ m &amp;= \mu_{N+1} + \mathbf{a}^T \mathbf{K}N^{-1}(\mathbf{t}_N - \boldsymbol{\mu}_N) \ \sigma^2 &amp;= k{N+1} - \mathbf{a}^T\mathbf{K}_N^{-1}\mathbf{a} \end{aligned}] Predictions using noise observations [\begin{aligned} t_{n} = f(\boldsymbol{x}_n) + \varepsilon_n \end{aligned}] here $\varepsilon_n$ is a random noisy variable, and we this is a noisy processes that have a Gaussian distribution $\varepsilon_n \sim \mathcal{N}(0, \beta^{-1}))$. then: [\begin{aligned} p(\boldsymbol{f}) &amp;= \mathcal{N}(\boldsymbol{f}\vert 0, \mathbf{K}) \ p(\boldsymbol{t} \vert \boldsymbol{f}) &amp;= \mathcal{N}(\boldsymbol{t} \vert \boldsymbol{f}, \beta^{-1}I) \ \ p(\boldsymbol{t}) &amp;= \mathcal{N}(\boldsymbol{t} \vert 0, \mathbf{K}+\beta^{-1}I) \end{aligned}] then the posterior distribution: [\begin{aligned} \mathbf{K}N &amp;= \mathbf{K}_N + \beta^{-1}I\ \ p(t{N+1} \vert \mathbf{t}N) &amp;= \mathcal{N}(t{N+1} \vert m, \sigma^2) \ \ m &amp;= \mathbf{a}^T \mathbf{K}N^{-1}\mathbf{t}_N \ \sigma^2 &amp;= k{N+1} - \mathbf{a}^T\mathbf{K}_N^{-1}\mathbf{a} \end{aligned}] Some typical Kernel Functions used for Gaussian Processes Linear kernel \[\begin{aligned} k(x,x&#39;) = x^Tx&#39; \end{aligned}\] Note this kernel does not correspond to a stationary process Gaussian Kernel or Squared Exponential Kernel \[\begin{aligned} k(x,x&#39;) = \exp\left( -\frac {1}{2h^2}\|x-x&#39;\|^2 \right) \end{aligned}\] where $h$ determines the length scale of process. The smaller the value of $h$, the larger the “statistical” similarity (stronger correlation) of two points having a distance $d=|x-x’|^2$ apart Ornstein-Uhlenbeck Kernel \[\begin{aligned} k(x,x&#39;) = \exp\left( -\frac {1}{h}\|x-x&#39;\| \right) \end{aligned}\] Rational Quadratic Kernel \[\begin{aligned} k(x,x&#39;) = ( 1+\|x-x&#39;\|^2)^{-\alpha}, \qquad \alpha \ge 0 \end{aligned}\] Parameterized Kernel Functions \[\begin{aligned} k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x&#39;\|^2 \right) \\ k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x&#39;\|^2 \right) + \theta_3 \\ k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x&#39;\|^2 \right) + \theta_3 + \theta_4 x^Tx&#39; \\ k(x,x&#39;;\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {1}{2}(x-x&#39;)^TM(x-x&#39;) \right) \\ &amp;\space\space\vdots \end{aligned}\] For Parameterized Kernel Functions, we can use MLE to find the parameters given a data set. Also Bayesian inference can be used to find the posterior distribution of parameters, and then MAP estimation. Another topics Multiple kernel learning Semi-parametric Computational and numerical issues …… Gaussian Processes for Classification Binary Classification Assume $t_n \in (0,1)$, and let $a_n = f(\boldsymbol{x}_n)$ then we define: \[\begin{aligned} p(t_n\vert a_n) = \sigma(a)^t(1-\sigma)^{1-t} \end{aligned}\] $a_n$ is a Gaussian Processes \[\begin{aligned} p(\boldsymbol{a}) = \mathcal{N}(\boldsymbol{a}\vert \boldsymbol{\mu}, \mathbf{K}) \end{aligned}\] But note that ${t_n}$ is not a gaussian processes And we can introduce a noise-like term by a parameter $\nu$ that ensures that the covariance matrix $\mathbf{K}$ is positive definite. \[\begin{aligned} K_{nm} = k(\boldsymbol{x_n}, \boldsymbol{x}_m) + \nu\delta_{nm} \end{aligned}\] Unnormalized posterior: \[\begin{aligned} \tilde{p}(\boldsymbol{t}, \boldsymbol{a}) = \mathcal{N}(\boldsymbol{a}\vert \boldsymbol{\mu}, \mathbf{K})\prod_{n=1}^N p(t_n\vert a_n) \end{aligned}\] wighting for update…" />
<link rel="canonical" href="http://0.0.0.0:4000/posts/gaussian_processes/" />
<meta property="og:url" content="http://0.0.0.0:4000/posts/gaussian_processes/" />
<meta property="og:site_name" content="Candy Note" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-21T01:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Gaussian Processes" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@luo-songtao" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"luo-songtao"},"dateModified":"2022-02-21T01:00:00+00:00","datePublished":"2022-02-21T01:00:00+00:00","description":"Gaussian Processes In the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly. At first sight, it might seem difficult to work with a distribution over the uncountably infinite space of functions. However, as we shall see, for a finite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points, and so in practice we can work in a finite space. Definition In general, a Gaussian process is defined as a probability distribution over functions $y(\\boldsymbol{x})$ such that the set of values of $y(\\boldsymbol{x})$ evaluated at an arbitrary set of points $\\boldsymbol{x}_1, . . . , \\boldsymbol{x}_N$ jointly have a Gaussian distribution. More generally, a stochastic process $y(\\boldsymbol{x})$ is specified by giving the joint probability distribution for any finite set of values $y(\\boldsymbol{x}_1), . . . , y(\\boldsymbol{x}_N)$ in a consistent manner. A key point about Gaussian stochastic processes is that the joint distribution over $N $ variables $y_1, . . . , y_N$ is specified completely by the second-order statistics, namely the mean and the covariance. Mean: In most applications, we will not have any prior knowledge about the mean of $y(\\boldsymbol{x})$ and so by symmetry we take it to be zero. This is equivalent to choosing the mean of the prior over weight values $p(w|α)$ to be zero in the basis function viewpoint. Covariance The specification of the Gaussian process is then completed by giving the covariance of $y(\\boldsymbol{x})$ evaluated at any two values of $\\boldsymbol{x}$, which is given by the kernel function. \\[\\begin{aligned} \\text{cov} = \\mathbb{E}[y(\\boldsymbol{x}_n)y(\\boldsymbol{x}_m)] = k(y(\\boldsymbol{x}_n, y(\\boldsymbol{x}_m)) \\end{aligned}\\] Gaussian Processes for Regression Let the prior on the regression function be a Gaussian Processes, denoted by: \\(\\begin{aligned} f(\\boldsymbol{x}) \\sim \\text{GP}(\\mu(\\boldsymbol{x}), k(\\boldsymbol{x}, \\boldsymbol{x}&#39;)) \\end{aligned}\\) where $\\mu(\\boldsymbol{x})$ is the mean function and $k(\\boldsymbol{x}, \\boldsymbol{x}’)$ is the kernel or covariance function, i.e., \\[\\begin{aligned} \\mu(\\boldsymbol{x}) &amp;= \\mathbb{E}[f(\\boldsymbol{x})] \\\\ k(\\boldsymbol{x}, \\boldsymbol{x}&#39;) &amp;= \\mathbb{E}\\left[(f(\\boldsymbol{x})-\\mu(\\boldsymbol{x}))(f(\\boldsymbol{x}&#39;)-\\mu(\\boldsymbol{x}&#39;))^T \\right] \\end{aligned}\\] Note: We obviously require that $k()$ be a positive definite kernel. And then for any finite set of points, this process defines a joint Gaussian: [\\begin{aligned} p(\\mathbf{f} \\vert \\mathbf{X}) = \\mathcal{N}(\\mathbf{f} \\vert \\boldsymbol{\\mu}, \\mathbf{K}) \\end{aligned}] where $\\mathbf{K}_{ij} = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$ and $\\boldsymbol{\\mu} = (\\mu(\\boldsymbol{x}_1), . . . , \\mu(\\boldsymbol{x}_N))$. Note that it is common to use a mean function of $\\mu(\\boldsymbol{x})$ = 0, since the GP is flexible enough to model the mean arbitrarily well. Predictions using noise-free observations Suppose we observe a training set $\\mathbf{X}_N = {\\boldsymbol{x}_1, …,\\boldsymbol{x}_N }$, $\\mathbf{t}_N = {f(\\boldsymbol{x}_1),…,f(\\boldsymbol{x}_N)}$, and this is the noise-free observation of the function evaluated at $\\boldsymbol{x}_n$ [\\begin{aligned} p(\\boldsymbol{t}) &amp;= p(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{K}) \\end{aligned}] The predictino problem \\(\\begin{aligned} \\binom{\\mathbf{t}_N}{t_{N+1}} \\sim \\mathcal{N}\\left(\\binom{\\boldsymbol{\\mu}_N}{\\mu_{N+1}}, \\binom{\\mathbf{K}_N\\qquad \\mathbf{a}}{\\mathbf{a}^T \\qquad k_{N+1}} \\right) \\end{aligned}\\) where we have defined: \\[\\begin{aligned} \\boldsymbol{\\mu}_N &amp;= [\\mu_1,...,\\mu_N]^T \\\\ \\mu_{N+1} &amp;= \\mu(\\boldsymbol{x}_{N+1}) = \\mathbb{E}[f(\\boldsymbol{x}_{N+1})] \\\\ \\mathbf{a} &amp;= [k(\\boldsymbol{x}_1,\\boldsymbol{x}_{N+1}),...,k(\\boldsymbol{x}_N,\\boldsymbol{x}_{N+1})]^T \\\\ k_{N+1} &amp;= k(\\boldsymbol{x}_{N+1}, \\boldsymbol{x}_{N+1}) \\end{aligned}\\] then the posterior distribution: [\\begin{aligned} p(t_{N+1} \\vert \\mathbf{t}N) &amp;= \\mathcal{N}(t{N+1} \\vert m, \\sigma^2) \\ \\ m &amp;= \\mu_{N+1} + \\mathbf{a}^T \\mathbf{K}N^{-1}(\\mathbf{t}_N - \\boldsymbol{\\mu}_N) \\ \\sigma^2 &amp;= k{N+1} - \\mathbf{a}^T\\mathbf{K}_N^{-1}\\mathbf{a} \\end{aligned}] Predictions using noise observations [\\begin{aligned} t_{n} = f(\\boldsymbol{x}_n) + \\varepsilon_n \\end{aligned}] here $\\varepsilon_n$ is a random noisy variable, and we this is a noisy processes that have a Gaussian distribution $\\varepsilon_n \\sim \\mathcal{N}(0, \\beta^{-1}))$. then: [\\begin{aligned} p(\\boldsymbol{f}) &amp;= \\mathcal{N}(\\boldsymbol{f}\\vert 0, \\mathbf{K}) \\ p(\\boldsymbol{t} \\vert \\boldsymbol{f}) &amp;= \\mathcal{N}(\\boldsymbol{t} \\vert \\boldsymbol{f}, \\beta^{-1}I) \\ \\ p(\\boldsymbol{t}) &amp;= \\mathcal{N}(\\boldsymbol{t} \\vert 0, \\mathbf{K}+\\beta^{-1}I) \\end{aligned}] then the posterior distribution: [\\begin{aligned} \\mathbf{K}N &amp;= \\mathbf{K}_N + \\beta^{-1}I\\ \\ p(t{N+1} \\vert \\mathbf{t}N) &amp;= \\mathcal{N}(t{N+1} \\vert m, \\sigma^2) \\ \\ m &amp;= \\mathbf{a}^T \\mathbf{K}N^{-1}\\mathbf{t}_N \\ \\sigma^2 &amp;= k{N+1} - \\mathbf{a}^T\\mathbf{K}_N^{-1}\\mathbf{a} \\end{aligned}] Some typical Kernel Functions used for Gaussian Processes Linear kernel \\[\\begin{aligned} k(x,x&#39;) = x^Tx&#39; \\end{aligned}\\] Note this kernel does not correspond to a stationary process Gaussian Kernel or Squared Exponential Kernel \\[\\begin{aligned} k(x,x&#39;) = \\exp\\left( -\\frac {1}{2h^2}\\|x-x&#39;\\|^2 \\right) \\end{aligned}\\] where $h$ determines the length scale of process. The smaller the value of $h$, the larger the “statistical” similarity (stronger correlation) of two points having a distance $d=|x-x’|^2$ apart Ornstein-Uhlenbeck Kernel \\[\\begin{aligned} k(x,x&#39;) = \\exp\\left( -\\frac {1}{h}\\|x-x&#39;\\| \\right) \\end{aligned}\\] Rational Quadratic Kernel \\[\\begin{aligned} k(x,x&#39;) = ( 1+\\|x-x&#39;\\|^2)^{-\\alpha}, \\qquad \\alpha \\ge 0 \\end{aligned}\\] Parameterized Kernel Functions \\[\\begin{aligned} k(x,x&#39;;\\boldsymbol{\\theta}) &amp;= \\theta_1 \\exp\\left( -\\frac {\\theta_2}{2}\\|x-x&#39;\\|^2 \\right) \\\\ k(x,x&#39;;\\boldsymbol{\\theta}) &amp;= \\theta_1 \\exp\\left( -\\frac {\\theta_2}{2}\\|x-x&#39;\\|^2 \\right) + \\theta_3 \\\\ k(x,x&#39;;\\boldsymbol{\\theta}) &amp;= \\theta_1 \\exp\\left( -\\frac {\\theta_2}{2}\\|x-x&#39;\\|^2 \\right) + \\theta_3 + \\theta_4 x^Tx&#39; \\\\ k(x,x&#39;;\\boldsymbol{\\theta}) &amp;= \\theta_1 \\exp\\left( -\\frac {1}{2}(x-x&#39;)^TM(x-x&#39;) \\right) \\\\ &amp;\\space\\space\\vdots \\end{aligned}\\] For Parameterized Kernel Functions, we can use MLE to find the parameters given a data set. Also Bayesian inference can be used to find the posterior distribution of parameters, and then MAP estimation. Another topics Multiple kernel learning Semi-parametric Computational and numerical issues …… Gaussian Processes for Classification Binary Classification Assume $t_n \\in (0,1)$, and let $a_n = f(\\boldsymbol{x}_n)$ then we define: \\[\\begin{aligned} p(t_n\\vert a_n) = \\sigma(a)^t(1-\\sigma)^{1-t} \\end{aligned}\\] $a_n$ is a Gaussian Processes \\[\\begin{aligned} p(\\boldsymbol{a}) = \\mathcal{N}(\\boldsymbol{a}\\vert \\boldsymbol{\\mu}, \\mathbf{K}) \\end{aligned}\\] But note that ${t_n}$ is not a gaussian processes And we can introduce a noise-like term by a parameter $\\nu$ that ensures that the covariance matrix $\\mathbf{K}$ is positive definite. \\[\\begin{aligned} K_{nm} = k(\\boldsymbol{x_n}, \\boldsymbol{x}_m) + \\nu\\delta_{nm} \\end{aligned}\\] Unnormalized posterior: \\[\\begin{aligned} \\tilde{p}(\\boldsymbol{t}, \\boldsymbol{a}) = \\mathcal{N}(\\boldsymbol{a}\\vert \\boldsymbol{\\mu}, \\mathbf{K})\\prod_{n=1}^N p(t_n\\vert a_n) \\end{aligned}\\] wighting for update…","headline":"Gaussian Processes","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/posts/gaussian_processes/"},"url":"http://0.0.0.0:4000/posts/gaussian_processes/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Gaussian Processes | Candy Note
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Candy Note">
<meta name="application-name" content="Candy Note">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  

    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

</head>


  
    <!--
  Switch the mode between dark and light.
-->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get MODE_ATTR() { return "data-mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    static get ID() { return "mode-toggle"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener("change", () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();

      });

    } /* constructor() */

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage({
        direction: ModeToggle.ID,
        message: this.modeStatus
      }, "*");
    }

  } /* ModeToggle */

  const toggle = new ModeToggle();

  function flipMode() {
    if (toggle.hasMode) {
      if (toggle.isSysDarkPrefer) {
        if (toggle.isLightMode) {
          toggle.clearMode();
        } else {
          toggle.setLight();
        }

      } else {
        if (toggle.isDarkMode) {
          toggle.clearMode();
        } else {
          toggle.setDark();
        }
      }

    } else {
      if (toggle.isSysDarkPrefer) {
        toggle.setLight();
      } else {
        toggle.setDark();
      }
    }

    toggle.notify();

  } /* flipMode() */

</script>

  

  <body data-spy="scroll" data-target="#toc" data-topbar-visible="true">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
          
          <img src="
            
              /assets/img/head.jpg
            
          " alt="avatar" onerror="this.style.display='none'">
        
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Candy Note</a>
    </div>
    <div class="site-subtitle font-italic">Personal Notes</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>CATEGORIES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>TAGS</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ARCHIVES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center">

    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
      <a href="https://github.com/github_username" aria-label="github"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/twitter_username" aria-label="twitter"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['ryomawithlst','gmail.com'].join('@')" aria-label="email"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          <span>
            <a href="/">
              Home
            </a>
          </span>

        

      

        

      

        

          
            <span>Gaussian Processes</span>
          

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        









<div class="row">

  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-8">
    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    

    
      
      
        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Add attribute 'hide-bullet' to the checkbox list -->



<!-- images -->





<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  




<!-- Wrap prompt element of blockquote with the <div> tag -->







<!-- return -->







<h1 data-toc-skip>Gaussian Processes</h1>

<div class="post-meta text-muted">

  <!-- author -->
  <div>
    
    

    

    By
    <em>
      
        <a href="https://github.com/luo-songtao">luo-songtao</a>
      
    </em>
  </div>

  <div class="d-flex">
    <div>
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago"
    data-ts="1645405200"
    
      data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll"
    
    >
  2022-02-21
</em>

      </span>

      <!-- lastmod date -->
      

      <!-- read time -->
      <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom"
  title="874 words">
  <em>4 min</em> read</span>


      <!-- page views -->
      
    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <h2 id="gaussian-processes"><span class="mr-2">Gaussian Processes</span><a href="#gaussian-processes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2>

<p>In the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly.</p>

<ul>
  <li>At first sight, it might seem difficult to work with a distribution over the uncountably infinite space of functions.</li>
  <li>However, as we shall see, for a finite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points, and so in practice we can work in a finite space.</li>
</ul>

<h3 id="definition"><span class="mr-2">Definition</span><a href="#definition" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3>

<p>In general, a Gaussian process is defined as a probability distribution over functions $y(\boldsymbol{x})$ such that the set of values of $y(\boldsymbol{x})$ evaluated at an arbitrary set of points $\boldsymbol{x}_1, . . . , \boldsymbol{x}_N$ jointly have a Gaussian distribution.</p>

<ul>
  <li>More generally, a stochastic process $y(\boldsymbol{x})$ is specified by giving the joint probability distribution for any finite set of values $y(\boldsymbol{x}_1), . . . , y(\boldsymbol{x}_N)$ in a consistent manner.</li>
</ul>

<p>A key point about Gaussian stochastic processes is that the joint distribution over $N $ variables $y_1, . . . , y_N$ is specified completely by the second-order statistics, namely the mean and the covariance.</p>

<ul>
  <li>Mean:
    <ul>
      <li>In most applications, we will not have any prior knowledge about the mean of $y(\boldsymbol{x})$ and so by symmetry we take it to be zero. This is equivalent to choosing the mean of the prior over weight values $p(w|α)$ to
be zero in the basis function viewpoint.</li>
    </ul>
  </li>
  <li>Covariance
    <ul>
      <li>
        <p>The specification of the Gaussian process is then completed by giving the covariance of $y(\boldsymbol{x})$ evaluated at any two values of $\boldsymbol{x}$, which is given by the <strong>kernel function</strong>.</p>

\[\begin{aligned} \text{cov} = \mathbb{E}[y(\boldsymbol{x}_n)y(\boldsymbol{x}_m)] = k(y(\boldsymbol{x}_n, y(\boldsymbol{x}_m)) \end{aligned}\]
      </li>
    </ul>
  </li>
</ul>

<h3 id="gaussian-processes-for-regression"><span class="mr-2">Gaussian Processes for Regression</span><a href="#gaussian-processes-for-regression" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3>

<p>Let the prior on the regression function be a Gaussian Processes, denoted by:</p>

<p>\(\begin{aligned} f(\boldsymbol{x}) \sim \text{GP}(\mu(\boldsymbol{x}), k(\boldsymbol{x}, \boldsymbol{x}')) \end{aligned}\)</p>
<ul>
  <li>
    <p>where $\mu(\boldsymbol{x})$ is the mean function and $k(\boldsymbol{x}, \boldsymbol{x}’)$ is the kernel or covariance function, i.e.,</p>

\[\begin{aligned} \mu(\boldsymbol{x}) &amp;= \mathbb{E}[f(\boldsymbol{x})] \\ k(\boldsymbol{x}, \boldsymbol{x}') &amp;= \mathbb{E}\left[(f(\boldsymbol{x})-\mu(\boldsymbol{x}))(f(\boldsymbol{x}')-\mu(\boldsymbol{x}'))^T \right] \end{aligned}\]
  </li>
  <li>
    <p>Note: We obviously require that $k()$ be a positive definite kernel.</p>
  </li>
</ul>

<p>And then for any finite set of points, this process defines a joint Gaussian:</p>

\[\begin{aligned} p(\mathbf{f} \vert \mathbf{X}) = \mathcal{N}(\mathbf{f} \vert \boldsymbol{\mu}, \mathbf{K}) \end{aligned}\]

<ul>
  <li>
    <p>where $\mathbf{K}_{ij} = k(\boldsymbol{x}_i, \boldsymbol{x}_j)$ and $\boldsymbol{\mu} = (\mu(\boldsymbol{x}_1), . . . , \mu(\boldsymbol{x}_N))$.</p>
  </li>
  <li>
    <p>Note that it is common to use a mean function of $\mu(\boldsymbol{x})$ = 0, since the GP is flexible enough to model the mean arbitrarily well.</p>
  </li>
</ul>

<h4 id="predictions-using-noise-free-observations"><span class="mr-2">Predictions using noise-free observations</span><a href="#predictions-using-noise-free-observations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<ul>
  <li>Suppose we observe a training set $\mathbf{X}_N = {\boldsymbol{x}_1, …,\boldsymbol{x}_N }$, $\mathbf{t}_N = {f(\boldsymbol{x}_1),…,f(\boldsymbol{x}_N)}$, and this is the noise-free observation of the function evaluated at $\boldsymbol{x}_n$</li>
</ul>

\[\begin{aligned} p(\boldsymbol{t}) &amp;= p(\boldsymbol{f}) \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{K}) \end{aligned}\]

<p>The predictino problem</p>

<p>\(\begin{aligned} \binom{\mathbf{t}_N}{t_{N+1}} \sim \mathcal{N}\left(\binom{\boldsymbol{\mu}_N}{\mu_{N+1}}, \binom{\mathbf{K}_N\qquad \mathbf{a}}{\mathbf{a}^T \qquad k_{N+1}} \right) \end{aligned}\)</p>
<ul>
  <li>
    <p>where we have defined:</p>

\[\begin{aligned} \boldsymbol{\mu}_N &amp;= [\mu_1,...,\mu_N]^T \\ \mu_{N+1} &amp;= \mu(\boldsymbol{x}_{N+1}) = \mathbb{E}[f(\boldsymbol{x}_{N+1})] \\ \mathbf{a} &amp;= [k(\boldsymbol{x}_1,\boldsymbol{x}_{N+1}),...,k(\boldsymbol{x}_N,\boldsymbol{x}_{N+1})]^T \\ k_{N+1} &amp;= k(\boldsymbol{x}_{N+1}, \boldsymbol{x}_{N+1}) \end{aligned}\]
  </li>
</ul>

<p>then the posterior distribution:</p>

\[\begin{aligned} p(t_{N+1} \vert \mathbf{t}_N) &amp;= \mathcal{N}(t_{N+1} \vert m, \sigma^2) \\ \\ m &amp;= \mu_{N+1} + \mathbf{a}^T \mathbf{K}_N^{-1}(\mathbf{t}_N - \boldsymbol{\mu}_N) \\ \sigma^2 &amp;= k_{N+1} - \mathbf{a}^T\mathbf{K}_N^{-1}\mathbf{a} \end{aligned}\]

<h4 id="predictions-using-noise-observations"><span class="mr-2">Predictions using noise observations</span><a href="#predictions-using-noise-observations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

\[\begin{aligned} t_{n} = f(\boldsymbol{x}_n) + \varepsilon_n \end{aligned}\]

<ul>
  <li>here $\varepsilon_n$ is a random noisy variable, and we this is a noisy processes that have a Gaussian distribution $\varepsilon_n \sim \mathcal{N}(0, \beta^{-1}))$.</li>
</ul>

<p>then:</p>

\[\begin{aligned} p(\boldsymbol{f}) &amp;= \mathcal{N}(\boldsymbol{f}\vert 0, \mathbf{K}) \\ p(\boldsymbol{t} \vert \boldsymbol{f}) &amp;= \mathcal{N}(\boldsymbol{t} \vert \boldsymbol{f}, \beta^{-1}I) \\ \\ p(\boldsymbol{t}) &amp;= \mathcal{N}(\boldsymbol{t} \vert 0, \mathbf{K}+\beta^{-1}I) \end{aligned}\]

<p>then the posterior distribution:</p>

\[\begin{aligned} \mathbf{K}_N &amp;= \mathbf{K}_N + \beta^{-1}I\\ \\ p(t_{N+1} \vert \mathbf{t}_N) &amp;= \mathcal{N}(t_{N+1} \vert m, \sigma^2) \\ \\ m &amp;= \mathbf{a}^T \mathbf{K}_N^{-1}\mathbf{t}_N \\ \sigma^2 &amp;= k_{N+1} - \mathbf{a}^T\mathbf{K}_N^{-1}\mathbf{a} \end{aligned}\]

<h4 id="some-typical-kernel-functions-used-for-gaussian-processes"><span class="mr-2">Some typical Kernel Functions used for Gaussian Processes</span><a href="#some-typical-kernel-functions-used-for-gaussian-processes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<ul>
  <li>
    <p>Linear kernel</p>

\[\begin{aligned} k(x,x') = x^Tx' \end{aligned}\]

    <ul>
      <li>Note this kernel does not correspond to a stationary process</li>
    </ul>
  </li>
  <li>
    <p>Gaussian Kernel or Squared Exponential Kernel</p>

\[\begin{aligned} k(x,x') = \exp\left( -\frac {1}{2h^2}\|x-x'\|^2 \right) \end{aligned}\]

    <ul>
      <li>where $h$ determines the length scale of process. The smaller the value of $h$, the larger the “statistical” similarity (stronger correlation) of two points having a distance $d=|x-x’|^2$ apart</li>
    </ul>
  </li>
  <li>
    <p>Ornstein-Uhlenbeck Kernel</p>

\[\begin{aligned} k(x,x') = \exp\left( -\frac {1}{h}\|x-x'\| \right) \end{aligned}\]
  </li>
  <li>
    <p>Rational Quadratic Kernel</p>

\[\begin{aligned} k(x,x') = ( 1+\|x-x'\|^2)^{-\alpha}, \qquad \alpha \ge 0 \end{aligned}\]
  </li>
  <li>
    <p>Parameterized Kernel Functions</p>

\[\begin{aligned} k(x,x';\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x'\|^2 \right) \\ k(x,x';\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x'\|^2 \right) + \theta_3 \\ k(x,x';\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {\theta_2}{2}\|x-x'\|^2 \right) + \theta_3 + \theta_4 x^Tx' \\ k(x,x';\boldsymbol{\theta}) &amp;= \theta_1 \exp\left( -\frac {1}{2}(x-x')^TM(x-x') \right) \\ &amp;\space\space\vdots \end{aligned}\]
  </li>
</ul>

<p>For Parameterized Kernel Functions, we can use MLE to find the parameters given a data set. Also Bayesian inference can be used to find the posterior distribution of parameters, and then MAP estimation.</p>

<h4 id="another-topics"><span class="mr-2">Another topics</span><a href="#another-topics" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<ul>
  <li>Multiple kernel learning</li>
  <li>Semi-parametric</li>
  <li>Computational and numerical issues
……</li>
</ul>

<h3 id="gaussian-processes-for-classification"><span class="mr-2">Gaussian Processes for Classification</span><a href="#gaussian-processes-for-classification" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3>

<h4 id="binary-classification"><span class="mr-2">Binary Classification</span><a href="#binary-classification" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4>

<p>Assume $t_n \in (0,1)$, and let $a_n = f(\boldsymbol{x}_n)$</p>

<ul>
  <li>
    <p>then we define:</p>

\[\begin{aligned} p(t_n\vert a_n) = \sigma(a)^t(1-\sigma)^{1-t} \end{aligned}\]
  </li>
  <li>
    <p>$a_n$ is a Gaussian Processes</p>

\[\begin{aligned} p(\boldsymbol{a}) = \mathcal{N}(\boldsymbol{a}\vert \boldsymbol{\mu}, \mathbf{K}) \end{aligned}\]

    <ul>
      <li>But note that ${t_n}$ is not a gaussian processes</li>
      <li>
        <p>And we can introduce a noise-like term by a parameter $\nu$ that ensures that the covariance matrix $\mathbf{K}$  is positive definite.</p>

\[\begin{aligned} K_{nm} = k(\boldsymbol{x_n}, \boldsymbol{x}_m) + \nu\delta_{nm} \end{aligned}\]
      </li>
    </ul>
  </li>
  <li>
    <p>Unnormalized posterior:</p>

\[\begin{aligned} \tilde{p}(\boldsymbol{t}, \boldsymbol{a}) = \mathcal{N}(\boldsymbol{a}\vert \boldsymbol{\mu}, \mathbf{K})\prod_{n=1}^N p(t_n\vert a_n) \end{aligned}\]
  </li>
</ul>

<p>wighting for update…</p>


</div>

<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw mr-1"></i>
    
      <a href='/categories/machine-learning/'>Machine Learning</a>,
      <a href='/categories/stochastic-processes/'>Stochastic Processes</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw mr-1"></i>
      
      <a href="/tags/gaussian-processes/"
          class="post-tag no-text-decoration" >Gaussian Processes</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.

      
    </div>

    <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=Gaussian Processes - Candy Note&amp;url=http://0.0.0.0:4000/posts/gaussian_processes/" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=Gaussian Processes - Candy Note&amp;u=http://0.0.0.0:4000/posts/gaussian_processes/" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://t.me/share/url?url=http://0.0.0.0:4000/posts/gaussian_processes/&amp;text=Gaussian Processes - Candy Note" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i id="copy-link" class="fa-fw fas fa-link small"
        data-toggle="tooltip" data-placement="top"
        title="Copy link"
        data-title-succeed="Link copied successfully!">
    </i>

  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


      
    
    

    </div>
  </div> <!-- #core-wrapper -->

  <!-- pannel -->
  <div id="panel-wrapper" class="col-xl-3 pl-2 text-muted">

    <div class="access">
      















      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/gaussian-distribution/">Gaussian Distribution</a>
    
      
      <a class="post-tag" href="/tags/pca/">PCA</a>
    
      
      <a class="post-tag" href="/tags/em/">EM</a>
    
      
      <a class="post-tag" href="/tags/bayesian/">Bayesian</a>
    
      
      <a class="post-tag" href="/tags/markov-chain/">Markov Chain</a>
    
      
      <a class="post-tag" href="/tags/bernoulli/">Bernoulli</a>
    
      
      <a class="post-tag" href="/tags/binomial/">Binomial</a>
    
      
      <a class="post-tag" href="/tags/gamma-distribution/">Gamma Distribution</a>
    
      
      <a class="post-tag" href="/tags/gmm/">GMM</a>
    
      
      <a class="post-tag" href="/tags/kernel/">Kernel</a>
    

    </div>
  </div>


    </div>

    
      
      



<!-- BS-toc.js will be loaded at medium priority -->
<script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>

<div id="toc-wrapper" class="pl-0 pr-4 mb-5">
  <div class="panel-heading pl-3 pt-2 mb-2">Contents</div>
  <nav id="toc" data-toggle="toc"></nav>
</div>


    
  </div>

</div>

<!-- tail -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
      
        
        <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->






  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/posts/HMM/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1645401600"
    
    >
  2022-02-21
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Hidden Markov Model</h3>
            <div class="text-muted small">
              <p>
                





                Hidden Markov Model

Assume that all latent variables form a Markov chain, giving rise to the graphical structure, this is known as a state space model.

If the latent variables are discrete, then ...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/markov_chain/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1645401600"
    
    >
  2022-02-21
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Markov Chain</h3>
            <div class="text-muted small">
              <p>
                





                Markov Chain

A first-Order Markov Chain

[\begin{aligned} p(z^{(m+1)}\vert z^{(m)}, …,z^{(1)} )  = p(z^{(m+1)}\vert z^{(m)})\end{aligned}]

then given

  initial variable: $p(z^{(0)})$
  transitio...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/markov_chain_monte_carlo_methods/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/timeago.js
-->

<em class="timeago small"
    data-ts="1645401600"
    
    >
  2022-02-21
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Markov Chain Monte Carlo</h3>
            <div class="text-muted small">
              <p>
                





                Markov Chain Monte Carlo

As with rejection and importance sampling, we again sample from a proposal distribution.


  
    
      This time, however, we maintain a record of the current state $z(\...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->


      
        
        <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/posts/gaussian_mixture_model/" class="btn btn-outline-primary"
    prompt="Older">
    <p>Gaussian Mixture Model</p>
  </a>
  

  
  <a href="/posts/Bayesian_gmm_varinfer/" class="btn btn-outline-primary"
    prompt="Newer">
    <p>Bayesian Gaussian Mixture Model - Variational Inference</p>
  </a>
  

</div>

      
        
        <!--  The comments switcher -->


      
    </div>
  </div>
</div> <!-- .row -->



        <!--
  The Footer
-->

<footer class="d-flex w-100 justify-content-center">
  <div class="d-flex justify-content-between align-items-center text-muted">
    <div class="footer-left">
      <p class="mb-0">
        © 2022
        <a href="https://twitter.com/username">luo-songtao</a>.
        
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
        
      </p>
    </div>

    <div class="footer-right">
      <p class="mb-0">
        

        

        Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
         with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
         theme.

      </p>
    </div>

  </div> <!-- div.d-flex -->
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/gaussian-distribution/">Gaussian Distribution</a>
    
      
      <a class="post-tag" href="/tags/pca/">PCA</a>
    
      
      <a class="post-tag" href="/tags/em/">EM</a>
    
      
      <a class="post-tag" href="/tags/bayesian/">Bayesian</a>
    
      
      <a class="post-tag" href="/tags/markov-chain/">Markov Chain</a>
    
      
      <a class="post-tag" href="/tags/bernoulli/">Bernoulli</a>
    
      
      <a class="post-tag" href="/tags/binomial/">Binomial</a>
    
      
      <a class="post-tag" href="/tags/gamma-distribution/">Gamma Distribution</a>
    
      
      <a class="post-tag" href="/tags/gmm/">GMM</a>
    
      
      <a class="post-tag" href="/tags/kernel/">Kernel</a>
    

    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    
      <!--
  mermaid-js loader
-->

<script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script>

<script>
  $(function() {
    function updateMermaid(event) {
      if (event.source === window && event.data &&
            event.data.direction === ModeToggle.ID) {

        const mode = event.data.message;

        if (typeof mermaid === "undefined") {
          return;
        }

        let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    let initTheme = "default";

    if ($("html[data-mode=dark]").length > 0
      || ($("html[data-mode]").length == 0
        && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) {
      initTheme = "dark";
    }

    let mermaidConf = {
      theme: initTheme  /* <default|dark|forest|neutral> */
    };

    /* Markdown converts to HTML */
    $("pre").has("code.language-mermaid").each(function() {
      let svgCode = $(this).children().html();
      $(this).addClass("unloaded");
      $(this).after(`<div class=\"mermaid\">${svgCode}</div>`);
    });

    mermaid.initialize(mermaidConf);

    window.addEventListener("message", updateMermaid);
  });
</script>

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup & clipboard -->
  

  







  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script>







  

  

  







  
    

    

  



  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script>








<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
  /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
  MathJax = {
    tex: {
      inlineMath: [              /* start/end delimiter pairs for in-line math */
        ['$','$'],
        ['\\(','\\)']
      ],
      displayMath: [             /* start/end delimiter pairs for display math */
        ['$$', '$$'],
        ['\\[', '\\]']
      ]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>


<!-- commons -->

<script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script>




  </body>

</html>

