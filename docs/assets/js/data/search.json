[
  
  {
    "title": "LeetCode - Dynamic Programming",
    "url": "/posts/dynamic_programming/",
    "categories": "LeetCode, Dynamic Programming",
    "tags": "LeetCode, Dynamic Programming",
    "date": "2022-03-01 00:00:00 +0000",
    





    "snippet": "Dynamic ProgrammingLevel EasyLeetCode0053 maximum-subarrayint LeetCode0053::maxContiguousSubArraySum(vector&lt;int&gt;&amp; nums) {    /**     * Find largest sum subarray, but just return its sum.     * Note: A subarray is a `contiguous` part of an array     *      * https://leetcode-cn.com/problems/maximum-subarray/     *      * Examples:     * Input: nums = [-2,1,-3,4,-1,2,1,-5,4]     * Output: 6     * Explanation: [4,-1,2,1] has the largest sum = 6.     *      * Input: nums = [5,4,-1,7,8]     * Output: 23     *      * Follow up: If you have figured out the O(n) solution, try coding another solution     * using the divide and conquer approach, which is more subtle.     *      * Solutions:      *		let `mcsas = maxContiguousSubArraySum`     *		     *		transition formula:      *			mcsas(n) = max(mcsas(n-1), mcsas(n-1)+num[n])     *      *		Keypoint: 考虑连续子序列的中断，然后重置起点的问题     *			if sum(curSubArray) + nums[n] &gt; num[n]，则子序列保持连续，即 curSum += nums[n]     *			else，子序列中断，curSubArray=[num[n]]，即 curSum = nums[n]     *      * \\\\param nums Array numbers     * \\\\return maxsum of subarray     */    if (nums.size() == 0) { return NULL; }    if (nums.size() == 1) { return nums[0]; }    int maxSum = nums[0];    int curSum = maxSum;    for (int n = 1; n &lt; nums.size(); ++n) {        curSum = max(nums[n], curSum + nums[n]);        maxSum = max(curSum, maxSum);    }    return maxSum;}LeetCode0070 climbing-stairsint LeetCode0070::climbStairs(int n) {    /**     * You are climbing a staircase. It takes n steps to reach the top.     * Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?     *     * https://leetcode-cn.com/problems/climbing-stairs/     *      * Example:     *      * Input: n = 2     * Output: 2     * Explanation: There are two ways to climb to the top.     * 1. 1 step + 1 step     * 2. 2 steps     *      * Input: n = 3     * Output: 3     * Explanation: There are three ways to climb to the top.     * 1. 1 step + 1 step + 1 step     * 2. 1 step + 2 steps     * 3. 2 steps + 1 step     *      * Solutions:      *		transition formula: steps_n = steps_n_1 + steps_n_2     * .     */    int steps_n_1 = 1, steps_n_2 = 2, steps_n = 0;    if (n == 1) steps_n = steps_n_1;    if (n == 2) steps_n = steps_n_2;    for (int i = 3; i &lt;= n; ++i) {        steps_n = steps_n_1 + steps_n_2;        steps_n_1 = steps_n_2;        steps_n_2 = steps_n;    }    return steps_n;}LeetCode0118 pascals-trianglevector&lt;vector&lt;int&gt;&gt; LeetCode0118::generatePascalsTriangle(int numRows) {    /**      * Given an integer numRows, return the first numRows of Pascal&#39;s triangle. (即杨辉三角)     * In Pascal&#39;s triangle, each number is the sum of the two numbers directly above it     *      * https://leetcode-cn.com/problems/pascals-triangle/     *      * Example:     * Input: numRows = 5     * Output: [[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1]]     *      * Solutions:     *		Boundary: n=1, array2D=[[1]]     *		Transition Formula:      *			for i in 0~n-1:     *				if i==0: array2D[n][0] = 1;      *				if 0&lt;i&lt;n-1: array2D[n][i] = array2D[n-1][i] + array2D[n-1][i-1]     *				if i== n-1: array2D[n][n-1] = 1     *			     * \\\\param numRows     * \\\\return All Rows     */    vector&lt;vector&lt;int&gt;&gt; array2D(numRows);    for (int n = 0; n &lt; numRows; ++n) {        array2D[n].resize(n + 1);        array2D[n][0] = 1;        for (int i = 1; i &lt; n; ++i) {            array2D[n][i] = array2D[n - 1][i] + array2D[n - 1][i-1];        }        array2D[n][n] = 1;    }    return array2D; }LeetCode0119 pascals-triangle-iivector&lt;int&gt; LeetCode0119::generatePascalsTriangleIIandGetLastRow(int rowIndex) {    /**     * Return the last row of PascalsTriangle given rowIndex     *      * Same as LeetCode0118 to generate Pascals&#39; Triangle, but this time, we do not need save all rows,      * only need to retain (n-1)-th row and update n-th row     *      * https://leetcode-cn.com/problems/pascals-triangle-ii/     *      * Solutions:     *		Transition Formula: curRow[i] = preRow[i] + preRow[i - 1], forall i in [1,n-1]     *      * \\\\param rowIndex     * \\\\return      */    vector&lt;int&gt; preRow;    vector&lt;int&gt; curRow;    for (int n = 0; n &lt;= rowIndex; ++n) {        for (int i = 1; i &lt; n; ++i) {            curRow[i] = preRow[i] + preRow[i - 1];        }        curRow.push_back(1);        preRow = curRow;    }    return curRow;}LeetCode0119 pascals-triangle-ii使用组合数学方式求解 LeetCode0119vector&lt;int&gt; LeetCode0119::generatePascalsTriangleIIandGetLastRow2(int rowIndex) {    /**     * Return the last row of PascalsTriangle given rowIndex     *      * Solution: Use a property of Combinatorics:      *			     *		C(n,m) = n!/(n-m)!  = C(n,m-1) * (n-m+1)/m     *      * \\\\param rowIndex     * \\\\return The last row of PascalsTriangle given rowIndex     */    vector&lt;int&gt; row(rowIndex+1);    row[0] = 1;    for (int m = 1; m &lt;= rowIndex; ++m) {        // using 1LL to avoid overflow problem in multiplying and dividing        row[m] = 1LL * row[m - 1] * (1LL * rowIndex - m + 1) / m;        }    return row;}LeetCode0121 best-time-to-buy-and-sell-stockint LeetCode0121::maxProfit(vector&lt;int&gt;&amp; prices) {    /**     * You are given an array prices where prices[i] is the price of a given stock on the ith day     * You want to maximize your profit by choosing a single day to buy one stock and choosing      * a different day in the future to sell that stock.     *      * Return the maximum profit you can achieve from this transaction.      * If you cannot achieve any profit, return 0.     *      * https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/     *      * Input: prices = [7,1,5,3,6,4]     * Output: 5     * Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.     * Note that buying on day 2 and selling on day 1 is not allowed because you must buy before you sell.     *      * Solutions:      *		Transition Formula: maxProfit(n) = max(maxProfit(n), maxProfit(n-1))     *		     *		Keypoint:      *			if prices[i] - min(prices[0:i-1]) &gt; maximumProfit,      *			then update maximumProfit, and buyIndex     *      * \\\\param prices : an array prices      * \\\\return maximum profit     */    int buyIndex = 0;    int minimumPriceIndex = 0;    int maximumProfit = 0;    int curProfit = 0;    for (int i = 1; i &lt; prices.size(); ++i) {        // record the min price of prices[0:i+1]        if (prices[minimumPriceIndex] &gt; prices[i] &amp;&amp; prices[buyIndex] &gt; prices[i]) {            minimumPriceIndex = i;        }        // Update max profit if exists smaller one        curProfit = prices[i] - prices[minimumPriceIndex];        if (maximumProfit &lt; curProfit) {            maximumProfit = curProfit;            buyIndex = minimumPriceIndex;        }    }    return maximumProfit;}LeetCode0338 counting-bitsvector&lt;int&gt; LeetCode0338::countBits(int n) {    /**     * Given an integer n, return an array ans of length n + 1 such that for each i (0 &lt;= i &lt;= n),      * ans[i] is the number of 1&#39;s in the binary representation of i.     *      * https://leetcode-cn.com/problems/counting-bits/     *      * Input: n = 2     * Output: [0,1,1]     * Explanation:     *	0 --&gt; 0     *	1 --&gt; 1     *	2 --&gt; 10     *      * Solutions:      *		&#39;highestBit&#39; Transition Formula:      *			if n &amp; (n-1) == 0, then n can be a &#39;highestBit&#39; number; i.e. &#39;highestBit&#39; \\\\in \\\\{1,2,4,8,16,......\\\\}     *				&#39;highestBit&#39; number has only one bit is &#39;1&#39; at its highest bit.     *				For example:     *					bin(2)=0b10     *					bin(16)=0b10000     *			     *			countBits[n] = countBits[n-highestBit] + 1     *				For example:      *					bin(6)=0b101, its highestBit is bin(4)=0b100, then countBits[6] = countBits[6-4]+1 = 2     *					bin(15)=0b1111, its highestBit is bin(8)=0b1000, then countBits[15] = countBits[15-8]+1 = 4     *				therefor, once we have known the &#39;highestBit&#39; of number &#39;n&#39; , we can obtain its 1-bits count in O(1)     *      *		&#39;lowestBit&#39; Transition Formula: using &#39;&gt;&gt;&#39; remove lowest bit, then we have: countBits[n] = countBits[n&gt;&gt;1] + (n&amp;1)     *			if &#39;n&#39; is egg (lowest bit is 0), then countBits[n] = countBits[n/2] = countBits[n&gt;&gt;1]     *				6&gt;&gt;1 = 3;    countBit[6] = countBits[3] = 2     *				14&gt;&gt;1 = 7;   countBit[14] = countBits[7] = 3     *			if &#39;n&#39; is odd (lowest bit is 1), then countBits[n] = countBits[(n-1)/2] + 1 = countBits[n&gt;&gt;1] + 1     *				7&gt;&gt;1 = 3;	countBit[7] = countBits[3] + 1 = 3     *				15&gt;&gt;1 = 7;   countBit[15] = countBits[7] + 1 = 4     *			n &amp; 1 = 0 or 1     *	     *		&#39;lowestBitof1&#39; Transition Formula:      *			using the property of operator &#39;n&amp;(n-1)&#39;,  we can remove the n&#39;s lowest bit of 1,      *			and then: conuntBits[n] = countBits[n&amp;(n-1)]+1     *				For example:     *					5 &amp; 4 = 4, i.e. 0b101 &amp; 0b100 = 0b100, then conuntBits[5] = countBits[5&amp;4]+1 = 2     *					8 &amp; 7 = 0, i.e. 0b1000 &amp; 0b0111 = 0b0, then conuntBits[8] = countBits[8&amp;7]+1 = 1     *      * \\\\param n     * \\\\return      */    vector&lt;int&gt; counts(n + 1);    for (int i = 1; i &lt;= n; ++i) {        // using &#39;lowestBitof1&#39; Transition Formula        counts[i] = counts[i &amp; (i-1)] + 1;    }    return counts;    }LeetCode0392 is-subsequencebool LeetCode0392::isSubsequence(string s, string t) {    /**     * Given two strings s and t, return true if s is a subsequence of t, or false otherwise.     *      * A subsequence of a string is a new string that is formed from the original string      * by deleting some (can be none) of the characters without disturbing the relative      * positions of the remaining characters. (i.e., &quot;ace&quot; is a subsequence of &quot;abcde&quot; while &quot;aec&quot; is not).     *      * s and t consist only of lowercase English letters     *      * https://leetcode-cn.com/problems/is-subsequence/     *      * Example:     * Input: s = &quot;abc&quot;, t = &quot;ahbgdc&quot;     * Output: true     *      * Input: s = &quot;axc&quot;, t = &quot;ahbgdc&quot;     * Output: false     *      * Follow up: Suppose there are lots of incoming s, say s1, s2, ..., sk where k &gt;= `10^9`,      * and you want to check one by one to see if t has its subsequence.      * In this scenario, how would you change your code?     *      * Solutions:     *		Normal Solution: double pointer     *			while i&lt;s.size &amp;&amp; j &lt;t.size:     *				if s[i] == t[j]:     *					i++      *				j++     *			return true if i == s.size, else false     *		     *		(DP)Transition Formula:     *			Let M = t.size     *			     *			Using a `M+1 x 26` matrix , named by &#39;P&#39;, P[i,j] indicates the first occurrence of      *			the letter &#39;x&#39; in t.     *			     *			If some letter &#39;x&#39; not in t, then we simply let P[i,&#39;x&#39;] = M, else P[i,&#39;x&#39;] should be some numnber in [0,M-1].     *			     *			for i in M~0: if t[i] == &#39;x&#39;, P[i,&#39;x&#39;] = i else P[i,&#39;x&#39;] = P[i+1,&#39;x&#39;]     *      *			Note: here &#39;x&#39; represents the index of some letter in array [a,b,c,d,......,x,y,z],      *						for example, if currentletter is &#39;c&#39; then &#39;x&#39; = 2     *      * \\\\param s      * \\\\param t     * \\\\return      */    // Method1: Double Pointer    /*int sLen = s.size(), tLen = t.size();    int sIdx = 0, tIdx = 0;    while (sIdx &lt; sLen &amp;&amp; tIdx &lt; tLen) {        if (s[sIdx] == t[tIdx]) sIdx++;        tIdx++;    }    return sIdx == sLen;*/    // Method2: DP solution. This method works for the scenario described in &quot;Follow up&quot;    int M = t.size();    // 1. initialize a `M+1 x 26` dimension Matrix, and fill it with 0     vector&lt;vector &lt;int&gt;&gt; P(M + 1, vector&lt;int&gt;(26, 0));    // 2. initlialize the numbers of row P[M+1]  with &quot;M&quot;    for (int j = 0; j &lt; 26; ++j) {        P[M][j] = M;    }    // 3. update matrix P respect to target string `t`    for (int i = M - 1; i &gt;= 0; --i) {        for (int j = 0; j &lt; 26; ++j) {            if (t[i] == j + &#39;a&#39;)                 P[i][j] = i;            else                 P[i][j] = P[i + 1][j];        }    }    // 4. search if string `s` is a non-contiguous subsequence of `t` according Matrix P    int rowIdx = 0;    for (int n = 0; n &lt; s.size(); ++n) {        if (P[rowIdx][s[n] - &#39;a&#39;] == M) {            return false;        }        rowIdx = P[rowIdx][s[n] - &#39;a&#39;] + 1;    }    return true;}LeetCode0509 fibonacci-numberint LeetCode0509::fibonacci(int n) {    /**     * The Fibonacci numbers, commonly denoted F(n) form a sequence, called the Fibonacci      * sequence, such that each number is the sum of the two preceding ones, starting      * from 0 and 1. That is,     *		     *		F(0) = 0, F(1) = 1     *		F(n) = F(n - 1) + F(n - 2), for n &gt; 1.     *      * https://leetcode-cn.com/problems/fibonacci-number/     *      * \\\\param n     * \\\\return      */    // Recursion version    /*    if (n&lt;=1) return n;    return fibonacci(n - 1) + fibonacci(n - 2);    */    // DP version: bottom up    if (n&lt;=1) return n;    int fnMinus1 = 0, fnMinus2 = 1;    int fn = fnMinus1;    for (int i = 2; i &lt;= n; ++i) {        fn = fnMinus1 + fnMinus2;        fnMinus1 = fnMinus2;        fnMinus2 = fn;    }    return fn;}LeetCode0746 min-cost-climbing-stairsint LeetCode0746::minCostClimbingStairs(vector&lt;int&gt;&amp; cost) {    /**     * You are given an integer array cost where cost[i] is the cost of ith step on a staircase.     * Once you pay the cost, you can either climb one or two steps.     *      * You can either start from the step with index 0, or the step with index 1.     *      * Return the minimum cost to reach the top of the floor.     *      * 2 &lt;= cost.length      *      * https://leetcode-cn.com/problems/min-cost-climbing-stairs/     *      * Example:     * Input: cost = [10,15,20]     * Output: 15     * Explanation: You will start at index 1.     * - Pay 15 and climb two steps to reach the top.     * The total cost is 15.     *      * Input: cost = [1,100,1,1,1,100,1,1,100,1]     * Output: 6     * Explanation: You will start at index 0.     * - Pay 1 and climb two steps to reach index 2.     * - Pay 1 and climb two steps to reach index 4.     * - Pay 1 and climb two steps to reach index 6.     * - Pay 1 and climb one step to reach index 7.     * - Pay 1 and climb two steps to reach index 9.     * - Pay 1 and climb one step to reach the top.     * The total cost is 6.     *      * Solutions:      *		Transition Formula:     *			totalCost[n] = min(totalCost[n-1] + cost[n-1], totalCost[n-2]+cost[n-2])     *      *      Note: in this problem, the finall result of n will equal cost.size.     *		     *		I think this is not a well defined problem.     *      * \\\\param cost     * \\\\return      */    int n = cost.size();    int costIMinus1 = 0, costIMinus2 = 0;    int costI = 0;    for (int i = 2; i &lt;= n; ++i) {        costI = min(costIMinus1 + cost[i - 1], costIMinus2 + cost[i - 2]);        costIMinus2 = costIMinus1;        costIMinus1 = costI;    }    return costI;}LeetCode1025 divisor-gamebool LeetCode1025::divisorGame(int n) {    /**     * Alice and Bob take turns playing a game, with Alice starting first.     *      * Initially, there is a number n on the chalkboard.     * On each player&#39;s turn, that player makes a move consisting of:     *	- Choosing any x with 0 &lt; x &lt; n and n % x == 0.     *	- Replacing the number n on the chalkboard with n - x.     *      * Also, if a player cannot make a move, they lose the game.     *      * Return true if and only if Alice wins the game, assuming both players play optimally.     *      * https://leetcode-cn.com/problems/divisor-game/     *      * Example:     * Input: n = 2     * Output: true     * Explanation: Alice chooses 1, and Bob has no more moves.     *      * Input: n = 3     * Output: false     * Explanation: Alice chooses 1, Bob chooses 1, and Alice has no more moves.     *      * Solutions:      *		https://leetcode-cn.com/problems/divisor-game/solution/chu-shu-bo-yi-by-leetcode-solution/     *	     *		This problem is so boring.      *		结论是：n 为奇数的时候 Alice（先手）必败，n 为偶数的时候 Alice 必胜     *		The optimal result is &#39;n%2==0&#39; ......     *		转换成DP问题：     *      * \\\\param n     * \\\\return      */    return n % 2 == 0;}LeetCode1137 n-th-tribonacci-numberint LeetCode1137::tribonacci(int n) {    /**     * The Tribonacci sequence Tn is defined as follows:      * T0 = 0, T1 = 1, T2 = 1, and Tn+3 = Tn + Tn+1 + Tn+2 for n &gt;= 0.     * Given n, return the value of Tn     *      * https://leetcode-cn.com/problems/n-th-tribonacci-number/     *      * Example:     * Input: n = 4     * Output: 4     * Explanation:     * T_3 = 0 + 1 + 1 = 2     * T_4 = 1 + 1 + 2 = 4     *      * Input: n = 25     * Output: 1389537     *      *      * \\\\param n     * \\\\return      */    int t0 = 0, t1 = 1, t2 = 1;    if (n == 0) return 0;    if (n == 1) return 1;    if (n == 1) return 1;    int tn = 1;    for (int i = 3; i &lt;= n; i++) {        tn = t2 + t1 + t0;        t0 = t1;        t1 = t2;        t2 = tn;    }    return tn;}LeetCode1646 get-maximum-in-generated-arrayint LeetCode1646::getMaximumGenerated(int n) {    /**     * You are given an integer n. A 0-indexed integer array nums of length n + 1 is      * generated in the following way:     * - nums[0] = 0     * - nums[1] = 1     * - nums[2 * i] = nums[i] when 2 &lt;= 2 * i &lt;= n     * - nums[2 * i + 1] = nums[i] + nums[i + 1] when 2 &lt;= 2 * i + 1 &lt;= n     * Return the maximum integer in the array nums​​​.     *      * https://leetcode-cn.com/problems/get-maximum-in-generated-array/     *      * Example:     * Input: n = 7     * Output: 3     * Explanation: According to the given rules:     * - nums[0] = 0     * - nums[1] = 1     * - nums[(1 * 2) = 2] = nums[1] = 1     * - nums[(1 * 2) + 1 = 3] = nums[1] + nums[2] = 1 + 1 = 2     * - nums[(2 * 2) = 4] = nums[2] = 1     * - nums[(2 * 2) + 1 = 5] = nums[2] + nums[3] = 1 + 2 = 3     * - nums[(3 * 2) = 6] = nums[3] = 2     * - nums[(3 * 2) + 1 = 7] = nums[3] + nums[4] = 2 + 1 = 3     * Hence, nums = [0,1,1,2,1,3,2,3], and      * the maximum is max(0,1,1,2,1,3,2,3) = 3     *      * Input: n = 2     * Output: 1     * Explanation: According to the given rules, nums = [0,1,1].      * The maximum is max(0,1,1) = 1     *      * Input: n = 3     * Output: 2     * Explanation: According to the given rules, nums = [0,1,1,2].      * The maximum is max(0,1,1,2) = 2     *      *      *      * \\\\param n     * \\\\return      */    if (n &lt;= 1) return n;    vector&lt;int&gt; nums(n + 1);    nums[1] = 1;    int maxNum = nums[1];    for (int j = 2; j &lt;= n; j++) {        nums[j] = nums[j / 2] + nums[j / 2 + 1] * (j % 2);        maxNum = max(maxNum, nums[j]);    }    return maxNum;}Level MediumLeetCode0005 longest-palindromic-substringstring LeetCode0005::longestPalindrome(string s) {    /**     * Given a string s, return the longest palindromic substring in s..     *      * https://leetcode-cn.com/problems/longest-palindromic-substring/     *      * 1 &lt;= s.length &lt;= 1000     * s consist of only digits and English letters     *      * Example:     * Input: s = &quot;babad&quot;     * Output: &quot;bab&quot;     * Explanation: &quot;aba&quot; is also a valid answer     *      * Input: s = &quot;cbbd&quot;     * Output: &quot;bb&quot;     *      * Solutions:     *		DP:      *		     *      * \\\\param s     * \\\\return      */    int N = s.size();    vector&lt;vector&lt;int&gt;&gt; P(N, vector&lt;int&gt;(N, 0));    vector&lt;int&gt; longestPd(2);    longestPd[1] = 1;    for (int i = 0; i &lt; N; ++i) {        P[i][i] = 1;        if (i + 1 &lt; N &amp;&amp; s[i] == s[i + 1]) {            P[i][i+1] = 1;            if (longestPd[1] &lt; 2) {                longestPd[0] = i;                longestPd[1] = 2;            }        }    }        for (int l = 3; l &lt;= N; ++l) {        for (int i = 0; i &lt; N-l+1; ++i) {            if (s[i] == s[i + l - 1] &amp;&amp; P[i+1][i + l - 2] == 1) {                P[i][i + l - 1] = 1;                if (longestPd[1] &lt; l) {                    longestPd[0] = i;                    longestPd[1] = l;                }            }        }    }    return s.substr(longestPd[0], longestPd[1]);}"
  },
  
  {
    "title": "Variational Inference",
    "url": "/posts/variational_inference/",
    "categories": "Machine Learning, Variational Inference",
    "tags": "",
    "date": "2022-02-21 23:00:00 +0000",
    





    "snippet": "Variational InferenceIn Bayesian Learning, when the involved integrations  are no longer computationally tractable. Then Variational Approximation can be used.Although there is nothing intrinsically approximate about variational methods, they do naturally lend themselves to finding approximate solutions.  This is done by restricting the range of functions over which the optimization is performed, forinstance by considering only quadratic functions or by considering functions composed of a linear combination of fixed basis functions in which only the coefficients of the linear combination can vary.  In the case of applications to probabilistic inference, the restriction may for example take the form of factorization assumptionsLower Bound of Variational ApproximationJust like the Lower bound analysis of EM, but now $Z$ contains all latent variables and paramaters, because now all parameters are stochastic variables.\\\\[\\\\begin{aligned} \\\\ln p(\\\\mathbf{X}) = \\\\mathcal{L}(q) + \\\\text{KL}(q\\\\| p) \\\\end{aligned}\\\\]      where we have defined:\\\\[\\\\begin{aligned} \\\\mathcal{L}(q) &amp;= \\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z}) \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{X},\\\\mathbf{Z} )}{q(\\\\mathbf{Z})} \\\\right\\\\} \\\\\\\\ \\\\text{KL}(q\\\\|p) &amp;= -\\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z}) \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X})}{q(\\\\mathbf{Z})} \\\\right\\\\}  \\\\end{aligned}\\\\]    As before, we can maximize the lower bound $\\\\mathcal{L}(q)$ by optimization with respect to the distribution $q(\\\\mathbf{Z})$, which is equivalent to minimizing the KL divergence.  If we allow any possible choice for $q(\\\\mathbf{Z})$, then the maximum of the lower bound occurs when the KL divergence vanishes, which occurs when $q(\\\\mathbf{Z})$ equals the posterior distribution $p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X})$.  However, we shall suppose the model is such that working with the true posterior distribution is intractable.  We therefore consider instead a restricted family of distributions $q(\\\\mathbf{Z})$ and then seek the member of this family for which the KL divergence is minimized.          Our goal is to restrict the family sufficiently that they comprise only tractable distributions, while at the same time allowing the family to be sufficiently rich and flexible that it can provide a good approximation to the true posterior distribution.      It is important to emphasize that the restriction is imposed purely to achieve tractability, and that subject to this requirement we should use as rich a family of approximating distributions as possible.      In particular, there is no ‘over-fitting’ associated with highly flexible distributions.      Using more flexible approximations simply allows us to approach the true posterior distribution more closely.                  One way to restrict the family of approximating distributions is to use a parametric distribution q(Z      ω) governed by a set of parameters ω. The lower bound L(q) then becomes a function of ω, and we can exploit standard nonlinear optimization techniques to determine the optimal values for the parameters.      Factorized distributions (Mean Field Approximation)  Suppose we partition the elements of $Z$ into disjoint groups that we denote by $Z_i$ where $i = 1, . . . , M$.\\\\[\\\\begin{aligned} q(\\\\boldsymbol{Z}) = \\\\prod_{i=1}^M q_i(\\\\boldsymbol{Z}_i) \\\\end{aligned}\\\\]Amongst all distributions $q(\\\\boldsymbol{Z})$ having this form ,we now seek that distribution for which the lower bound $\\\\mathcal{L}(q)$ is largest.      First, in the lower bound, we first dissect out the dependence on one of the factors $q_j(\\\\boldsymbol{Z}_j)$\\\\[\\\\begin{aligned} \\\\mathcal{L}(q) &amp;= \\\\int q(\\\\mathbf{Z}) \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{X},\\\\mathbf{Z} )}{q(\\\\mathbf{Z})} \\\\right\\\\} d\\\\mathbf{Z} \\\\\\\\ &amp;= \\\\int \\\\prod_i q_i\\\\left\\\\{ \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z} ) - \\\\sum_i \\\\ln q_i \\\\right\\\\}d\\\\mathbf{Z} \\\\qquad \\\\text{assume} \\\\space \\\\int q_jdZ_j = 1  \\\\\\\\&amp;= \\\\int q_j\\\\left\\\\{ \\\\int \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z} ) \\\\prod_{i\\\\neq j}q_i d\\\\boldsymbol{Z}_i \\\\right\\\\}d\\\\boldsymbol{Z}_j - \\\\int q_j\\\\ln q_j d\\\\boldsymbol{Z}_j + \\\\text{const} \\\\\\\\ &amp;= \\\\int q_j \\\\ln \\\\tilde{p}(\\\\boldsymbol{X},\\\\boldsymbol{Z}_j)d\\\\boldsymbol{Z}_j - \\\\int q_j \\\\ln q_j d\\\\boldsymbol{Z}_j + \\\\text{const} \\\\\\\\&amp;= -\\\\text{KL}(q_j\\\\| \\\\tilde{p}) + \\\\text{const} \\\\\\\\ \\\\\\\\  \\\\ln \\\\tilde{p}(\\\\boldsymbol{X},\\\\boldsymbol{Z}_j) &amp;= \\\\mathbb{E}_{i\\\\neq j} [\\\\ln p(\\\\boldsymbol{X},\\\\boldsymbol{Z})] +\\\\text{const} \\\\\\\\ &amp;= \\\\int \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z} ) \\\\prod_{i\\\\neq j}q_i d\\\\boldsymbol{Z}_i + \\\\text{const} \\\\end{aligned}\\\\]                  thus maximizing $\\\\mathcal{L}(q)$ is equivalent to minimizing $\\\\text{KL}(q_j| \\\\tilde{p})$, this leads to:\\\\[\\\\begin{aligned} \\\\ln \\\\hat{q}_j(\\\\boldsymbol{Z}_j) =  \\\\mathbb{E}_{i\\\\neq j} [\\\\ln p(\\\\boldsymbol{X},\\\\boldsymbol{Z})] +\\\\text{const} \\\\\\\\ \\\\\\\\ \\\\hat{q}_j(\\\\boldsymbol{Z}_j) = \\\\frac {\\\\exp(\\\\mathbb{E}_{i\\\\neq j} [\\\\ln p(\\\\boldsymbol{X},\\\\boldsymbol{Z})])}{\\\\int \\\\exp(\\\\mathbb{E}_{i\\\\neq j} [\\\\ln p(\\\\boldsymbol{X},\\\\boldsymbol{Z})])d\\\\boldsymbol{Z}_j} \\\\end{aligned}\\\\]            Difference of Minimizing $\\\\text{KL}(q|p)$ and $\\\\text{KL}(p|q)$      Minimizing $\\\\text{KL}(q|p)$ leads to distributions q(Z) that avoid regions in which $p(Z)$ is small.        Conversely, the Kullback-Leibler divergence $\\\\text{KL}(p|q)$ is minimized by distributions $q(Z)$ that are nonzero in regions where $p(Z)$ is nonzero.  By contrast, if we were to minimize $\\\\text{KL}(p|q)$, the resulting approximations would average across all of the modes and, in the context of the mixture model, would lead to poor predictive distributions (because the average of two good parameter values is typically itself not a good parameter value)."
  },
  
  {
    "title": "Expectation Maximization",
    "url": "/posts/expectation_maximization/",
    "categories": "Machine Learning, Expectation Maximization",
    "tags": "EM",
    "date": "2022-02-21 23:00:00 +0000",
    





    "snippet": "Expectation MaximizationEM Lower Bound analysis      The expectation Maximization algorithm, or EM aoglrithm, is a general technique for finding maximum likelihood solutoins for probabilistic models having latent variables.    Consider a probabilistic model in which we collectively denote:          all of the observed variables by $\\\\mathbf{X}$      all of the hidden variables by $\\\\mathbf{Z}$      the joint distribution $p(\\\\mathbf{X}, \\\\mathbf{Z}\\\\mid \\\\boldsymbol{\\\\theta})$ is governed by a set of parameters denoted $\\\\boldsymbol{\\\\theta}$.        then our goal is  to maximize the likelihood function that is given by:\\\\[\\\\begin{aligned} p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}) =\\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) \\\\end{aligned}\\\\]          here we are assuming $\\\\mathbf{Z}$ is discrete, although the discussion is identical if $\\\\mathbf{Z}$ comprises continuous variables or a combination of discrete and continuous variables, with summation replaced by integration as appropriate            We shall suppose that direct optimization of $p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta})$ is difficult, but that optimization of the complete-data likelihood function $p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta})$ is significantly easier        Next we introduce a distribution $q(\\\\mathbf{Z})$ defined over the latent variables, and we observe that, for any choice of $q(\\\\mathbf{Z})$, the following decomposition holds.\\\\[\\\\begin{aligned} \\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}) = \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}) + \\\\text{KL}(q\\\\|p) \\\\end{aligned}\\\\]                  where we have defined:\\\\[\\\\begin{aligned} \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}) &amp;= \\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z}) \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta})}{q(\\\\mathbf{Z})} \\\\right\\\\} \\\\\\\\ \\\\text{KL}(q\\\\|p) &amp;= -\\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z}) \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta})}{q(\\\\mathbf{Z})} \\\\right\\\\}  \\\\end{aligned}\\\\]                    Proof:\\\\[\\\\begin{aligned} p(\\\\mathbf{X}, \\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) &amp;= p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta})p(\\\\mathbf{X}\\\\mid \\\\boldsymbol{\\\\theta}) \\\\\\\\ \\\\ln p(\\\\mathbf{X}\\\\mid \\\\boldsymbol{\\\\theta}) &amp;= \\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) -  \\\\ln p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}) \\\\\\\\ \\\\ln p(\\\\mathbf{X}\\\\mid \\\\boldsymbol{\\\\theta})\\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z}) &amp;= \\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z})\\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) -  \\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z})\\\\ln p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}) \\\\\\\\ .&amp;..... \\\\end{aligned}\\\\]                  And we can see that $\\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta})$ is a lower bound on $\\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta})$\\\\[\\\\begin{aligned} \\\\because \\\\space&amp; \\\\text{KL}(q\\\\|p) \\\\ge 0 \\\\\\\\ \\\\therefore \\\\space &amp; \\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}) \\\\ge \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}) \\\\\\\\ &amp; \\\\text{if} \\\\space q(\\\\mathbf{Z}) = p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}) \\\\\\\\ &amp; \\\\text{then}\\\\space \\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}) = \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta})  \\\\end{aligned}\\\\]    Step 1 (E step): fix $\\\\boldsymbol{\\\\theta}=\\\\boldsymbol{\\\\theta}^{old}$ , update $q(\\\\mathbf{Z})$          then $\\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}^{old})$ is fixed.      but if $q(\\\\mathbf{Z}) = p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})$, then $\\\\text{KL}(q| p)$ vanishes, then this leads to maximize the lower bound to $\\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}^{old})$.      Then we have $\\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}^{old}) = \\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}^{old})$.        Step 2 (M step): fix $q(\\\\mathbf{Z})=p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})$, update $\\\\boldsymbol{\\\\theta}$          let $\\\\boldsymbol{\\\\theta}^{new} = \\\\arg\\\\max_{\\\\boldsymbol{\\\\theta}} \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta})$      if $\\\\boldsymbol{\\\\theta}^{new} \\\\neq \\\\boldsymbol{\\\\theta}^{old}$, then $\\\\text{KL}(q| p)&gt;0$ and $\\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}^{new})\\\\ge \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta}^{old})$      then we have $\\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}^{new}) &gt; \\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta}^{old})$            Note: at the $E$ step: we have set $q(\\\\mathbf{Z}) = p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})$, then\\\\[\\\\begin{aligned} \\\\mathcal{L}(q, \\\\boldsymbol{\\\\theta} ) &amp;= \\\\sum_{\\\\mathbf{Z}} q(\\\\mathbf{Z}) \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta})}{q(\\\\mathbf{Z})} \\\\right\\\\} \\\\\\\\ &amp;= \\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})  \\\\ln \\\\left\\\\{ \\\\frac {p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta})}{p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old}) } \\\\right\\\\} \\\\\\\\ &amp;=  \\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})  \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) +\\\\text{const} \\\\\\\\ &amp;= \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) + \\\\text{const} \\\\end{aligned}\\\\]  EM in General Describe  So EM algorithm can be described as:          In the E step:                  we firstly use the current parameter values $\\\\boldsymbol{\\\\theta}^{old}$ to find the posterior distribution of the latent variables given by $p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})$                      and then use this posterior to find the expectation of the complete-data log likelihood evaluated for some general parameter value $\\\\boldsymbol{\\\\theta}$\\\\[\\\\begin{aligned} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) = \\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})  \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) \\\\end{aligned}\\\\]                              In the M step:                              we determine the revised parameter estimate $\\\\boldsymbol{\\\\theta}^{new}$ by maximizing this function\\\\[\\\\begin{aligned} \\\\boldsymbol{\\\\theta}^{new} = \\\\arg \\\\max_{\\\\boldsymbol{\\\\theta}} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) \\\\end{aligned}\\\\]                                Now, we have seen why each cycle of EM will increase the incomplete-data log likelihood $\\\\ln p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\theta})$, unless it is already at a local maximum."
  },
  
  {
    "title": "Bayesian Gaussian Mixture Model - Variational Inference",
    "url": "/posts/Bayesian_gmm_varinfer/",
    "categories": "Machine Learning, Variational Inference",
    "tags": "Bayesian, GMM",
    "date": "2022-02-21 23:00:00 +0000",
    





    "snippet": "Bayesian Gaussian Mixture Model - Variational InferenceLikelihood Funcitons\\\\[\\\\begin{aligned} p(\\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\pi}) &amp;= \\\\prod_{n=1}^N\\\\prod_{k=1}^K \\\\pi_k^{z_{nk}} \\\\\\\\ \\\\\\\\ p(\\\\mathbf{X}\\\\vert \\\\mathbf{Z}, \\\\boldsymbol{\\\\mu}, \\\\Lambda) &amp;= \\\\prod_{n=1}^N\\\\prod_{k=1}^K \\\\mathcal{N}(\\\\boldsymbol{x}_n \\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k^{-1})^{z_{nk}} \\\\end{aligned}\\\\]Conjugate Priors\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\pi}) &amp;= \\\\text{Dir}(\\\\boldsymbol{\\\\pi} \\\\vert \\\\boldsymbol{\\\\alpha}_0) = \\\\frac {\\\\Gamma(\\\\sum_{k=1}^K \\\\alpha_k)}{\\\\prod_{k=1}^K \\\\Gamma(\\\\alpha_k)} \\\\prod_{k=1}^K \\\\pi_k^{\\\\alpha_k - 1} \\\\\\\\ \\\\\\\\ p(\\\\boldsymbol{\\\\mu}, \\\\Lambda) &amp;= p(\\\\boldsymbol{\\\\mu} \\\\vert \\\\Lambda) p( \\\\Lambda) \\\\\\\\ &amp;= \\\\prod_{k=1}^K \\\\mathcal{N}(\\\\boldsymbol{\\\\mu}_k\\\\vert \\\\boldsymbol{m}_k, (\\\\beta_k\\\\Lambda_k)^{-1}) \\\\mathcal{W}(\\\\Lambda_k\\\\vert W_k, \\\\nu_k) \\\\end{aligned}\\\\]  Where we have introduced an independent Gaussian-Wishart prior governing the mean and precision of each Gaussian component.      $\\\\boldsymbol{\\\\alpha}_0 = [\\\\alpha_1,\\\\alpha_2,…,\\\\alpha_K ]$        The Joint distribution\\\\[\\\\begin{aligned} p(\\\\mathbf{X}, \\\\mathbf{Z}, \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda) = p(\\\\mathbf{X}\\\\vert \\\\mathbf{Z}, \\\\boldsymbol{\\\\mu}, \\\\Lambda)p(\\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\pi})p(\\\\boldsymbol{\\\\pi})p(\\\\boldsymbol{\\\\mu} \\\\vert \\\\Lambda) p( \\\\Lambda)  \\\\end{aligned}\\\\]  Variational Distribution$\\\\mathbf{Z}$      First, we consider $\\\\mathbf{Z}$: factorizes between the latent variables and the parameters so that:\\\\[\\\\begin{aligned} q(\\\\mathbf{Z}, \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda) = q(\\\\mathbf{Z}) q(\\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda) \\\\end{aligned}\\\\]                  and then:  according to the Variational Approximation\\\\[\\\\begin{aligned} \\\\ln \\\\hat{q}(\\\\mathbf{Z}) &amp;= \\\\mathbb{E}_{\\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda}[\\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z}, \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda)] + \\\\text{const} \\\\\\\\ \\\\\\\\ &amp;= \\\\mathbb{E}_{\\\\boldsymbol{\\\\pi}}[\\\\ln p(\\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\pi})] + \\\\mathbb{E}_{\\\\boldsymbol{\\\\mu}, \\\\Lambda}[\\\\ln p(\\\\mathbf{X}\\\\vert \\\\mathbf{Z}, \\\\boldsymbol{\\\\mu}, \\\\Lambda)] + \\\\text{const} \\\\\\\\ \\\\\\\\ &amp;= \\\\sum_{n=1}^N\\\\sum_{k=1}^K z_{nk} \\\\ln \\\\rho_{nk} + \\\\text{const} \\\\\\\\\\\\\\\\ \\\\ln \\\\rho_{nk} &amp;= \\\\mathbb{E}[\\\\ln \\\\pi_k] + \\\\frac 12 \\\\left\\\\{ \\\\mathbb{E}[\\\\ln \\\\vert \\\\Lambda_k \\\\vert] - D\\\\ln (2\\\\pi) - \\\\mathbb{E}_{\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k}[(\\\\boldsymbol{x}_n - \\\\boldsymbol{\\\\mu}_k)^T\\\\Lambda_k(\\\\boldsymbol{x}_n - \\\\boldsymbol{\\\\mu}_k)] \\\\right\\\\} \\\\end{aligned}\\\\]                    so we can obtain:\\\\[\\\\begin{aligned} \\\\hat{q}(\\\\mathbf{Z}) &amp;\\\\propto \\\\prod_{n=1}^N \\\\prod_{k=1}^K \\\\rho_{nk}^{z_{nk}} \\\\\\\\\\\\\\\\ \\\\hat{q}(\\\\mathbf{Z}) &amp;= \\\\prod_{n=1}^N \\\\prod_{k=1}^K \\\\gamma_{nk}^{z_{nk}}  \\\\\\\\ \\\\gamma_{nk} &amp;= \\\\frac {1}{\\\\sum_{j=1}^K  \\\\rho_{nj}}  \\\\rho_{nk} \\\\end{aligned}\\\\]                    we see, it has the same form as the prior $p(\\\\mathbf{Z} \\\\vert \\\\boldsymbol{\\\\pi})$, and\\\\[\\\\begin{aligned} \\\\mathbb{E}[z_{nk}] = \\\\hat{q}(z_{nk} = 1) = \\\\gamma_{nk} \\\\end{aligned}\\\\]            $\\\\boldsymbol{\\\\pi}$      second, we consider the factor $q(\\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda)$\\\\[\\\\begin{aligned} \\\\ln \\\\hat{q}(\\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda) &amp;= \\\\mathbb{E}_{\\\\mathbf{Z}}[\\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z}, \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda)] + \\\\text{const} \\\\\\\\ &amp;= \\\\ln p(\\\\boldsymbol{\\\\pi}) + \\\\mathbb{E}_{\\\\mathbf{Z}}[\\\\ln p(\\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\pi})] + \\\\sum_{k=1}^K \\\\ln p(\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k) + \\\\sum_{k=1}^K \\\\sum_{n=1}^N \\\\mathbb{E}[z_{nk}]\\\\ln \\\\mathcal{N}(\\\\boldsymbol{x}_n \\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k^{-1}) \\\\end{aligned}\\\\]                  this leads to the further factorization:\\\\[\\\\begin{aligned} q(\\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Lambda) = q(\\\\boldsymbol{\\\\pi}) \\\\sum_{k=1}^K q(\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k) \\\\end{aligned}\\\\]                    then:\\\\[\\\\begin{aligned} \\\\ln \\\\hat{q}(\\\\boldsymbol{\\\\pi}) &amp;= \\\\ln p(\\\\boldsymbol{\\\\pi}) + \\\\mathbb{E}_{\\\\mathbf{Z}}[\\\\ln p(\\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\pi})] + \\\\text{const} \\\\\\\\&amp;= \\\\prod_{k=1}^K (a_k-1)\\\\ln \\\\pi_k + \\\\sum_{k=1}^K\\\\sum_{n=1}^N \\\\gamma_{nk}\\\\ln \\\\pi_k + \\\\text{const} \\\\\\\\ &amp;= \\\\sum_{k=1}^K \\\\left\\\\{a_k-1 + \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\right\\\\} \\\\ln\\\\pi_k + \\\\text{const} \\\\\\\\ \\\\\\\\ \\\\hat{q}(\\\\boldsymbol{\\\\pi}) &amp;= \\\\text{Dir}(\\\\boldsymbol{\\\\pi}\\\\vert \\\\boldsymbol{\\\\alpha}_N) \\\\\\\\ \\\\alpha_{Nk} &amp;= a_k + \\\\sum_{n=1}^N \\\\gamma_{nk}\\\\end{aligned}\\\\]            also, we see $\\\\hat{q}(\\\\boldsymbol{\\\\pi})$ has same form as the prior $p(\\\\boldsymbol{\\\\pi})$              the mean of $\\\\pi_k$:\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\pi_k] = \\\\frac {\\\\alpha_{Nk}}{\\\\sum_{k=1}^K \\\\alpha_{Nk} } = \\\\frac {a_k + \\\\sum_{n=1}^N \\\\gamma_{nk}}{N + \\\\sum_k \\\\alpha_k} \\\\end{aligned}\\\\]            $\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k$      Finally, the variational posterior distribution $\\\\hat{q}(\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k)$ doen not factorize into the product of the marginals, then\\\\[\\\\begin{aligned} \\\\ln \\\\hat{q}(\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k) &amp;= \\\\ln p(\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k) + \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\ln \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k^{-1}) +\\\\text{const} \\\\\\\\&amp;= \\\\ln \\\\mathcal{N}(\\\\boldsymbol{\\\\mu}_k\\\\vert \\\\boldsymbol{m}_k, (\\\\beta_k\\\\Lambda_k)^{-1}) + \\\\ln \\\\mathcal{W}(\\\\Lambda_k\\\\vert W_k, \\\\nu_k) + \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\ln \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k^{-1}) +\\\\text{const} \\\\end{aligned}\\\\]                  We Know:\\\\[\\\\begin{aligned} \\\\hat{q}(\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k) &amp;= \\\\hat{q}(\\\\boldsymbol{\\\\mu}_k \\\\vert \\\\Lambda_k)  \\\\hat{q}(\\\\Lambda_k) \\\\\\\\  &amp;= \\\\mathcal{N}(\\\\boldsymbol{\\\\mu}_{Nk}\\\\vert \\\\boldsymbol{m}_{Nk}, (\\\\beta_{Nk}\\\\Lambda_{Nk})^{-1}) \\\\mathcal{W}(\\\\Lambda_{Nk}\\\\vert W_{Nk}, \\\\nu_{Nk})  \\\\end{aligned}\\\\]                              then we have:\\\\[\\\\begin{aligned} \\\\beta_{Nk} &amp;= \\\\beta_k + N_k\\\\\\\\ \\\\boldsymbol{\\\\mu}_{Nk} &amp;= \\\\frac {1}{\\\\beta_{Nk}} (\\\\beta_k \\\\boldsymbol{m}_k + N_k \\\\overline\\\\boldsymbol{x}_k) \\\\\\\\ W_{Nk}^{-1} &amp;= W_k^{-1} + N_kS_k + \\\\frac {\\\\beta_kN_k}{beta_k + N_k}(\\\\overline\\\\boldsymbol{x}_k-\\\\boldsymbol{m}_k)(\\\\overline\\\\boldsymbol{x}_k-\\\\boldsymbol{m}_k)^T \\\\\\\\ \\\\nu_{Nk} &amp;= \\\\nu_k + N_k \\\\end{aligned}\\\\]                                where we have defined:\\\\[\\\\begin{aligned} N_k &amp;= \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\\\\\ \\\\overline\\\\boldsymbol{x}_k &amp;= \\\\frac 1{N_k} \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\boldsymbol{x}_n \\\\\\\\ S_k &amp;= \\\\frac 1{N_k} \\\\sum_{n=1}^N \\\\gamma_{nk}(\\\\boldsymbol{x}_n-\\\\overline\\\\boldsymbol{x}_k)(\\\\boldsymbol{x}_n-\\\\overline\\\\boldsymbol{x}_k)^T \\\\end{aligned}\\\\]                              Evaluate $\\\\ln \\\\rho_{nk}$\\\\[\\\\begin{aligned} \\\\ln \\\\rho_{nk} &amp;= \\\\mathbb{E}[\\\\ln \\\\pi_k] + \\\\frac 12 \\\\left\\\\{ \\\\mathbb{E}[\\\\ln \\\\vert \\\\Lambda_k \\\\vert] - D\\\\ln (2\\\\pi) - \\\\mathbb{E}_{\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k}[(\\\\boldsymbol{x}_n - \\\\boldsymbol{\\\\mu}_k)^T\\\\Lambda_k(\\\\boldsymbol{x}_n - \\\\boldsymbol{\\\\mu}_k)] \\\\right\\\\} \\\\end{aligned}\\\\]      $\\\\mathbb{E}[\\\\ln \\\\pi_k]$\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\ln \\\\pi_k] &amp;= \\\\psi(\\\\alpha_{Nk}) - \\\\psi\\\\left(\\\\sum_{k=1}^K \\\\alpha_{Nk}\\\\right) \\\\\\\\ \\\\mathbb{E}[\\\\vert \\\\Lambda_k \\\\vert ] &amp;= \\\\sum_{i=1}^D\\\\psi\\\\left(\\\\frac {\\\\nu_{Nk} + 1 -i }{2} + D\\\\ln 2 + \\\\ln \\\\vert W_{Nk} \\\\vert \\\\right) \\\\\\\\ \\\\mathbb{E}_{\\\\boldsymbol{\\\\mu}_k, \\\\Lambda_k}[(\\\\boldsymbol{x}_n&amp; - \\\\boldsymbol{\\\\mu}_k)^T\\\\Lambda_k(\\\\boldsymbol{x}_n - \\\\boldsymbol{\\\\mu}_k)] \\\\\\\\ &amp;=  D\\\\beta_{Nk}^{-1} + \\\\nu_{Nk}(\\\\boldsymbol{x}_n - \\\\boldsymbol{m}_{Nk})^TW_{Nk}(\\\\boldsymbol{x}_n - \\\\boldsymbol{m}_{Nk})   \\\\end{aligned}\\\\]          Where $\\\\psi$ is the digamma function      Variational Lower Boundto be continue"
  },
  
  {
    "title": "Gaussian Processes",
    "url": "/posts/gaussian_processes/",
    "categories": "Machine Learning, Stochastic Processes",
    "tags": "Gaussian Processes",
    "date": "2022-02-21 01:00:00 +0000",
    





    "snippet": "Gaussian ProcessesIn the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly.  At first sight, it might seem difficult to work with a distribution over the uncountably infinite space of functions.  However, as we shall see, for a finite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points, and so in practice we can work in a finite space.DefinitionIn general, a Gaussian process is defined as a probability distribution over functions $y(\\\\boldsymbol{x})$ such that the set of values of $y(\\\\boldsymbol{x})$ evaluated at an arbitrary set of points $\\\\boldsymbol{x}_1, . . . , \\\\boldsymbol{x}_N$ jointly have a Gaussian distribution.  More generally, a stochastic process $y(\\\\boldsymbol{x})$ is specified by giving the joint probability distribution for any finite set of values $y(\\\\boldsymbol{x}_1), . . . , y(\\\\boldsymbol{x}_N)$ in a consistent manner.A key point about Gaussian stochastic processes is that the joint distribution over $N $ variables $y_1, . . . , y_N$ is specified completely by the second-order statistics, namely the mean and the covariance.  Mean:          In most applications, we will not have any prior knowledge about the mean of $y(\\\\boldsymbol{x})$ and so by symmetry we take it to be zero. This is equivalent to choosing the mean of the prior over weight values $p(w|α)$ tobe zero in the basis function viewpoint.        Covariance                  The specification of the Gaussian process is then completed by giving the covariance of $y(\\\\boldsymbol{x})$ evaluated at any two values of $\\\\boldsymbol{x}$, which is given by the kernel function.\\\\[\\\\begin{aligned} \\\\text{cov} = \\\\mathbb{E}[y(\\\\boldsymbol{x}_n)y(\\\\boldsymbol{x}_m)] = k(y(\\\\boldsymbol{x}_n, y(\\\\boldsymbol{x}_m)) \\\\end{aligned}\\\\]            Gaussian Processes for RegressionLet the prior on the regression function be a Gaussian Processes, denoted by:\\\\(\\\\begin{aligned} f(\\\\boldsymbol{x}) \\\\sim \\\\text{GP}(\\\\mu(\\\\boldsymbol{x}), k(\\\\boldsymbol{x}, \\\\boldsymbol{x}&#39;)) \\\\end{aligned}\\\\)      where $\\\\mu(\\\\boldsymbol{x})$ is the mean function and $k(\\\\boldsymbol{x}, \\\\boldsymbol{x}’)$ is the kernel or covariance function, i.e.,\\\\[\\\\begin{aligned} \\\\mu(\\\\boldsymbol{x}) &amp;= \\\\mathbb{E}[f(\\\\boldsymbol{x})] \\\\\\\\ k(\\\\boldsymbol{x}, \\\\boldsymbol{x}&#39;) &amp;= \\\\mathbb{E}\\\\left[(f(\\\\boldsymbol{x})-\\\\mu(\\\\boldsymbol{x}))(f(\\\\boldsymbol{x}&#39;)-\\\\mu(\\\\boldsymbol{x}&#39;))^T \\\\right] \\\\end{aligned}\\\\]        Note: We obviously require that $k()$ be a positive definite kernel.  And then for any finite set of points, this process defines a joint Gaussian:\\\\[\\\\begin{aligned} p(\\\\mathbf{f} \\\\vert \\\\mathbf{X}) = \\\\mathcal{N}(\\\\mathbf{f} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{K}) \\\\end{aligned}\\\\]      where $\\\\mathbf{K}_{ij} = k(\\\\boldsymbol{x}_i, \\\\boldsymbol{x}_j)$ and $\\\\boldsymbol{\\\\mu} = (\\\\mu(\\\\boldsymbol{x}_1), . . . , \\\\mu(\\\\boldsymbol{x}_N))$.        Note that it is common to use a mean function of $\\\\mu(\\\\boldsymbol{x})$ = 0, since the GP is flexible enough to model the mean arbitrarily well.  Predictions using noise-free observations  Suppose we observe a training set $\\\\mathbf{X}_N = {\\\\boldsymbol{x}_1, …,\\\\boldsymbol{x}_N }$, $\\\\mathbf{t}_N = {f(\\\\boldsymbol{x}_1),…,f(\\\\boldsymbol{x}_N)}$, and this is the noise-free observation of the function evaluated at $\\\\boldsymbol{x}_n$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{t}) &amp;= p(\\\\boldsymbol{f}) \\\\sim \\\\mathcal{N}(\\\\boldsymbol{\\\\mu}, \\\\mathbf{K}) \\\\end{aligned}\\\\]The predictino problem\\\\(\\\\begin{aligned} \\\\binom{\\\\mathbf{t}_N}{t_{N+1}} \\\\sim \\\\mathcal{N}\\\\left(\\\\binom{\\\\boldsymbol{\\\\mu}_N}{\\\\mu_{N+1}}, \\\\binom{\\\\mathbf{K}_N\\\\qquad \\\\mathbf{a}}{\\\\mathbf{a}^T \\\\qquad k_{N+1}} \\\\right) \\\\end{aligned}\\\\)      where we have defined:\\\\[\\\\begin{aligned} \\\\boldsymbol{\\\\mu}_N &amp;= [\\\\mu_1,...,\\\\mu_N]^T \\\\\\\\ \\\\mu_{N+1} &amp;= \\\\mu(\\\\boldsymbol{x}_{N+1}) = \\\\mathbb{E}[f(\\\\boldsymbol{x}_{N+1})] \\\\\\\\ \\\\mathbf{a} &amp;= [k(\\\\boldsymbol{x}_1,\\\\boldsymbol{x}_{N+1}),...,k(\\\\boldsymbol{x}_N,\\\\boldsymbol{x}_{N+1})]^T \\\\\\\\ k_{N+1} &amp;= k(\\\\boldsymbol{x}_{N+1}, \\\\boldsymbol{x}_{N+1}) \\\\end{aligned}\\\\]  then the posterior distribution:\\\\[\\\\begin{aligned} p(t_{N+1} \\\\vert \\\\mathbf{t}_N) &amp;= \\\\mathcal{N}(t_{N+1} \\\\vert m, \\\\sigma^2) \\\\\\\\ \\\\\\\\ m &amp;= \\\\mu_{N+1} + \\\\mathbf{a}^T \\\\mathbf{K}_N^{-1}(\\\\mathbf{t}_N - \\\\boldsymbol{\\\\mu}_N) \\\\\\\\ \\\\sigma^2 &amp;= k_{N+1} - \\\\mathbf{a}^T\\\\mathbf{K}_N^{-1}\\\\mathbf{a} \\\\end{aligned}\\\\]Predictions using noise observations\\\\[\\\\begin{aligned} t_{n} = f(\\\\boldsymbol{x}_n) + \\\\varepsilon_n \\\\end{aligned}\\\\]  here $\\\\varepsilon_n$ is a random noisy variable, and we this is a noisy processes that have a Gaussian distribution $\\\\varepsilon_n \\\\sim \\\\mathcal{N}(0, \\\\beta^{-1}))$.then:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{f}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{f}\\\\vert 0, \\\\mathbf{K}) \\\\\\\\ p(\\\\boldsymbol{t} \\\\vert \\\\boldsymbol{f}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{t} \\\\vert \\\\boldsymbol{f}, \\\\beta^{-1}I) \\\\\\\\ \\\\\\\\ p(\\\\boldsymbol{t}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{t} \\\\vert 0, \\\\mathbf{K}+\\\\beta^{-1}I) \\\\end{aligned}\\\\]then the posterior distribution:\\\\[\\\\begin{aligned} \\\\mathbf{K}_N &amp;= \\\\mathbf{K}_N + \\\\beta^{-1}I\\\\\\\\ \\\\\\\\ p(t_{N+1} \\\\vert \\\\mathbf{t}_N) &amp;= \\\\mathcal{N}(t_{N+1} \\\\vert m, \\\\sigma^2) \\\\\\\\ \\\\\\\\ m &amp;= \\\\mathbf{a}^T \\\\mathbf{K}_N^{-1}\\\\mathbf{t}_N \\\\\\\\ \\\\sigma^2 &amp;= k_{N+1} - \\\\mathbf{a}^T\\\\mathbf{K}_N^{-1}\\\\mathbf{a} \\\\end{aligned}\\\\]Some typical Kernel Functions used for Gaussian Processes      Linear kernel\\\\[\\\\begin{aligned} k(x,x&#39;) = x^Tx&#39; \\\\end{aligned}\\\\]          Note this kernel does not correspond to a stationary process            Gaussian Kernel or Squared Exponential Kernel\\\\[\\\\begin{aligned} k(x,x&#39;) = \\\\exp\\\\left( -\\\\frac {1}{2h^2}\\\\|x-x&#39;\\\\|^2 \\\\right) \\\\end{aligned}\\\\]          where $h$ determines the length scale of process. The smaller the value of $h$, the larger the “statistical” similarity (stronger correlation) of two points having a distance $d=|x-x’|^2$ apart            Ornstein-Uhlenbeck Kernel\\\\[\\\\begin{aligned} k(x,x&#39;) = \\\\exp\\\\left( -\\\\frac {1}{h}\\\\|x-x&#39;\\\\| \\\\right) \\\\end{aligned}\\\\]        Rational Quadratic Kernel\\\\[\\\\begin{aligned} k(x,x&#39;) = ( 1+\\\\|x-x&#39;\\\\|^2)^{-\\\\alpha}, \\\\qquad \\\\alpha \\\\ge 0 \\\\end{aligned}\\\\]        Parameterized Kernel Functions\\\\[\\\\begin{aligned} k(x,x&#39;;\\\\boldsymbol{\\\\theta}) &amp;= \\\\theta_1 \\\\exp\\\\left( -\\\\frac {\\\\theta_2}{2}\\\\|x-x&#39;\\\\|^2 \\\\right) \\\\\\\\ k(x,x&#39;;\\\\boldsymbol{\\\\theta}) &amp;= \\\\theta_1 \\\\exp\\\\left( -\\\\frac {\\\\theta_2}{2}\\\\|x-x&#39;\\\\|^2 \\\\right) + \\\\theta_3 \\\\\\\\ k(x,x&#39;;\\\\boldsymbol{\\\\theta}) &amp;= \\\\theta_1 \\\\exp\\\\left( -\\\\frac {\\\\theta_2}{2}\\\\|x-x&#39;\\\\|^2 \\\\right) + \\\\theta_3 + \\\\theta_4 x^Tx&#39; \\\\\\\\ k(x,x&#39;;\\\\boldsymbol{\\\\theta}) &amp;= \\\\theta_1 \\\\exp\\\\left( -\\\\frac {1}{2}(x-x&#39;)^TM(x-x&#39;) \\\\right) \\\\\\\\ &amp;\\\\space\\\\space\\\\vdots \\\\end{aligned}\\\\]  For Parameterized Kernel Functions, we can use MLE to find the parameters given a data set. Also Bayesian inference can be used to find the posterior distribution of parameters, and then MAP estimation.Another topics  Multiple kernel learning  Semi-parametric  Computational and numerical issues……Gaussian Processes for ClassificationBinary ClassificationAssume $t_n \\\\in (0,1)$, and let $a_n = f(\\\\boldsymbol{x}_n)$      then we define:\\\\[\\\\begin{aligned} p(t_n\\\\vert a_n) = \\\\sigma(a)^t(1-\\\\sigma)^{1-t} \\\\end{aligned}\\\\]        $a_n$ is a Gaussian Processes\\\\[\\\\begin{aligned} p(\\\\boldsymbol{a}) = \\\\mathcal{N}(\\\\boldsymbol{a}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{K}) \\\\end{aligned}\\\\]          But note that ${t_n}$ is not a gaussian processes              And we can introduce a noise-like term by a parameter $\\\\nu$ that ensures that the covariance matrix $\\\\mathbf{K}$  is positive definite.\\\\[\\\\begin{aligned} K_{nm} = k(\\\\boldsymbol{x_n}, \\\\boldsymbol{x}_m) + \\\\nu\\\\delta_{nm} \\\\end{aligned}\\\\]                  Unnormalized posterior:\\\\[\\\\begin{aligned} \\\\tilde{p}(\\\\boldsymbol{t}, \\\\boldsymbol{a}) = \\\\mathcal{N}(\\\\boldsymbol{a}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{K})\\\\prod_{n=1}^N p(t_n\\\\vert a_n) \\\\end{aligned}\\\\]  wighting for update…"
  },
  
  {
    "title": "Gaussian Mixture Model",
    "url": "/posts/gaussian_mixture_model/",
    "categories": "Machine Learning, Mixture Model",
    "tags": "GMM",
    "date": "2022-02-21 01:00:00 +0000",
    





    "snippet": "Gaussian Mixture ModelDefiniteA simple linear superposition of multiple Gaussian would give a better characterization of multimodal data set.General Form of GMM\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) &amp;= \\\\sum_{k=1}^K \\\\pi_k \\\\mathcal{N}(\\\\boldsymbol{x}\\\\mid \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k) \\\\\\\\ \\\\\\\\ \\\\sum_{k=1}^K \\\\pi_k &amp;= 1 \\\\end{aligned}\\\\]Or we can describe GMM as follows:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z}) &amp;= \\\\text{Cate}(\\\\boldsymbol{z} \\\\vert \\\\boldsymbol{\\\\pi}) = \\\\prod_{k=1}^K \\\\pi_k^{z_k} \\\\qquad \\\\qquad z_k \\\\in \\\\{0,1\\\\} \\\\\\\\ p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{z}) &amp;= \\\\prod_{k=1}^K \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)^{z_k} \\\\end{aligned}\\\\]      Union distribution of $\\\\boldsymbol{x}, \\\\boldsymbol{z}$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}, \\\\boldsymbol{z}) &amp;= p(\\\\boldsymbol{z})p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{z}) \\\\\\\\ &amp;= \\\\prod_{k=1}^K \\\\left[\\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)\\\\right]^{z_k} \\\\end{aligned}\\\\]        Marginal distribution of $\\\\boldsymbol{x}$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) &amp;= \\\\sum_{\\\\boldsymbol{z}} p(\\\\boldsymbol{z})p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{z}) \\\\\\\\ &amp;= \\\\sum_{\\\\boldsymbol{z}} \\\\prod_{k=1}^K \\\\left[\\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)\\\\right]^{z_k} \\\\\\\\ &amp;= \\\\sum_{k=1}^K \\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)  \\\\end{aligned}\\\\]        Conditional distribution of $\\\\boldsymbol{z} \\\\mid \\\\boldsymbol{x}$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z} \\\\vert \\\\boldsymbol{x}) &amp;= \\\\frac {p(\\\\boldsymbol{x}, \\\\boldsymbol{z})}{p(\\\\boldsymbol{x})} \\\\\\\\ &amp;= \\\\frac {\\\\prod_{k=1}^K [\\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)]^{z_{k}}}{\\\\sum_{j=1}^K \\\\pi_j\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_j, \\\\Sigma_j)} \\\\\\\\ p(z_k=1 \\\\vert \\\\boldsymbol{x}) &amp;= \\\\frac {p(\\\\boldsymbol{x}, z_k=1)}{p(\\\\boldsymbol{x})} \\\\\\\\ &amp;= \\\\frac {\\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)}{\\\\sum_{j=1}^K \\\\pi_j\\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_j, \\\\Sigma_j)} \\\\end{aligned}\\\\]  Collapse Problem in GMM  if the variance of someone component in GMM goes to zero, then the value of that term would goes to infinityLikelihood Function      Incomplete-data likelihood\\\\[\\\\begin{aligned} p(\\\\mathbf{X} \\\\mid \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Sigma) = \\\\prod_{n=1}^N \\\\sum_{k=1}^K \\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k) \\\\end{aligned}\\\\]        Complete-data likelihood\\\\[\\\\begin{aligned} p(\\\\mathbf{X}, \\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Sigma) &amp;=  \\\\prod_{n=1}^N\\\\prod_{k=1}^K \\\\left[\\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)\\\\right]^{z_{nk}} \\\\end{aligned}\\\\]        If we directly work on maximum likelihood, we cannot get closed form solution. Then we can use gradient method, but EM method will be more effecient.  EM for GMM      Posterior of Latent Variable:\\\\[\\\\begin{aligned} p(z_{nk} = 1\\\\vert \\\\boldsymbol{x}_{n}, \\\\boldsymbol{\\\\theta}^{old}) = \\\\gamma_{nk} = \\\\frac {\\\\pi_k\\\\mathcal{N}(\\\\boldsymbol{x}_{n}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)}{\\\\sum_{j=1}^K \\\\pi_j\\\\mathcal{N}(\\\\boldsymbol{x}_{n}\\\\vert \\\\boldsymbol{\\\\mu}_j, \\\\Sigma_j)} \\\\end{aligned}\\\\]        $\\\\mathcal{Q}$ 函数\\\\[\\\\begin{aligned} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) &amp;= \\\\mathbb{E}_{\\\\boldsymbol{z}}\\\\left[\\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\pi}, \\\\boldsymbol{\\\\mu}, \\\\Sigma)  \\\\right]\\\\\\\\&amp;=\\\\sum_{n=1}^N\\\\sum_{k=1}^K \\\\left[\\\\ln \\\\pi_k + \\\\ln \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)\\\\right] \\\\mathbb{E}_{\\\\boldsymbol{z}}[z_{nk}] \\\\\\\\ &amp;= \\\\sum_{n=1}^N\\\\sum_{k=1}^K \\\\left[\\\\ln \\\\pi_k + \\\\ln \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)\\\\right] p(z_{nk} = 1\\\\vert \\\\boldsymbol{x}, \\\\boldsymbol{\\\\theta}^{old}) \\\\\\\\ &amp;= \\\\sum_{n=1}^N\\\\sum_{k=1}^K \\\\left[\\\\ln \\\\pi_k + \\\\ln \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma_k)\\\\right]\\\\gamma_{nk}  \\\\end{aligned}\\\\]        then:\\\\[\\\\begin{aligned} \\\\pi_k^{new} &amp;= \\\\arg\\\\max_{\\\\pi} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) + \\\\lambda \\\\left(\\\\sum_{k=1}^K \\\\pi_k-1\\\\right) \\\\\\\\ \\\\pi_k^{new} &amp;= -\\\\frac 1\\\\lambda \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\\\\\ \\\\sum^K  \\\\pi_k^{new} &amp;= -\\\\frac 1\\\\lambda \\\\sum_{n=1}^N \\\\sum^K \\\\gamma_{nk} = 1 \\\\\\\\ \\\\lambda &amp;= -N \\\\\\\\ \\\\pi_k^{new} &amp;= \\\\frac 1N \\\\sum_{n=1}^N \\\\gamma_{nk} \\\\\\\\\\\\\\\\\\\\\\\\  \\\\boldsymbol{\\\\mu_k}^{new} &amp;= \\\\arg\\\\max_{\\\\boldsymbol{\\\\mu_k}} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) \\\\\\\\ \\\\boldsymbol{\\\\mu_k}^{new} &amp;=  \\\\frac {\\\\sum_{n=1}^N \\\\gamma_{nk} \\\\boldsymbol{x}_n}{\\\\sum_{n=1}^N \\\\gamma_{nk}} \\\\\\\\\\\\\\\\\\\\\\\\ \\\\Sigma_k^{new} &amp;= \\\\arg\\\\max_{\\\\Sigma_k} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) \\\\\\\\ \\\\Sigma_k^{new} &amp;= \\\\frac {\\\\sum_{n=1}^N \\\\gamma_{nk}(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu_k}^{new})(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu_k}^{new})^T}{\\\\sum_{n=1}^N \\\\gamma_{nk}} \\\\end{aligned}\\\\]  "
  },
  
  {
    "title": "Sampling Methods",
    "url": "/posts/sampling_methods/",
    "categories": "Machine Learning, Sampling Methods",
    "tags": "Monte Carlo",
    "date": "2022-02-21 00:00:00 +0000",
    





    "snippet": "Sampling MethodsAncestral Sampling ApproachDirected Graph\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z}) = \\\\prod_{i}^M p(\\\\boldsymbol{z} \\\\vert pa_i) \\\\end{aligned}\\\\]            make one pass through the set of variables in the order $z_1, . . . , z_M$ sampling from the conditional distributions $p(\\\\boldsymbol{z}_i      pa_i)$      if some of the nodes are instantiated with observed values:  logic sampling approach: which can be seen as a special case of importance sampling          At each step, when a sampled value is obtained for a variable zi whose value is observed, the sampled value is compared to the observed value, and if they agree then the sample value is retained and the algorithm proceeds to the next variable in turn. However, if the sampled value and the observed value disagree, then the whole sample so far is discarded and the algorithm starts again with the first node in the graph.      UDG: Gibbs samplingFor marginonal: If we already have a strategy for sampling from a joint distribution $p(u, v)$, then it is straightforward to obtain samples from the marginal distribution $p(u)$ simply by ignoring the values for $v$ in each sample.Inverse function通过分布函数逆函数的解析形式，使用均匀分布随机数求解。同时利用：\\\\[p(y) = p(z) \\\\left \\\\vert \\\\frac {dz}{dy} \\\\right\\\\vert\\\\]  exponential distribution  Cauchy distributionSampling from Gaussian使用专属抽样算法  Box-muller  Marsaglia Polar methodRejection Sampling\\\\[\\\\begin{aligned} p(z) = \\\\frac {1}{Z_p} \\\\tilde{p}(z) \\\\end{aligned}\\\\]  proposal distribution $q(z)$  $kq(z) \\\\ge p(z)$\\\\[\\\\begin{aligned} &amp;q(z) \\\\rightarrow z_0 \\\\\\\\ &amp; u_0 = \\\\text{random}(0,kq(z_0)) \\\\\\\\ &amp; \\\\text{if } u_0 &gt; \\\\tilde p(z_0), \\\\text{then accept} \\\\\\\\\\\\\\\\ p(\\\\text{accept}) &amp;= \\\\int \\\\frac {\\\\tilde p(z)}{kq(z)} q(z) dz = \\\\frac 1k \\\\int \\\\tilde p(z) dz \\\\end{aligned}\\\\]the fraction of points that are rejected by this method depends on the ratio of the area under the unnormalized distribution $p(z)$ to the area under the curve $kq(z)$.Adaptive Rejection Samplingwhen difficult to determine a suitable analytic form for the envelope distribution $q(z)$Construction of an envelope function is particularly straightforward for cases in which p(z) is log concave(梯度持续下降的类型)The function $ln p(z)$ and its gradient are evaluated at some initial set of grid points, and the intersections of the resulting tangent lines are used to construct the envelope function.其他变种，用于处理梯度不是持续下降的类型Rejection Sampling 对高维采样的困难：高维下，拒绝率将会非常高 （直观解释：因为高维中一个球的体积主要集中在离球面近的区域，相对于近球心的区域，是指数级差距） 从而导致拒绝率可能接近1Importance SamplingThe technique of importance sampling provides a framework for approximating expectations directly but does not itself provide a mechanism for drawing samples from distributionalso using proposal distribution $q(z)$      Importance Weights\\\\[\\\\begin{aligned} \\\\mathbb{E}[f] &amp;= \\\\int f(z)p(z)dz \\\\\\\\  &amp;= \\\\int f(z) \\\\frac {p(z)}{q(z)}q(z) dz \\\\\\\\ &amp;\\\\simeq \\\\frac 1L \\\\sum_{l=1}^L \\\\frac {p(z^{(l)})}{q(z^{(l)})} f(z^{(l)}) \\\\end{aligned}\\\\]    $r_l = \\\\frac {p(z^{(l)})}{q(z^{(l)})}$ are known as Importance Weights        work on unnormoalized distribution\\\\[\\\\begin{aligned} \\\\mathbb{E}[f] &amp;= \\\\int f(z)p(z)dz \\\\\\\\  &amp;= \\\\frac{Z_p}{Z_q}  \\\\int f(z) \\\\frac {\\\\tilde p(z)}{\\\\tilde q(z)} q(z) dz \\\\\\\\ &amp;\\\\simeq \\\\frac{Z_q}{Z_p} \\\\frac 1L \\\\sum_{l=1}^L \\\\frac {\\\\tilde p(z)}{\\\\tilde q(z)} f(z^{(l)}) \\\\\\\\\\\\\\\\ \\\\frac{Z_q}{Z_p} &amp;= \\\\frac {1}{Z_q} \\\\int \\\\tilde p(z) dz \\\\\\\\ &amp;= \\\\int \\\\frac {\\\\tilde p(z)}{\\\\tilde q(z)}q(z) dz \\\\\\\\ &amp;\\\\simeq \\\\frac 1L \\\\sum_{l=1}^L \\\\frac {\\\\tilde p(z)}{\\\\tilde q(z)} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[f] &amp;\\\\simeq \\\\frac {1}{\\\\sum_{l=1}^L \\\\frac {\\\\tilde p(z^{(l)})}{\\\\tilde q(z^{(l)})}}\\\\sum_{l=1}^L \\\\frac {\\\\tilde p(z^{(l)})}{\\\\tilde q(z^{(l)})} f(z^{(l)}) \\\\end{aligned}\\\\]  注意：  如果通常情况下，在$p(z)f(z)$ 变化很大并且在$z$空间的相对较小区域上具有显着的集中的值存在，则重要性权重 ${rl}$ 的集合可能由一些具有较大值的权重控制，其余权重相对无关紧要  也可能存在着在$p(z)f(z)$很大时，但却没有样本对应此区间Hence a major drawback of the importance sampling method is the potential to produce results that are arbitrarily in error and with no diagnostic indication.因此需要保证$p(z)$可能取值较大时，$q(z)$不能很小或者为0，否则将使抽样结果与真实值存在很大差异In Graphical Model:  Likelihood Weighted Sampling          This is based on ancestral sampling of the variables. - Each sample from the joint distribution is obtained by first setting those variables zi that are in the evidence set equal to their observed values.                                                  For each variable in turn, if that variable is in the evidence set, then it is just set to its instantiated value. If it is not in the evidence set, then it is sampled from the conditional distribution $p(z_i              pa_i)$ in which the conditioning variables are set to their currently sampled values. The weighting associated with the resulting sample $z$ is then given by                              \\\\[\\\\begin{aligned} r(z) = \\\\prod_{z_i\\\\in e} p(z_i\\\\vert pa_i) \\\\end{aligned}\\\\]            self-importance sampling      Sampling-Importance-resampling  Step 1: Use proposal distribution generate $L$ samples $z^{(1)},…,z^{(L)}$      Step 2: A second set of $L$ samples is drawn from the discrete distribution $z^{(1)},…,z^{(L)}$ with probabilities\\\\[\\\\begin{aligned} \\\\frac {\\\\frac {\\\\tilde p(z^{(l)})}{\\\\tilde q(z^{(l)})}}{\\\\sum_{l=1}^L \\\\frac {\\\\tilde p(z^{(l)})}{\\\\tilde q(z^{(l)})}} \\\\end{aligned}\\\\]  Note: the discrete distribution $z^{(1)},…,z^{(L)}$ are only approximately distributed according to $p(z)$, but the distribution becomes correct in the limit $L \\\\rightarrow \\\\infty$Monte Carlo EM algorithmIn particular, sampling methods can be used to approximate the E step of the EM algorithm for models in which the E step cannot be performed analytically.\\\\[\\\\begin{aligned} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) = \\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{Z} \\\\mid \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old})  \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z} \\\\mid \\\\boldsymbol{\\\\theta}) \\\\end{aligned}\\\\]      using sampling methods to generate ${\\\\mathbf{Z}^{(L)}}$ and then:\\\\[\\\\begin{aligned} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) \\\\simeq \\\\frac 1L \\\\sum_{l=1}^L \\\\ln p(\\\\mathbf{X},\\\\mathbf{Z}^{(L)} \\\\mid \\\\boldsymbol{\\\\theta}) \\\\end{aligned}\\\\]  A particular instance of the Monte Carlo EM algorithm, called stochastic EM, arises if we consider a finite mixture model, and draw just one sample at each E stepImputation-Posterior AlgorithmThis inspires the data augmentation algorithm, which alternates between two steps known as the I-step (imputation step, analogous to an E step) and the P-step (posterior step, analogous to an M step).$Z$ hidden variables, $\\\\theta$ parameter (random variables)  I-step:                  wish to sample from $p(Z\\\\vert X)$ but we cannot do this disectly\\\\[\\\\begin{aligned} p(Z\\\\vert X) = \\\\int p(Z\\\\vert \\\\theta, X)p(\\\\theta \\\\vert X) d\\\\theta \\\\end{aligned}\\\\]                    and hence for $1,…,L$, $p(\\\\theta\\\\vert X) \\\\rightarrow \\\\theta^{(l)}$, and then $p(Z\\\\vert \\\\theta^{(l)}, X)\\\\rightarrow Z^{(l)}$              P-step                  Given the relation\\\\[\\\\begin{aligned} p(\\\\theta \\\\vert X) = \\\\int p(\\\\theta\\\\vert Z, X) p(Z\\\\vert X) dZ \\\\end{aligned}\\\\]                    then\\\\[\\\\begin{aligned} p(\\\\theta\\\\vert X) \\\\simeq p(\\\\theta\\\\vert Z^{(l)}, X) \\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "Markov Chain Monte Carlo",
    "url": "/posts/markov_chain_monte_carlo_methods/",
    "categories": "Machine Learning, Markov Chain",
    "tags": "Markov Chain, Monte Carlo",
    "date": "2022-02-21 00:00:00 +0000",
    





    "snippet": "Markov Chain Monte CarloAs with rejection and importance sampling, we again sample from a proposal distribution.            This time, however, we maintain a record of the current state $z(\\\\tau)$, and the proposal distribution $q(z      z(\\\\tau))$depends on this current state, and so the sequence of samples $z^(1), z^(2), . . .$ forms a Markov Section chain.      Basic Metropolis AlgorithmAssume that the proposal distribution is symmetric, that is $q(z_A\\\\vert z_B) = q(z_B\\\\vert z_A)$ for all values of $z_A$ and $z_B$Firstly, we generate a candidate sample $z^*$ from the proposal distribution, and then it is accepted with probability:\\\\[\\\\begin{aligned} \\\\boldsymbol{A}(z^*, z^{(\\\\tau)}) = \\\\min \\\\left(1, \\\\frac {\\\\tilde p(z^*)}{\\\\tilde p(z^{(\\\\tau)}} \\\\right) \\\\end{aligned}\\\\]  $u = \\\\text{random}(0,1)$, if $\\\\boldsymbol{A}(z^*, z^{(\\\\tau)}) &gt; u$, then accept  if accepted, then $z^{(\\\\tau+1)} = z^*$, else $z^{(\\\\tau+1)}=z^{(\\\\tau)}$  A new candidate sample is drawn from the distribution $q(z\\\\vert z^{(\\\\tau+1)})$Note: In the Metropolis algorithm when a candidate point is rejected, the previous sample is included instead in the final list of samples, leading to multiple copies of samples.To obtain i.i.d samples:  we can discard most of the sequence and just retain every $M^{th}$ sample  or run multiple algorithm at same time.Random Walk BehaviourConsider a state space $z$ consisting of the integers, with probabilities:\\\\[\\\\begin{aligned} p(z^{\\\\tau + 1} = z^{\\\\tau})  &amp;= 0.5 \\\\\\\\ p(z^{\\\\tau + 1} = z^{\\\\tau} + 1) &amp;= 0.25 \\\\\\\\ p(z^{\\\\tau + 1} = z^{\\\\tau} - 1) &amp;= 0.25 \\\\end{aligned}\\\\]  Assume if $z^{1} = 0$, then $\\\\mathbb{E}[z^{\\\\tau}] = 0$ and $\\\\mathbb{E}[(z^{\\\\tau})^2] = \\\\tau / 2$  Thus after $\\\\tau$ steps, the random walk has only travelled a distance that on average is proportional to the square root of $\\\\tau$.  This square root dependence is typical of random walk behaviour and shows that random walks are very inefficient in exploring the state space.Note: a central goal in designing Markov chain Monte Carlo methods is to avoid random walk behaviour.The Metropolis-Hastings AlgorithmHere the proposal distribution is no longer a symmetric function of its arguments.At step $\\\\tau$, draw a sample $z^$ from the distribution $q_k(z\\\\vert z^{(\\\\tau)})$, and then accept it with probability $A_k(z^, z^{(\\\\tau)})$ where\\\\[\\\\begin{aligned} A_k(z^*, z^{(\\\\tau)}) \\\\min \\\\left(1, \\\\frac {\\\\tilde p(z^*)q_k(z^{(\\\\tau)}\\\\vert z^*)}{\\\\tilde p(z^{(\\\\tau)})q_k(z^*\\\\vert z^{(\\\\tau)})} \\\\right) \\\\end{aligned}\\\\]The specific choice of proposal distribution can have a marked effect on the performance of the algorithm.For continuous state spaces, a common choice is a Gaussian centred on the current state, leading to an important trade-off in determining the variance parameter of this distribution.  If the variance is small, then the proportion of accepted transitions will be high, but progress through the state space takes the form of a slow random walk leading to long correlation times.  However, if the variance parameter is large, then the rejection rate will be high because, in the kind of complex problems we are considering, many of the proposed steps will be to states for which the probability $p(z)$ is lowGibbs SamplingGibbs sampling is appropriate for drawing samples from multidimensional distributions.  Initialize $[z_1^{(0)},…,z_m^{(0)}]$  For $\\\\tau$ in $1,2,…,\\\\Tau$:          $z_1^{(\\\\tau + 1)} \\\\sim p(z_1\\\\vert z_2^{(\\\\tau)}, z_3^{(\\\\tau)},…,z_M^{(\\\\tau)})$      $z_2^{(\\\\tau + 1)} \\\\sim p(z_1\\\\vert z_1^{(\\\\tau+1)}, z_3^{(\\\\tau)},…,z_M^{(\\\\tau)})$      $\\\\cdots$      $z_M^{(\\\\tau + 1)} \\\\sim p(z_1\\\\vert z_1^{(\\\\tau+1)}, z_2^{(\\\\tau+1)},…,z_{M-1}^{(\\\\tau+1)})$      The Gibbs scheme can be viewed as a realization of a Markov chain, where the transition matrix/pdf is sequentially constructed from $l$ base transitions, this is:\\\\[\\\\begin{aligned} T = B_1\\\\cdots B_M \\\\end{aligned}\\\\]  Blocking Gibbs Sampling  Collapsed Gibbs Sampling"
  },
  
  {
    "title": "Markov Chain",
    "url": "/posts/markov_chain/",
    "categories": "Machine Learning, Markov Chain",
    "tags": "Markov Chain",
    "date": "2022-02-21 00:00:00 +0000",
    





    "snippet": "Markov ChainA first-Order Markov Chain\\\\[\\\\begin{aligned} p(z^{(m+1)}\\\\vert z^{(m)}, ...,z^{(1)} )  = p(z^{(m+1)}\\\\vert z^{(m)})\\\\end{aligned}\\\\]then given  initial variable: $p(z^{(0)})$  transition probabilities: $T_m(z^{(m)}, z^{(m+1)})$we can obtain a specify Markov ChainHomogeneous Markov ChainTransition probabilities are the same for all $m$.Stationary or Invariant Markov ChainEach step in the chain leaves that distribution invariant.For a homogeneous Markov chain with transition probabilities $T(z’,z)$, the distribution $p^*(z)$ is invariant:\\\\[\\\\begin{aligned} p^*(z) = \\\\sum_{z&#39;} T(z&#39;,z)p^*(z&#39;) \\\\end{aligned}\\\\]Note: a given Markov chain may have more than one invariant distribution. For instance: an identity transformationDetailed Balance propertyA sufficient but not necessary condition for ensuring that the required distribution $p(z)$ is invariant is to choose the transition probability to satisfy the property of detailed balance for the particular distribution $p^*(z)$\\\\[\\\\begin{aligned} p^*(z&#39;)T(z&#39;,z) = p^*(z)T(z,z&#39;)  \\\\end{aligned}\\\\]$\\\\text{Detailed Balance} \\\\Longrightarrow \\\\text{Stationary}$\\\\[\\\\begin{aligned} \\\\sum_{z&#39;} p^*(z&#39;)T(z&#39;,z) &amp;= \\\\sum_{z&#39;} p^*(z)T(z,z&#39;) \\\\\\\\ &amp;= p^*(z)\\\\sum_{z&#39;} T(z,z&#39;) \\\\\\\\ &amp;= p^*(z)  \\\\end{aligned}\\\\]Reversible Markov ChainA Markov chain that respects detailed balance is said to be reversible.Ergodicity Markov ChainFor a stationary Markov Chain, if for $m\\\\rightarrow \\\\infty$, $p(z^{(m)}$ converges to a required invariant distribution $p^*(z)$, irrespective of the choice of initial distribution $p(z^{(0)}$.This property is called ergodicity.And the invariant distribution is then called the equilibrium distribution.Note: an ergodic Markov chain can have only one equilibrium distribution.A homogeneous Markov chain will be ergodic, subject only to weak restrictions on the invariant distribution and the transition probabilities.Base TransitionsIn practice we often construct the transition probabilities from a set of ‘base’ transitions $B_1,…,B_K$. This can be achieved through a mixture distribution of the form\\\\[\\\\begin{aligned} T(z&#39;,z) = \\\\sum_{k=1}^K \\\\alpha_k B_k(z&#39;,z) \\\\\\\\ \\\\sum\\\\alpha_k &amp;= 1 \\\\end{aligned}\\\\]"
  },
  
  {
    "title": "Hidden Markov Model",
    "url": "/posts/HMM/",
    "categories": "Machine Learning, HMM",
    "tags": "HMM, EM, Markov Chain",
    "date": "2022-02-21 00:00:00 +0000",
    





    "snippet": "Hidden Markov ModelAssume that all latent variables form a Markov chain, giving rise to the graphical structure, this is known as a state space model.If the latent variables are discrete, then we obtain the hidden Markov model, or HMM.  Note that the observed variables in an HMM may be discrete or continuous, and a variety of different conditional distributions can be used to model them.Transition and Emmission ProbabilitiesIf we allow the probability distribution of $z_n$ to depend on the state of the previous latent variable $z_{n-1}$ through a conditional distribution $p(z_n\\\\vert z_{n−1})$.Because the latent variables are K-dimensional binary variables, this conditional distribution corresponds to a table of numbers that we denote by $\\\\mathbf{A}$, the elements of which are known as transition probabilitiesTransition ProbabilitiesTransition Probabilities is defined as:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z}_n\\\\vert \\\\boldsymbol{z}_{n-1}, \\\\mathbf{A}) &amp;= \\\\prod_{k=1}^K \\\\prod_{k=1}^K A_{jk}^{z_{n-1,j},z_{nk}} \\\\\\\\ p(\\\\boldsymbol{z_1}\\\\vert \\\\boldsymbol{\\\\pi}) &amp;= \\\\prod_{k=1}^K \\\\pi_k^{z_{1k}} \\\\end{aligned}\\\\]      where $\\\\sum_k z_{1k} = 1$        A state transition diagram of HMM as shown follow            A Lattice or Trellis diagram of HMM as shown follow      Emmission ProbabilitiesEmmission Probabilities:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_{n}, \\\\boldsymbol{w}) &amp;= \\\\prod_{k=1}^K p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{w})^{z_{nk}} \\\\end{aligned}\\\\]  $\\\\boldsymbol{w}$ is a set of parameter ofHomogeneous ModelWe shall focuss attention on homogeneous models for which all of the conditional distributions governing the latent variables share the same parameters $\\\\mathbf{A}$, and similarly all of the emission distributions share the same parameters $\\\\boldsymbol{w}$ (the extension to more general cases is straightforward).HMM and Mixture ModelNote that a mixture model for an i.i.d data set corresponds to the special case in which the parameters $A_{jk}$ are the same for all values of $j$, so that the conditional distribution $p(z_n\\\\vert z_n−1)$ is independent of $z_{n−1}$.Under the Homogeneous Model assumption, the joint probability distribution over both latent and observed variables is then given by\\\\[\\\\begin{aligned} p(\\\\boldsymbol{X} ,\\\\boldsymbol{Z} \\\\vert \\\\boldsymbol{\\\\theta}) = p(\\\\boldsymbol{z}_1\\\\vert \\\\boldsymbol{\\\\pi}) \\\\left[ \\\\prod_{n=1}^N p(\\\\boldsymbol{z}_n\\\\vert \\\\boldsymbol{z}_{n-1}, \\\\mathbf{A}) \\\\right] \\\\prod_{n=1}^N p(\\\\boldsymbol{x}_m\\\\vert \\\\boldsymbol{z}_m, \\\\boldsymbol{w})  \\\\end{aligned}\\\\]  \\\\[\\\\boldsymbol{\\\\theta} = \\\\{\\\\boldsymbol{\\\\pi}, \\\\mathbf{A}, \\\\boldsymbol{w}\\\\}\\\\]  Most of our discussion of the hidden Markov model will be independent of the particular choice of the emission probabilities.Indeed, the model is tractable for a wide range of emission distributions including discrete tables, Gaussians, and mixtures of Gaussians. It is also possible to exploit discriminative models such as neural networks.These can be used to model the emission density $p(x\\\\vert z)$ directly, or to provide a representation for $p(z\\\\vert x)$ that can be converted into the required emission density $p(x\\\\vert z) $ using Bayes’ theorem.We also can gain a better understanding of the hidden Markov model by considering it from a generative point of view. Just like a ancestral sampling for a directed graphical model.Left-to-Right ModelBy setting the elements of $A_{jk}$ of $\\\\mathbf{A}$ to zero if $j &gt; k$      the Transition Diagram and Lattice or Trellis Diagram of Left-to-Right Model          Inference and Learning of HMMGiven a data set $\\\\mathbf{X} = {\\\\boldsymbol{x}_1, . . . , \\\\boldsymbol{x}_N}$Likelihood FunctionThe likelihood function is obtained from the joint distribution by marginalizing over the latent variables\\\\[\\\\begin{aligned} p(\\\\mathbf{X} \\\\vert \\\\boldsymbol{\\\\theta}) = \\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{X} ,\\\\mathbf{Z} \\\\vert \\\\boldsymbol{\\\\theta}) \\\\end{aligned}\\\\]EM for HMME-step\\\\[\\\\begin{aligned} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) =&amp; \\\\sum_{\\\\mathbf{Z}} p(\\\\mathbf{Z}\\\\vert \\\\mathbf{X}, \\\\boldsymbol{\\\\theta}^{old}) \\\\ln p(\\\\mathbf{X} ,\\\\mathbf{Z} \\\\vert \\\\boldsymbol{\\\\theta}) \\\\\\\\\\\\\\\\ =&amp; \\\\sum_{k=1}^K \\\\mathbb{E}[z_{1k}]\\\\ln \\\\pi_k + \\\\sum_{n=2}^N\\\\sum_{j=1}^K\\\\sum_{k=1}^K \\\\mathbb{E}[z_{n-1,j},z_{nk}]\\\\ln A_{jk} \\\\\\\\ &amp; + \\\\sum_{n=1}^N \\\\sum_{k=1}^K \\\\mathbb{E}[z_{nk}] \\\\ln p(\\\\boldsymbol{x}_n \\\\vert \\\\boldsymbol{w}_k) \\\\end{aligned}\\\\]      To simplify notion, we define:\\\\[\\\\begin{aligned} \\\\gamma_{nk} &amp;= \\\\mathbb{E}[z_{nk}] = \\\\sum_{\\\\boldsymbol{z}_n} \\\\gamma(\\\\boldsymbol{z}_n) z_{nk} \\\\\\\\ \\\\xi_{z_{n-1,j}, z_{nk}} &amp;= \\\\mathbb{E}[z_{n-1,j}, z_{nk}]  = \\\\sum_{\\\\boldsymbol{z}_n} \\\\xi(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n})z_{n-1,j}z_{nk} \\\\end{aligned}\\\\]  M-Step\\\\[\\\\begin{aligned} \\\\boldsymbol{\\\\theta} = \\\\arg \\\\max_{\\\\boldsymbol{\\\\theta}} \\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old}) \\\\end{aligned}\\\\]      then we can obtain:\\\\[\\\\begin{aligned} \\\\pi_k &amp;= \\\\frac {\\\\gamma_{1k}}{\\\\sum_{j=1}^K \\\\gamma_{1j}} \\\\\\\\ A_{jk} &amp;= \\\\frac {\\\\sum_{n=2}^N \\\\xi_{z_{n-1,j}z_{nk}}}{\\\\sum_{l=1}^K\\\\sum_{n=2}^K\\\\xi_{z_{n-1,j}z_{nl}}}  \\\\end{aligned}\\\\]  The EM algorithm must be initialized by choosing starting values for $\\\\boldsymbol{\\\\pi}$ and $\\\\mathbf{A}$, which should of course respect the summation constraints associated with their probabilistic interpretation.To maximize $\\\\mathcal{Q}(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{\\\\theta}^{old})$ with respect to $\\\\boldsymbol{w}_k$, we notice that only the final term depends on $\\\\boldsymbol{w}_k$ and furthermore this term has exactly the same form as the data-dependent term in the corresponding function for a standard mixture distribution for i.i.d data,If the parameters $\\\\boldsymbol{w}_k$ are independent for the different components, then this term decouples into a sum of terms one for each value of $k$, each of which can bemaximized independentlyThe EM algorithm requires initial values for the parameters of the emission distribution. One way to set these is first to treat the data initially as i.i.d. and fit the emission density by maximum likelihood, and then use the resulting values to initialize the parameters for EM.The forward-backward algorithmNext we seek an efficient procedure for evaluating the quantities $\\\\gamma_{nk}$ and $\\\\xi_{z_{n-1,j}, z_{nk}}$, corresponding to the E step of the EM algorithm.The graph for the hidden Markov model is a tree, and so we know that the posterior distribution of the latent variables can be obtained efficiently using a two-stage message passing algorithmIn the particular context of the hidden Markov model, this is known as the forward-backward algorithm or the Baum-Welch algorithm.There are in fact several variants of the basic algorithm, all of which lead to the exact marginals, according to the precise form of the messages that are propagated along the chain. Alpha-beta algorithm.Evaluate $\\\\gamma_{nk}$Recall that for a discrete multinomial random variable the expected value of one of its components is just the probability of that component having the value 1.Thus we are interested in finding the posterior distribution $p(\\\\boldsymbol{z}_n\\\\vert \\\\mathbf{X})$.This represents a vector of length $K$ whose entries correspond to the expected values of $z_{nk}$.\\\\[\\\\begin{aligned} \\\\gamma_{\\\\boldsymbol{z}_n} = p(\\\\boldsymbol{z}_n\\\\vert \\\\mathbf{X}) = \\\\frac {p(\\\\mathbf{X}\\\\vert \\\\boldsymbol{z}_n) P(\\\\boldsymbol{z}_n)}{p(\\\\mathbf{X})} \\\\end{aligned}\\\\]      Note that the denominator $p(\\\\mathbf{X})$ is implicitly conditioned on the parameters $\\\\boldsymbol{\\\\theta}^{old}$ of the HMM and hence represents the likelihood function.        Using some conditional independence property:\\\\[\\\\begin{aligned} \\\\gamma_{\\\\boldsymbol{z}_n} &amp;= \\\\frac {p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_n, \\\\boldsymbol{z}_n)p(\\\\boldsymbol{x}_{n+1},...,\\\\boldsymbol{x}_N \\\\vert \\\\boldsymbol{z}_n)}{p(\\\\mathbf{X})} = \\\\frac {\\\\alpha(\\\\boldsymbol{z}_n)\\\\beta(\\\\boldsymbol{z}_n)}{p(\\\\mathbf{X})} \\\\\\\\ \\\\\\\\ \\\\alpha(\\\\boldsymbol{z}_n) &amp;= p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_n, \\\\boldsymbol{z}_n) \\\\\\\\ \\\\beta(\\\\boldsymbol{z}_n) &amp;= p(\\\\boldsymbol{x}_{n+1},...,\\\\boldsymbol{x}_N \\\\vert \\\\boldsymbol{z}_n) \\\\end{aligned}\\\\]        recursion formula of $\\\\alpha(\\\\boldsymbol{z}_n)$\\\\[\\\\begin{aligned} \\\\alpha(\\\\boldsymbol{z}_n) &amp;= p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_n, \\\\boldsymbol{z}_n) \\\\\\\\ &amp;= p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)p(\\\\boldsymbol{z}_n) \\\\\\\\&amp;= p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_{n-1}\\\\vert \\\\boldsymbol{z}_n) p(\\\\boldsymbol{z}_n) \\\\\\\\ &amp;= p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_{n-1}, \\\\boldsymbol{z}_n) \\\\\\\\ &amp;= p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)\\\\sum_{\\\\boldsymbol{z}_{n-1}} p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_{n-1}, \\\\boldsymbol{z}_{n-1} ,\\\\boldsymbol{z}_n) \\\\\\\\ &amp;= p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)\\\\sum_{\\\\boldsymbol{z}_{n-1}} p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_{n-1}, \\\\boldsymbol{z}_{n-1})p(\\\\boldsymbol{z}_n\\\\vert \\\\boldsymbol{z}_{n-1}) \\\\\\\\ &amp;= p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)\\\\sum_{\\\\boldsymbol{z}_{n-1}} \\\\alpha(\\\\boldsymbol{z}_{n-1})p(\\\\boldsymbol{z}_n\\\\vert \\\\boldsymbol{z}_{n-1}) \\\\\\\\ \\\\\\\\ \\\\alpha(\\\\boldsymbol{z}_1) &amp;= p(\\\\boldsymbol{x}_1 \\\\vert \\\\boldsymbol{z}_1)p(\\\\boldsymbol{z}_1) = \\\\prod_{k=1}^K (\\\\pi_k p(\\\\boldsymbol{x}_1\\\\vert \\\\boldsymbol{w}_k))^{z_{1k}} \\\\end{aligned}\\\\]          forward message passing: $1 \\\\rightarrow n$            recursion formula of $\\\\beta(\\\\boldsymbol{z}_n)$\\\\[\\\\begin{aligned} \\\\beta(\\\\boldsymbol{z}_n) &amp;= p(\\\\boldsymbol{x}_{n+1},...,\\\\boldsymbol{x}_N \\\\vert \\\\boldsymbol{z}_n) \\\\\\\\&amp;= \\\\sum_{\\\\boldsymbol{z}_{n+1}} p(\\\\boldsymbol{x}_{n+2},...,\\\\boldsymbol{x}_N \\\\vert \\\\boldsymbol{z}_{n+1}) p(\\\\boldsymbol{x}_{n+1}\\\\vert \\\\boldsymbol{z}_{n+1}) p(\\\\boldsymbol{z}_{n+1}\\\\vert \\\\boldsymbol{z}_{n}) \\\\\\\\ &amp;= p(\\\\boldsymbol{x}_{n+1},...,\\\\boldsymbol{x}_N \\\\vert \\\\boldsymbol{z}_n) \\\\\\\\&amp;= \\\\sum_{\\\\boldsymbol{z}_{n+1}} \\\\beta(\\\\boldsymbol{z}_{n+1}) p(\\\\boldsymbol{x}_{n+1}\\\\vert \\\\boldsymbol{z}_{n+1}) p(\\\\boldsymbol{z}_{n+1}\\\\vert \\\\boldsymbol{z}_{n})  \\\\end{aligned}\\\\]                  backward message passing: $N \\\\rightarrow n$                    here we also need a strarting condition for the recursion, and we can be obtained by setting $n=N$, and easily obtain: $\\\\beta(\\\\boldsymbol{Z}_N) = 1$                    However, the quantity $p(\\\\mathbf{X})$ represents the likelihood function whose value we typically wish to monitor during the EM optimization, and so it is useful to be able to evaluate it.\\\\[\\\\begin{aligned} p(\\\\mathbf{X}) &amp;= \\\\sum_{\\\\boldsymbol{z}_n} \\\\alpha(\\\\boldsymbol{z}_n)\\\\beta(\\\\boldsymbol{z}_n)\\\\\\\\ &amp;= \\\\sum_{\\\\boldsymbol{z}_N} \\\\alpha(\\\\boldsymbol{z}_N) \\\\end{aligned}\\\\]            Evaluate $\\\\xi_{z_{n-1,j}, z_{nk}}$\\\\[\\\\begin{aligned}\\\\xi(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n}) = p(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n} \\\\vert \\\\mathbf{X})\\\\end{aligned}\\\\]  for each of the $K \\\\times K$ settings for $(\\\\boldsymbol{z}{n-1}, \\\\boldsymbol{z}{n})$.\\\\[\\\\begin{aligned} \\\\xi(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n}) &amp;= p(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n} \\\\vert \\\\mathbf{X}) \\\\\\\\ &amp;= \\\\frac {p(\\\\mathbf{X}\\\\vert \\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n})p(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_{n})}{p(\\\\mathbf{X})} \\\\\\\\ &amp;= \\\\frac {p(\\\\boldsymbol{x}_{`n+1`}, ...,\\\\boldsymbol{x}_{n-1} \\\\vert \\\\boldsymbol{z}_{n-1})p(\\\\boldsymbol{z}_{n-1})p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n) p(\\\\boldsymbol{x}_{n+1}, ...,\\\\boldsymbol{x}_{N}\\\\vert \\\\boldsymbol{z}_n)p(\\\\boldsymbol{z}_{n}\\\\vert \\\\boldsymbol{z}_{n-1})}{p(\\\\mathbf{X})} \\\\\\\\ &amp;= \\\\frac {\\\\alpha(\\\\boldsymbol{z}_{n-1}) p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n) p(\\\\boldsymbol{z}_{n}\\\\vert \\\\boldsymbol{z}_{n-1}) \\\\beta(\\\\boldsymbol{z}_{n})}{p(\\\\mathbf{X})} \\\\end{aligned}\\\\]Predictive Distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}_{N+1}\\\\vert \\\\mathbf{X}) &amp;= \\\\sum_{\\\\boldsymbol{z}_{N+1}} p(\\\\boldsymbol{x}_{N+1}, \\\\boldsymbol{z}_{N+1} \\\\vert \\\\mathbf{X}) \\\\\\\\ &amp;= \\\\sum_{\\\\boldsymbol{z}_{N+1}} p(\\\\boldsymbol{x}_{N+1} \\\\vert \\\\boldsymbol{z}_{N+1}) \\\\sum_{\\\\boldsymbol{z}_N} p(\\\\boldsymbol{z}_{N+1}, \\\\boldsymbol{z}_N \\\\vert \\\\mathbf{X}) \\\\\\\\ &amp;=  \\\\sum_{\\\\boldsymbol{z}_{N+1}} p(\\\\boldsymbol{x}_{N+1} \\\\vert \\\\boldsymbol{z}_{N+1}) \\\\sum_{\\\\boldsymbol{z}_N} p(\\\\boldsymbol{z}_{N+1}\\\\vert \\\\boldsymbol{z}_N) p(\\\\boldsymbol{z}_N \\\\vert \\\\mathbf{X}) \\\\\\\\ &amp;=  \\\\sum_{\\\\boldsymbol{z}_{N+1}} p(\\\\boldsymbol{x}_{N+1} \\\\vert \\\\boldsymbol{z}_{N+1}) \\\\sum_{\\\\boldsymbol{z}_N} p(\\\\boldsymbol{z}_{N+1}\\\\vert \\\\boldsymbol{z}_N) \\\\frac {p(\\\\boldsymbol{z}_N, \\\\mathbf{X}) }{p(\\\\mathbf{X})} \\\\\\\\ &amp;= \\\\frac {1}{p(\\\\mathbf{X})} \\\\sum_{\\\\boldsymbol{z}_{N+1}} p(\\\\boldsymbol{x}_{N+1} \\\\vert \\\\boldsymbol{z}_{N+1}) \\\\sum_{\\\\boldsymbol{z}_N} p(\\\\boldsymbol{z}_{N+1}\\\\vert \\\\boldsymbol{z}_N) \\\\alpha(\\\\boldsymbol{z}_N) \\\\end{aligned}\\\\]Scaling FactorsThere is an important issue that must be addressed before we can make use of the forward backward algorithm in practice.Because these probabilities are often significantly less than unity, as we work our way forward along the chain, the values of $α(\\\\boldsymbol{z}_{n})$ can go to zero exponentially quickly.In the case of i.i.d. data, we implicitly circumvented this problem with the evaluation of likelihood functions by taking logarithms.  Unfortunately, this will not help here because we are forming sums of products of small numbers (we are in fact implicitly summing over all possible paths through the lattice diagramre-scale:\\\\[\\\\begin{aligned}&amp;\\\\alpha(\\\\boldsymbol{z}_{n}) \\\\\\\\ &amp;\\\\beta(\\\\boldsymbol{z}_{n})\\\\end{aligned}\\\\]      this leads their values remain of order unity\\\\[\\\\begin{aligned} \\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_n) = p(\\\\boldsymbol{z}_n \\\\vert \\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_n) =\\\\frac {\\\\alpha(\\\\boldsymbol{z}_n)}{p(\\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_n)} \\\\end{aligned}\\\\]        In order to relate the scaled and original alpha variables, we introduce scaling factors defined by conditional distributions over the observed variables\\\\[\\\\begin{aligned} c_n = p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_{n-1}) \\\\end{aligned}\\\\]          Easily, we have:    \\\\[\\\\begin{aligned} p( \\\\boldsymbol{x}_1,...,\\\\boldsymbol{x}_{n}) &amp;=\\\\prod_{m=1}^n c_m \\\\\\\\ \\\\alpha(\\\\boldsymbol{z}_n) &amp;= \\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_n)\\\\prod_{m=1}^n c_m \\\\\\\\ \\\\\\\\ c_n \\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_n) &amp;= p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)\\\\sum_{\\\\boldsymbol{z}_{n-1}} \\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_{n-1})p(\\\\boldsymbol{z}_n\\\\vert \\\\boldsymbol{z}_{n-1}) \\\\end{aligned}\\\\]          Note: we also have to evaluate and store $c_n$, but it is easily done because it is the coefficient that normalizes the right-hand side to give $\\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_{n})$            Also, for $\\\\beta(\\\\boldsymbol{z}_n)$\\\\[\\\\begin{aligned} \\\\beta(\\\\boldsymbol{z}_n) &amp;= \\\\widehat{\\\\beta}(\\\\boldsymbol{z}_n) \\\\prod_{m=n+1}^N c_m \\\\\\\\ \\\\widehat{\\\\beta}(\\\\boldsymbol{z}_n) &amp;= \\\\frac {p(\\\\boldsymbol{x}_{n+1}, ..., \\\\boldsymbol{x}_{N}\\\\vert \\\\boldsymbol{z}_{n})}{p(\\\\boldsymbol{x}_{n+1},...\\\\boldsymbol{x}_{N}\\\\vert \\\\boldsymbol{x}_{1},...,\\\\boldsymbol{x}_{n})} \\\\\\\\ c_{n+1}\\\\widehat{\\\\beta}(\\\\boldsymbol{z}_{n}) &amp;=  \\\\sum_{\\\\boldsymbol{z}_{n+1}} \\\\widehat{\\\\beta}(\\\\boldsymbol{z}_{n+1}) p(\\\\boldsymbol{x}_{n+1}\\\\vert \\\\boldsymbol{z}_{n+1}) p(\\\\boldsymbol{z}_{n+1}\\\\vert \\\\boldsymbol{z}_{n}) \\\\end{aligned}\\\\]        we also have a approximation of $p(\\\\mathbf{X})$:\\\\[\\\\begin{aligned} p(\\\\mathbf{X}) = \\\\prod_{n=1}^N c_n \\\\end{aligned}\\\\]        in the end, we got two iterative equations as follow:\\\\[\\\\begin{aligned} \\\\gamma(\\\\boldsymbol{z}_n) &amp;= \\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_n) \\\\widehat{\\\\beta}(\\\\boldsymbol{z}_{n}) \\\\\\\\ \\\\xi(\\\\boldsymbol{z}_{n-1}, \\\\boldsymbol{z}_n) &amp;= c_n\\\\widehat{\\\\alpha}(\\\\boldsymbol{z}_{n-1})p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{z}_n)p(\\\\boldsymbol{z}_n\\\\vert \\\\boldsymbol{z}_{n-1})  \\\\widehat{\\\\beta}(\\\\boldsymbol{z}_{n}) \\\\end{aligned}\\\\]  The Viterbi Algorithm\\\\[\\\\begin{aligned}  \\\\end{aligned}\\\\]"
  },
  
  {
    "title": "Dual Representations of Linear Models",
    "url": "/posts/dual_representation_of_linear_models/",
    "categories": "Machine Learning, Linear Models",
    "tags": "Kernel",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "Dual Representations of Linear Models      Many linear models for regression and classification can be reformulated in terms of a dual representation in which the kernel function arises naturally.        For example: the regularized sum-of-squared error function of linera regression model\\\\[\\\\begin{aligned} J(\\\\boldsymbol{w}) &amp;= \\\\frac 12 \\\\sum_{n=1}^N \\\\left( \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) -t_n \\\\right) + \\\\frac \\\\lambda 2 \\\\boldsymbol{w}^T\\\\boldsymbol{w} \\\\\\\\ \\\\\\\\ \\\\boldsymbol{w} &amp;= -\\\\frac 1\\\\lambda \\\\sum_{n=1}^N \\\\left( \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) -t_n \\\\right)\\\\phi(\\\\boldsymbol{x}_n) = \\\\sum_{n=1}^N a_n\\\\phi_n = \\\\Phi^T\\\\boldsymbol{a} \\\\\\\\ \\\\boldsymbol{a} &amp;= -\\\\frac 1\\\\lambda \\\\left( \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) -t_n \\\\right) \\\\\\\\ \\\\\\\\ \\\\text{then}: &amp;\\\\\\\\ J(\\\\boldsymbol{a}) &amp;= \\\\frac 12 \\\\boldsymbol{a}^T\\\\Phi\\\\Phi^T\\\\Phi\\\\Phi^T\\\\boldsymbol{a} -\\\\boldsymbol{a}^T\\\\Phi\\\\Phi^T\\\\mathbf{t} + \\\\frac 12 \\\\mathbf{t}^T\\\\mathbf{t} + \\\\frac \\\\lambda 2 \\\\boldsymbol{a}^T\\\\Phi\\\\Phi^T \\\\boldsymbol{a} \\\\\\\\\\\\\\\\J(\\\\boldsymbol{a}) &amp;= -\\\\frac12 \\\\boldsymbol{a}^TKK\\\\boldsymbol{a} - \\\\boldsymbol{a}^TK\\\\mathbf{t} + \\\\frac 12 \\\\mathbf{t}^T\\\\mathbf{t} + \\\\frac \\\\lambda 2 \\\\boldsymbol{a}^TK \\\\boldsymbol{a} \\\\\\\\ \\\\\\\\ \\\\boldsymbol{a} &amp;= (K+\\\\lambda I_N)^{-1}\\\\mathbf{t} \\\\end{aligned}\\\\]          where $\\\\Phi$ is the design matrix              $K$ is the Gram matrix: $K=\\\\Phi\\\\Phi^T$, and kernel function $k(x,x’)$\\\\[\\\\begin{aligned} K_{nm} = \\\\phi(\\\\boldsymbol{x}_n)^T\\\\phi(\\\\boldsymbol{x}_m) = k(\\\\boldsymbol{x}_n,\\\\boldsymbol{x}_m) \\\\end{aligned}\\\\]                    finally:\\\\[\\\\begin{aligned} y(\\\\boldsymbol{x}) &amp;= \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}) \\\\\\\\&amp;= \\\\boldsymbol{a}^T\\\\Phi\\\\phi(\\\\boldsymbol{x}) \\\\\\\\&amp;= \\\\boldsymbol{k}(\\\\boldsymbol{x})^T (K+\\\\lambda I_N)^{-1}\\\\mathbf{t} \\\\\\\\ \\\\boldsymbol{k}_n(\\\\boldsymbol{x}) &amp;= k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}) \\\\end{aligned}\\\\]              The advantage of the dual representation is that it is expressed entirely in terms of kernel function $k(x,x’)$  The existence of a dual representation based on the Gram matrix is a property of many linera models, including the perceptron.\\\\[\\\\begin{aligned} \\\\end{aligned}\\\\]"
  },
  
  {
    "title": "SVM for Regression",
    "url": "/posts/svm-regression/",
    "categories": "Machine Learning, SVM",
    "tags": "SVM, regression",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "SVM for RegressionUsing $\\\\epsilon$-insensitive error function$\\\\epsilon$-insensitive can lead to sparse solutions\\\\[\\\\begin{aligned} \\\\min &amp;\\\\qquad C\\\\sum_{n=1}^N (\\\\xi_n + \\\\widehat{\\\\xi}_n) + \\\\frac 12 \\\\| \\\\boldsymbol{w}\\\\|^2 \\\\\\\\ \\\\text{S.t.}&amp;\\\\qquad \\\\xi_n \\\\ge 0 \\\\\\\\&amp;\\\\qquad \\\\widehat{\\\\xi}_n \\\\ge 0 \\\\\\\\ &amp;\\\\qquad t_n \\\\le y(\\\\boldsymbol{x}_n) + \\\\epsilon + \\\\xi_n \\\\\\\\ &amp;\\\\qquad t_n \\\\ge y(\\\\boldsymbol{x}_n) - \\\\epsilon - \\\\widehat{\\\\xi}_n \\\\end{aligned}\\\\]  $\\\\xi_n &gt; 0$: corresponds to a point for which $t_n \\\\gt y(\\\\boldsymbol{x}_n) + \\\\epsilon$  $\\\\widehat\\\\xi_n &gt; 0$: corresponds to a point for which $t_n \\\\lt y(\\\\boldsymbol{x}_n) - \\\\epsilon$Origional Problem  Let $y_n = y(\\\\boldsymbol{x}_n) = \\\\boldsymbol{w}_n^T\\\\boldsymbol{x}_n + b$\\\\[\\\\begin{aligned} L =&amp; C\\\\sum_{n=1}^N (\\\\xi_n + \\\\widehat{\\\\xi}_n) + \\\\frac 12 \\\\| \\\\boldsymbol{w}\\\\|^2 - \\\\sum_{n=1}^N(\\\\mu_n\\\\xi_n + \\\\widehat\\\\mu_n\\\\widehat\\\\xi_n) \\\\\\\\ &amp; -\\\\sum_{n=1}^N a_n(\\\\epsilon + \\\\xi_n + y_n - t_n) -\\\\sum_{n=1}^N \\\\widehat a_n(\\\\epsilon+\\\\widehat{\\\\xi}_n-y_n+t_n)  \\\\end{aligned}\\\\]  Also set derivatives and …\\\\[\\\\begin{aligned} \\\\frac {\\\\partial L}{\\\\partial \\\\boldsymbol{w}} = 0 &amp;\\\\Rightarrow  \\\\boldsymbol{w} = \\\\sum_{n=1}^N (a_n-\\\\widehat a_n)\\\\phi_n \\\\\\\\ \\\\frac {\\\\partial L}{\\\\partial b} = 0 &amp;\\\\Rightarrow \\\\sum_{n=1}^N (a_n-\\\\widehat a_n) = 0 \\\\\\\\ \\\\frac {\\\\partial L}{\\\\partial \\\\xi_n} = 0 &amp;\\\\Rightarrow a_n = C-\\\\mu_n  \\\\Rightarrow a_n \\\\le C  \\\\\\\\ \\\\frac {\\\\partial L}{\\\\partial \\\\widehat\\\\xi_n} = 0 &amp;\\\\Rightarrow \\\\widehat a_n = C-\\\\widehat\\\\mu_n  \\\\Rightarrow \\\\widehat a_n \\\\le C  \\\\end{aligned}\\\\]Dual Problem\\\\[\\\\begin{aligned} \\\\widetilde L(\\\\boldsymbol{a}, \\\\widehat\\\\boldsymbol{a}) =&amp; -\\\\frac 12 \\\\sum_{n=1}^N\\\\sum_{m=1}^N (a_n - \\\\widehat{a}_n)(a_m - \\\\widehat{a}_m)k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) \\\\\\\\ &amp;- \\\\epsilon\\\\sum_{n=1}^N (a_n +\\\\widehat{a}_n) + \\\\sum_{n=1}^N (a_n-\\\\widehat{a}_n)t_n  \\\\end{aligned}\\\\]      Again, we have the box constraints\\\\[\\\\begin{aligned} &amp; 0\\\\le a_n \\\\le C \\\\\\\\ &amp; 0\\\\le \\\\widehat a_n \\\\le C \\\\end{aligned}\\\\]  Corresponding KKT conditions\\\\[\\\\begin{aligned} a_n(\\\\epsilon + \\\\xi_n + y_n - t_n) &amp;= 0 \\\\\\\\ \\\\widehat a_n(\\\\epsilon+\\\\widehat{\\\\xi}_n-y_n+t_n) &amp;= 0\\\\\\\\ (C-a_n)\\\\xi_n &amp;= 0 \\\\\\\\ (C-\\\\widehat a_n)\\\\widehat \\\\xi_n &amp;= 0 \\\\end{aligned}\\\\]  Support Vectors: for all          $0\\\\lt a_n \\\\lt C\\\\Rightarrow \\\\epsilon + \\\\xi_n + y_n - t_n = 0 \\\\Rightarrow \\\\xi_n = 0$: lie on upper boundary of $\\\\epsilon$-tube      $0\\\\lt \\\\widehat a_n \\\\lt C \\\\Rightarrow \\\\epsilon + \\\\widehat\\\\xi_n + y_n - t_n = 0 \\\\Rightarrow \\\\widehat\\\\xi_n = 0$: lie on lower boundary of $\\\\epsilon$-tube      The Predictions for new inputs\\\\[\\\\begin{aligned} y(\\\\boldsymbol{x}) = \\\\sum_{n=1}^N (a_n-\\\\widehat a_n)k(\\\\boldsymbol{x}, \\\\boldsymbol{x}_n) +b \\\\end{aligned}\\\\]\\\\[\\\\begin{aligned} b &amp;= t_n -\\\\epsilon - \\\\boldsymbol{w}^T\\\\phi_n \\\\\\\\&amp;= t_n \\\\epsilon -\\\\sum_{n=1}^N (a_m-\\\\widehat a_m)k(\\\\boldsymbol(x)_n, \\\\boldsymbol{x}_m) \\\\end{aligned}\\\\]$\\\\nu N$-SVM\\\\[\\\\begin{aligned} \\\\widetilde L(\\\\boldsymbol{a}, \\\\widehat\\\\boldsymbol{a}) =&amp; -\\\\frac 12 \\\\sum_{n=1}^N\\\\sum_{m=1}^N (a_n - \\\\widehat{a}_n)(a_m - \\\\widehat{a}_m)k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) + \\\\sum_{n=1}^N (a_n-\\\\widehat{a}_n)t_n  \\\\end{aligned}\\\\]      subject to the constraints:\\\\[\\\\begin{aligned} &amp; 0\\\\le a_n \\\\le C/N \\\\\\\\ &amp; 0\\\\le \\\\widehat a_n \\\\le C/N \\\\\\\\ &amp; \\\\sum_{n=1}^N(a_n-\\\\widehat{a}_n) = 0 \\\\\\\\ &amp; \\\\sum_{n=1}^N (a_n + \\\\widehat{a}_n) \\\\le \\\\nu C \\\\end{aligned}\\\\]  It can be shown that there are at most $\\\\nu N$ data points falling outside the $\\\\epsilon$-insensitive tube, while at least $\\\\nu N$ data points are support vectors and so lie either on the tube or outside it"
  },
  
  {
    "title": "SVM for classification",
    "url": "/posts/svm-classification/",
    "categories": "Machine Learning, SVM",
    "tags": "SVM, classification",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "SVM for classificationLinearly Separable caseThe Largest Margin PrincipleIn the two-classclassification problem using linear models of the form\\\\[\\\\begin{aligned} y(\\\\boldsymbol{x}) = \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}) + b \\\\end{aligned}\\\\]where $\\\\phi(\\\\boldsymbol{x})$ denotes a fixed feature-space transformation, and we have made the bias parameter b explicit.We shall assume for the moment that the training data set is linearly separable in feature space, so that by definition there exists at least one choice of the parameters $\\\\boldsymbol{w}$ and $b$ such that this function satisfies $y(\\\\boldsymbol{x}_n)&gt; 0$ for points having $t_n = +1$ and $y(\\\\boldsymbol{x}_n) &lt; 0$ for points having $t_n = -1$, so that $t_ny(\\\\boldsymbol{x}_n) &gt; 0$ for all training data points.  Margin: The support vector machine approaches this problem through the concept of the margin, which is defined to be the smallest distance between the decision boundary and any of the samples.  Maximum Margin: In support vector machines the decision boundary is chosen to be the one forwhich the margin is maximized.The distance of a point xn to the decision surface is given by\\\\[\\\\begin{aligned} d = \\\\frac {t_n y(\\\\boldsymbol{x}_n)}{\\\\|\\\\boldsymbol{w}\\\\|} = \\\\frac {t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b)}{\\\\|\\\\boldsymbol{w}\\\\|} \\\\end{aligned}\\\\]Thus the maximum margin solution is found by solving\\\\[\\\\begin{aligned} \\\\arg \\\\max_{\\\\boldsymbol{w}, b} \\\\left \\\\{ \\\\frac 1{\\\\|\\\\boldsymbol{w}\\\\|} \\\\min_n[t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b)] \\\\right\\\\} \\\\end{aligned}\\\\]      We can see, that if let $\\\\boldsymbol{w}=k\\\\boldsymbol{w},b=kb$, then the distance $d$ is unchanged. So we can use this freedom to set:\\\\[\\\\begin{aligned} t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b) = 1 \\\\end{aligned}\\\\]    for the point that is closest to the surface.                  And then, all data points will statisfy the constraints\\\\[\\\\begin{aligned} t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b) \\\\ge 1 \\\\end{aligned}\\\\]            The optimization problem then simply requires that we maximize $|\\\\boldsymbol{w}|^{-1}$, which is equivalent to minimizing $|\\\\boldsymbol{w}|^2$, and so we have to solve the optimization problem\\\\[\\\\begin{aligned} \\\\arg \\\\min_{\\\\boldsymbol{w}, b}&amp; \\\\qquad \\\\frac 12 \\\\|\\\\boldsymbol{w}\\\\|^2 \\\\\\\\ \\\\text{S.t.}&amp; \\\\qquad t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b) \\\\ge 1 \\\\end{aligned}\\\\]应用拉格朗日乘子法定义拉格朗日函数：\\\\[\\\\begin{aligned} L(\\\\boldsymbol{w}, b, \\\\boldsymbol{u}) = -\\\\frac 12 \\\\|\\\\boldsymbol{w}\\\\|^2 - \\\\sum_{n=1}^N u_n [t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b) - 1]  \\\\end{aligned}\\\\]  Note: 这是一个二次规划问题Setting the derivatives of $L$ with respect to $\\\\boldsymbol{w}, b$ equal to zero, we obtain the following two conditions:\\\\[\\\\begin{aligned} \\\\boldsymbol{w} &amp;= \\\\sum_{n=1}^N a_nt_n\\\\phi(\\\\boldsymbol{x}_n) \\\\\\\\ 0 &amp;= \\\\sum_{n=1}^N a_n t_n \\\\end{aligned}\\\\]Dual Representation and Kernel Function  Eliminating $\\\\boldsymbol{w}, b$ from $L(\\\\boldsymbol{w}, b, \\\\boldsymbol{a})$      得到 Dual Representaion of the Maximum margin problem in which we maximize:\\\\[\\\\begin{aligned} \\\\max&amp; \\\\qquad\\\\tilde{L}(\\\\boldsymbol{a}) = \\\\sum_{n=1}^N a_n - \\\\frac 12 \\\\sum_{n=1}^N\\\\sum_{m=1}^N a_na_mt_nt_m k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) \\\\\\\\ \\\\text{S.t.}&amp;\\\\qquad a_n \\\\ge 0 \\\\\\\\ &amp;\\\\qquad \\\\sum_{n=1}^N a_nt_n = 0 \\\\end{aligned}\\\\]          Here $k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) = \\\\phi(\\\\boldsymbol{x}_n)^T\\\\phi(\\\\boldsymbol{x}_m)$            Classify new data points:\\\\[\\\\begin{aligned} y(\\\\boldsymbol{x}) = \\\\sum_{n=1}^N a_n t_n k(\\\\boldsymbol{x}, \\\\boldsymbol{x}_n) + b \\\\end{aligned}\\\\]  This constrained optimization problem statisfies the Karush-Kuhn-Tucker (KKT) conditions, that is :\\\\[\\\\begin{aligned} a_n &amp;\\\\ge 0 \\\\\\\\ t_ny(\\\\boldsymbol{x}_n) - 1 &amp; \\\\ge 0 \\\\\\\\ a_n\\\\{t_ny(\\\\boldsymbol{x}_n) - 1\\\\} &amp;= 0 \\\\end{aligned}\\\\]      Thus for every data point, either $a_n=0$ or $t_ny(\\\\boldsymbol{x}_n) = 1$        Support Vectors: satisfy $t_ny(\\\\boldsymbol{x}_n) = 1$  解决上述的二次规划问题后，得到$\\\\hat{\\\\boldsymbol{a}}$, 那么对于所有的$a_n \\\\gt 0$的项及对应的$x_n$将构成支持向量集合$\\\\mathcal{S}$根据支持向量集合$\\\\mathcal{S}$计算$b$: using $t_n^2 = 1$\\\\[\\\\begin{aligned} t_n\\\\left(\\\\sum_{m\\\\in \\\\mathcal{S}} a_m t_m k(x_n, x_m) +b \\\\right) &amp;= 1 \\\\\\\\ t_n^2\\\\left(\\\\sum_{m\\\\in \\\\mathcal{S}} a_m t_m k(x_n, x_m) +b \\\\right) &amp;= t_n \\\\end{aligned}\\\\]  we can obtain\\\\[\\\\begin{aligned} b = t_n - \\\\sum_{m\\\\in \\\\mathcal{S}} a_m t_m k(x_n, x_m) \\\\end{aligned}\\\\]  a numerically more stable solution is obtained by averaging:\\\\[\\\\begin{aligned} b = \\\\frac 1{N_{\\\\mathcal{S}}} \\\\sum_{m\\\\in \\\\mathcal{S}} \\\\left(t_n - \\\\sum_{m\\\\in \\\\mathcal{S}} a_m t_m k(x_n, x_m) \\\\right)\\\\end{aligned}\\\\]Error functoin viewFor later comparison with alternative models, we can express the maximummargin classifier in terms of the minimization of an error function, with a simple quadratic regularizer, in the form\\\\[\\\\begin{aligned} \\\\sum_{n=1}^N E_{\\\\infty}(y(\\\\boldsymbol{x_n})t_n - 1) + \\\\lambda \\\\|\\\\boldsymbol{w}\\\\|^2 \\\\end{aligned}\\\\]  $E_{\\\\infty}(z)$ is a function that is zero if $z\\\\ge 0$ else $\\\\infty$ and ensures that $t_n(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n) + b) \\\\ge 1$ are statisfied.Nonlinear Separable caseSlack VariablesIntroduce slack variables $\\\\xi_n \\\\ge 0$  $\\\\xi = 0$: points are on or inside the correct margin boundary  $\\\\xi \\\\in (0,1)$: between correct margin boundary and decision boundary  $\\\\xi = 1$: on the decision boundary  $\\\\xi &gt; 1$: points are missclassifiedNew Classification Constraints: Soft Margin Constraints\\\\[\\\\begin{aligned} t_n y(\\\\boldsymbol{x}_n) \\\\ge 1 - \\\\xi_n \\\\end{aligned}\\\\]  Note: contrast to Hard Margin Constraints  $t_n y(\\\\boldsymbol{x}_n) \\\\ge 1$Original ProblemOur goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary.\\\\[\\\\begin{aligned} \\\\min&amp;\\\\qquad  C\\\\sum_{n=1}^N \\\\xi_n  + \\\\frac 12 \\\\|\\\\boldsymbol{w}\\\\|^2 \\\\\\\\ \\\\text{S.t.} &amp; \\\\qquad t_n y(\\\\boldsymbol{x}_n) \\\\ge 1 - \\\\xi_n \\\\\\\\ &amp;\\\\qquad C &gt; 0 \\\\\\\\ &amp; \\\\qquad \\\\xi_n \\\\ge 0 \\\\end{aligned}\\\\]where the parameter $C &gt; 0$ controls the trade-off between the slack variable penalty and the margin.  Because any point that is misclassified has $\\\\xi_n&gt; 1$ , it follows that $\\\\sum_n \\\\xi_n$ is an upper bound on the number of misclassified points. The parameter $C$ is therefore analogous to (the inverse of) a regularization  coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit $C \\\\rightarrow \\\\infty$, we will recover the earlier support vector machine for separable data.Lagrangian function:\\\\[\\\\begin{aligned} L(\\\\boldsymbol{w}, b, \\\\boldsymbol{a}) = \\\\frac 12 \\\\|\\\\boldsymbol{w}\\\\|^2 + C\\\\sum_{n=1}^N \\\\xi_n -\\\\sum_{n=1}^N a_n(t_n y(\\\\boldsymbol{x}_n) - 1 + \\\\xi_n) -\\\\sum+{n=1}^n \\\\mu_n \\\\xi_n \\\\end{aligned}\\\\]      With KKT conditions:\\\\[\\\\begin{aligned} a_n &amp;\\\\ge 0 \\\\\\\\ t_n y(\\\\boldsymbol{x}_n) - 1 + \\\\xi_n &amp;\\\\ge 0 \\\\\\\\ a_n(t_n y(\\\\boldsymbol{x}_n) - 1 + \\\\xi_n) &amp;= 0 \\\\\\\\ \\\\mu_n &amp;\\\\ge 0\\\\\\\\ \\\\xi_n &amp;\\\\ge 0\\\\\\\\ \\\\mu_n\\\\xi_n &amp;= 0 \\\\end{aligned}\\\\]        optimize out $\\\\boldsymbol{w}$, $b$, and ${\\\\xi_n}$\\\\[\\\\begin{aligned} \\\\frac {\\\\partial L}{\\\\partial \\\\boldsymbol{w}} = 0 &amp;\\\\Rightarrow  \\\\boldsymbol{w} = \\\\sum_{n=1}^N a_nt_n\\\\phi_n \\\\\\\\ \\\\frac {\\\\partial L}{\\\\partial b} = 0 &amp;\\\\Rightarrow \\\\sum_{n=1}^N a_nt_n = 0 \\\\\\\\ \\\\frac {\\\\partial L}{\\\\partial \\\\xi_n} = 0 &amp;\\\\Rightarrow a_n = C-\\\\mu_n \\\\\\\\ &amp;\\\\Rightarrow a_n \\\\le C \\\\end{aligned}\\\\]  Dual Problemeliminate $\\\\boldsymbol{w}$, $b$, and ${\\\\xi_n}$ from $L$\\\\[\\\\begin{aligned} \\\\max&amp; \\\\qquad\\\\tilde{L}(\\\\boldsymbol{a}) = \\\\sum_{n=1}^N a_n - \\\\frac 12 \\\\sum_{n=1}^N\\\\sum_{m=1}^N a_na_mt_nt_m k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) \\\\\\\\ \\\\text{S.t.}&amp;\\\\qquad 0 \\\\le a_n \\\\le C \\\\\\\\ &amp;\\\\qquad \\\\sum_{n=1}^N a_nt_n = 0 \\\\end{aligned}\\\\]  WOW, which is identical to the separable case, except that $a_n$ has a box constraints as $0 \\\\le a_n \\\\le C$.And also, 同样是一个二次规划问题Support Vectors: 在所有的$a_n&gt;0$中，判断：  $a_n &lt; C \\\\Longrightarrow \\\\mu&gt;0 \\\\Longrightarrow \\\\xi_n=0$: on the margin boundary  $a_n = C \\\\Longrightarrow \\\\mu=0 \\\\Longrightarrow \\\\xi_n &gt; 0$:          $\\\\xi \\\\le 1$: correctly clssified      $\\\\xi_n &gt; 1$: misclassified        换句话说，我只对$0&lt; a_n &lt; C$所对应的$\\\\boldsymbol{x}_n$感兴趣……所有总结如下：  先求对偶问题-二次规划解，得到$\\\\hat{\\\\boldsymbol{a}}$  对满足$0&lt; a_n &lt; C$条件的，用来计算$\\\\boldsymbol{w}，b$，和前面一样  其中同样用核函数，来表示$\\\\boldsymbol{w}$所对应的结果，也和前面一样与数据线性可分情况相比，唯一区别在于，限制了$a_n$的最大值，从而引入误分类点和边界内的点对决策边界的影响。如果$C$非常大，那么将等于没有任何影响，等价于解决数据线性可分情况，换句话说，此时忽略了所有误分类点和边界内的点。$\\\\nu$-SVM\\\\[\\\\begin{aligned} \\\\max&amp; \\\\qquad\\\\tilde{L}(\\\\boldsymbol{a}) = - \\\\frac 12 \\\\sum_{n=1}^N\\\\sum_{m=1}^N a_na_mt_nt_m k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) \\\\\\\\ \\\\text{S.t.}&amp;\\\\qquad 0 \\\\le a_n \\\\le \\\\frac 1N \\\\\\\\ &amp;\\\\qquad \\\\sum_{n=1}^N a_n \\\\ge \\\\nu \\\\\\\\ &amp;\\\\qquad \\\\sum_{n=1}^N a_nt_n = 0 \\\\end{aligned}\\\\]This approach has the advantage that the parameter $\\\\nu$, which replaces $C$, can be interpreted as both an upper bound on the fraction of margin errors(points for which $\\\\xi_n &gt; 0$ and hence which lie on the wrong side of the margin boundary and which may or may not be misclassified) and a lower bound on the fraction of support vectors"
  },
  
  {
    "title": "Probabilistic Generative Model",
    "url": "/posts/probabilistic_generative_models/",
    "categories": "Machine Learning, Generative Model",
    "tags": "Logit，Softmax, Naive Bayes",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "Probabilistic Generative Model      Model the class-conditional densities $p(x\\\\vert C_k )$, as well as the class priors $p(C_k)$        The posterior probabilities: can obtained through Bayes’ theorem        $K = 2$\\\\[\\\\begin{aligned} p(C_1\\\\vert x) &amp;= \\\\frac {p(x\\\\vert C_1)p(C_1)}{p(x\\\\vert C_1)p(C_1) + p(x\\\\vert C_2)p(C_2)} \\\\\\\\ &amp;= \\\\frac {1}{1+\\\\exp(-a)} \\\\\\\\&amp;= \\\\sigma(a) \\\\\\\\ \\\\\\\\ a &amp;=\\\\ln \\\\frac {p(x\\\\vert C_1)p(C_1)}{p(x\\\\vert C_2)p(C_2)} = \\\\ln \\\\frac {p(C_1\\\\vert x)}{p(C_2\\\\vert x)} \\\\end{aligned}\\\\]          $a$ represents the log of the ratio of posterior probabilities for the two classes, also known as the log oggs      $\\\\sigma$: logistic sigmoid function            $K \\\\ge 2$\\\\[\\\\begin{aligned} p(C_k\\\\vert x) &amp;= \\\\frac {p(x\\\\vert C_k)p(C_k)}{\\\\sum_j p(x\\\\vert C_j)p(C_j)} \\\\\\\\ &amp;= \\\\frac {\\\\exp(a_k)}{\\\\sum_j \\\\exp(a_j)} \\\\\\\\ \\\\\\\\ a_k &amp;= \\\\ln p(x\\\\vert C_k)p(C_k) \\\\end{aligned}\\\\]  ApplicationContinuous inputs  Under assumption:          share Covariance matrix      normally distributed classes            we can obtain a probabilistic linear discriminant model\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x} \\\\vert C_k) = \\\\mathcal{N} (\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}_k, \\\\Sigma) \\\\end{aligned}\\\\]                  Consider the case of two classes\\\\[\\\\begin{aligned} a =  \\\\ln \\\\frac {p(\\\\boldsymbol{x} \\\\vert C_1)p(C_1)}{p(\\\\boldsymbol{x} \\\\vert C_2)p(C_2)} &amp;= \\\\ln \\\\frac {p(\\\\boldsymbol{x} \\\\vert C_1)}{p(\\\\boldsymbol{x} \\\\vert C_2)} + \\\\ln \\\\frac {p(C_1)}{p(C_2)} \\\\\\\\ &amp;= -\\\\frac 12 (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}_1)^T\\\\Sigma^{-1}(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}_1) + \\\\frac 12 (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}_2)^T\\\\Sigma^{-1}(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}_2) + \\\\ln \\\\frac {p(C_1)}{p(C_2)} \\\\\\\\ &amp;= (\\\\boldsymbol{\\\\mu}_1-\\\\boldsymbol{\\\\mu}_2)^T\\\\Sigma^{-1}\\\\boldsymbol{x} -\\\\frac 12 \\\\boldsymbol{\\\\mu}_1\\\\Sigma^{-1}\\\\boldsymbol{\\\\mu}_1 + \\\\frac 12 \\\\boldsymbol{\\\\mu}_2\\\\Sigma^{-1}\\\\boldsymbol{\\\\mu}_2 + \\\\ln \\\\frac {p(C_1)}{p(C_2)} \\\\\\\\ &amp;= \\\\boldsymbol{w}^T\\\\boldsymbol{x} + b \\\\\\\\ \\\\\\\\ \\\\\\\\ \\\\boldsymbol{w} &amp;=  \\\\Sigma^{-1}(\\\\boldsymbol{\\\\mu}_1-\\\\boldsymbol{\\\\mu}_2) \\\\\\\\ b &amp;= -\\\\frac 12 \\\\boldsymbol{\\\\mu}_1\\\\Sigma^{-1}\\\\boldsymbol{\\\\mu}_1 + \\\\frac 12 \\\\boldsymbol{\\\\mu}_2\\\\Sigma^{-1}\\\\boldsymbol{\\\\mu}_2 + \\\\ln \\\\frac {p(C_1)}{p(C_2)} \\\\end{aligned}\\\\]                              Then we have:\\\\[\\\\begin{aligned} p(C_1\\\\vert \\\\boldsymbol{x}) =\\\\sigma(\\\\boldsymbol{w}^T\\\\boldsymbol{x} + b) \\\\end{aligned}\\\\]                                The number of parameters is $2D + \\\\frac {D^2-D}{2} + D + 1= \\\\frac {D^2 + 5D}{2} + 1$                          $\\\\boldsymbol{\\\\mu}_1$、$\\\\boldsymbol{\\\\mu}_2$、$\\\\Sigma$              $p(C_1)$、$p(C_2)$                                                  Consider the case of multiple classes\\\\[\\\\begin{aligned} p(C_k\\\\vert \\\\boldsymbol{x}) &amp;=  \\\\frac {p(x\\\\vert C_k)p(C_k)}{\\\\sum_j p(x\\\\vert C_j)p(C_j)} \\\\\\\\ &amp;= \\\\frac {\\\\exp(a_k)}{\\\\sum_j \\\\exp(a_j)} \\\\\\\\ \\\\\\\\ a_k &amp;=\\\\boldsymbol{w}_k^T\\\\boldsymbol{x} + b_k \\\\\\\\ \\\\\\\\ \\\\boldsymbol{w}_k &amp;= \\\\Sigma^{-1}\\\\boldsymbol{\\\\mu}_k \\\\\\\\ b_k &amp;= -\\\\boldsymbol{\\\\mu}_k^T\\\\Sigma^{-1}\\\\boldsymbol{\\\\mu}_k + \\\\ln p(C_k) \\\\end{aligned}\\\\]                  The number of parameters is $KD + \\\\frac {D^2-D}{2} + D + K-1$                          $\\\\boldsymbol{\\\\mu}_k$、$\\\\Sigma$、$p(C_k)$                                                Maximum Likelihood Estimatation                  Dataset $\\\\mathcal{D}$ and $\\\\mathbf{t}$, $t_n\\\\in {0,1}$            For case of two classes with gaussian and shared covariance assumption                  Let $p(C_1) = \\\\pi \\\\in [0,1]$, then $p(C_2) = 1-\\\\pi$        \\\\[\\\\begin{aligned} p(\\\\mathcal{D} \\\\vert \\\\boldsymbol{\\\\theta}) &amp;= \\\\prod_{n=1}^N p(\\\\boldsymbol{x}_n, C_1)^{t_n}p(\\\\boldsymbol{x}_n, C_2)^{1-t_n} \\\\\\\\ &amp;= \\\\prod_{i=1}^N [\\\\pi \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\mid \\\\boldsymbol{\\\\mu}_1, \\\\Sigma)]^{t_n} [(1-\\\\pi) \\\\mathcal{N}(\\\\boldsymbol{x}_n\\\\mid \\\\boldsymbol{\\\\mu}_2, \\\\Sigma)]^{1-t_n} \\\\end{aligned}\\\\]                  $\\\\boldsymbol{\\\\theta}$ denotes the parameters set ${ \\\\pi,\\\\boldsymbol{\\\\mu}_1 ,\\\\boldsymbol{\\\\mu}_2 ,\\\\Sigma }$                    For case of multiple classes with gaussian and shared covariance assumption                  Let $p(C_k) = \\\\pi_k$, and $\\\\pi_K = 1- \\\\sum_{k=1}^{K-1} \\\\pi_k$                      Let $\\\\mathbf{t}n = (0,…,1,…,0)$, if $C_k = k$, then $t{nk} = t_{n}[k]=1$ else $0$\\\\[\\\\begin{aligned} p(\\\\mathcal{D}\\\\vert \\\\boldsymbol{\\\\theta}) &amp;= \\\\prod_{n=1}^N\\\\prod_{k}^K [\\\\pi_k p(\\\\boldsymbol{x}_n\\\\vert C_k)]^{t_{nk}}   \\\\end{aligned}\\\\]                                Note: if without the assumption of shared covariance matrix, then this would lead to a quadratic discriminant. And the number of covariance matrix will be $\\\\frac {K(D^2+D)}{2}$Discrete inputs      Without any assumption:    \\\\(\\\\begin{aligned} p(x_1,...,x_D) = p(x_1)p(x_2\\\\mid x_1) p(x_3\\\\mid x_1,x_2)...... p(x_D\\\\mid x_1,...,x_{D-1})  \\\\end{aligned}\\\\)          if all features are binary, then in a general distribution, there could be $2^D-1$ parameters for each class            Naive Bayes assumption: the feature values are treated as independed, conditioned on the class $C_k$\\\\[\\\\begin{aligned} p(x_1,...,x_D \\\\vert C_k) = \\\\prod_{i=1}^D p(x_i\\\\vert C_k) \\\\end{aligned}\\\\]          then number of parameter could be $D$ for binary features.            Note: if using naive bayes assumption in continuous inputs with gaussian distribution, this will lead to the covariance matrix to be diagonal, then number of covariance matrix would be $D$ (if also shared), but not $\\\\frac {D^2+D}{2}$ anymore.  "
  },
  
  {
    "title": "Probabilistic Discriminative Models",
    "url": "/posts/probabilistic_discriminative_model/",
    "categories": "Machine Learning, Discriminant Analysis",
    "tags": "IRLS, Logit，Softmax",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "Probabilistic Discriminative Models  Explicitly to use the functional form of the generalized linear model and to determine its parameters by using ML          And in this direct approach, we are maximizing a likelihood function defined through the conditional distribution $p(C_k\\\\mid \\\\boldsymbol{x})$ ,( posterior distribution), which represents a form of discriminative training      Logistic RegressionDefinition  Let $\\\\phi = \\\\phi(\\\\boldsymbol{x})$ be the $M$-dim feature vector.\\\\[\\\\begin{aligned} p(C_1\\\\mid \\\\phi) &amp;= y(\\\\phi) = \\\\sigma(\\\\boldsymbol{w}^T\\\\phi) \\\\\\\\ p(C_2 \\\\mid \\\\phi) &amp;= 1 - y(\\\\phi) = 1 - p(C_1\\\\mid \\\\phi) \\\\end{aligned}\\\\]      Then the number of adjustable parameters in this model would be $M$.    Likelihood Function: For a data set ${\\\\boldsymbol{x}_n, t_n}$, $t_n \\\\in {0,1}$          Let $y_n = \\\\sigma(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_n))$    \\\\[\\\\begin{aligned} p(\\\\mathcal{D} \\\\mid \\\\boldsymbol{w}) &amp;= \\\\prod_{n=1}^N p(C_1\\\\mid \\\\phi_n)^{t_n} p(C_2\\\\mid \\\\phi)^{1-t_n} \\\\\\\\ &amp;= \\\\prod_{n=1}^N y_n^{t_n}(1-y_n)^{1-t_n} \\\\end{aligned}\\\\]        Cross Entropy error : The Negative logrithm of this likelihood function\\\\[\\\\begin{aligned} E(\\\\boldsymbol{w}) = -\\\\ln p(\\\\mathcal{D} \\\\mid \\\\boldsymbol{w}) = \\\\sum_{i=1}^N [t_n\\\\ln y_n + (1-t_n)\\\\ln (1-y_n)] \\\\end{aligned}\\\\]                  The Gradient:\\\\[\\\\begin{aligned} \\\\nabla E(\\\\boldsymbol{w}) &amp;= \\\\frac {dE}{d\\\\boldsymbol{w}} \\\\\\\\ &amp;= \\\\frac {dE}{dy} \\\\frac {dy}{d\\\\sigma}\\\\frac {d\\\\sigma}{da}\\\\frac {da}{d\\\\boldsymbol{w}} \\\\\\\\ &amp;= - \\\\sum_{n=1}^N \\\\frac {t_n}{y_n}y_n(1-y_n)\\\\phi_n+\\\\frac {1-t_n}{1-y_n}·-1·y_n(1-y_n)\\\\phi_n \\\\\\\\ &amp;= -\\\\sum_{n=1}^N (t_n(1-y_n) - (1-t_n)y_n) \\\\phi_n \\\\\\\\ &amp;= \\\\sum_{n=1}^N (y_n - t_n) \\\\phi_n   \\\\end{aligned}\\\\]              Note: Maximimum likelihood can exhibit server over-fitting for data sets that are linearly separable,. This arise because the maximum likelihood solution occurs when the hyperplane corresponding to $\\\\sigma=0.5$, equivalent to $\\\\boldsymbol{w}^T\\\\phi = 0$, separates the two classes and the magnitude of $\\\\boldsymbol{w}$  goes to infinity.Iterative Reweighted Least Squares      There is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid function.        Newton-Raphson method\\\\[\\\\begin{aligned} \\\\boldsymbol{w}^{(new)} = \\\\boldsymbol{w}^{(old)} - H^{-1}\\\\nabla E(\\\\boldsymbol{w})  \\\\end{aligned}\\\\]        Hessian\\\\[\\\\begin{aligned} H = \\\\nabla \\\\nabla E(\\\\boldsymbol{w}) &amp;= \\\\sum_{n=1}^N y_n(1-y_n)\\\\phi_n\\\\phi_n^T \\\\\\\\ &amp;= \\\\Phi^T R \\\\Phi \\\\\\\\ \\\\\\\\ R&amp;= \\\\text{diag}\\\\{y_n(1-y_n)\\\\} \\\\end{aligned}\\\\]          We can see the Hessian matrix is not a constant, but depends on $\\\\boldsymbol{w}$ through the weighting matrix $R$      And using the property $0&lt;y_n&lt;1$, then $\\\\boldsymbol{u}^TH\\\\boldsymbol{u} &gt; 0$. So the error function is a concave function of $\\\\boldsymbol{w}$ and hence has a unique minimum.            Iterative Reweighted Least Squares\\\\[\\\\begin{aligned} \\\\boldsymbol{w}^{(new)} &amp;= \\\\boldsymbol{w}^{(old)} - H^{-1}\\\\nabla E(\\\\boldsymbol{w})\\\\\\\\ &amp;= \\\\boldsymbol{w}^{(old)} - (\\\\Phi^T R \\\\Phi)^{-1}\\\\Phi^T(\\\\mathbf{y} - \\\\mathbf{t}) \\\\\\\\ &amp;= (\\\\Phi^T R \\\\Phi)^{-1}(\\\\Phi^T R \\\\Phi\\\\boldsymbol{w}^{(old)} - \\\\Phi^T(\\\\mathbf{y} - \\\\mathbf{t})) \\\\\\\\ &amp;= (\\\\Phi^T R \\\\Phi)^{-1}\\\\Phi^T R \\\\boldsymbol{z} \\\\\\\\ \\\\\\\\ \\\\boldsymbol{z} &amp;= \\\\Phi\\\\boldsymbol{w}^{(old)} - R^{-1}(\\\\mathbf{y} - \\\\mathbf{t})  \\\\end{aligned}\\\\]                  Now, we see why this method be called “Reweighted Least Squares”                    Further more, the $R$ can be interpreted as variances, just like the weighted Least Square method.\\\\[\\\\begin{aligned} t&amp;\\\\sim \\\\text{Bern}(t \\\\mid y) \\\\\\\\ \\\\mathbb{E} &amp;= y \\\\\\\\ \\\\text{var}[t] &amp;= y(1-y) \\\\end{aligned}\\\\]                  Addition: Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution.                  Latent Variable Interpretation                    latent variable $y’ = wx + w_0 + \\\\varepsilon$\\\\[\\\\begin{aligned} y = \\\\left\\\\{ \\\\begin{aligned} &amp;1\\\\qquad &amp;wx+w_0+\\\\varepsilon &gt;0 \\\\\\\\ &amp;0\\\\qquad &amp;\\\\text{else} \\\\end{aligned} \\\\right. \\\\end{aligned}\\\\]                    where $\\\\varepsilon$ is an error distributed by the standard logistic distribution.            Multiclass Logistic Regression (Softmax Regression)      Definition\\\\[\\\\begin{aligned} p(C_k\\\\mid \\\\phi) &amp;= y_k(\\\\phi) = \\\\frac {\\\\exp(a_k)}{\\\\sum_j \\\\exp(a_j)} \\\\\\\\ a_k &amp;= \\\\boldsymbol{w}^T_k\\\\phi(\\\\boldsymbol{x}) \\\\end{aligned}\\\\]        Likelihood Funciton\\\\[\\\\begin{aligned} p(\\\\mathcal{D} \\\\mid \\\\boldsymbol{w}_1, ..., \\\\boldsymbol{w}_K) &amp;= \\\\prod_{n=1}^N\\\\prod_{k=1}^K p(C_k\\\\mid \\\\phi_n)^{t_{nk}} = \\\\prod_{n=1}^N\\\\prod_{k=1}^K y_{nk}^{t_{nk}} \\\\end{aligned}\\\\]        Cross Entropy error\\\\[\\\\begin{aligned} E(\\\\boldsymbol{w}_1, ..., \\\\boldsymbol{w}_K) &amp;= -\\\\ln p(\\\\mathcal{D} \\\\mid \\\\boldsymbol{w}_1, ..., \\\\boldsymbol{w}_K) \\\\\\\\&amp;= -\\\\sum_{n=1}^N\\\\sum_{k=1}^K t_{nk}\\\\ln y_{nk} \\\\end{aligned}\\\\]        Partial Derivative                  $\\\\frac {\\\\partial y_{nk}}{\\\\partial a_j}$                              if $k\\\\neq j$\\\\[\\\\begin{aligned} \\\\frac {\\\\partial y_{nk}}{\\\\partial a_j} &amp;= \\\\frac {-\\\\exp(a_j)\\\\exp(a_k)}{(\\\\sum_i \\\\exp(a_i))^2} \\\\\\\\ &amp;= -y_{nj}y_{nk}   \\\\end{aligned}\\\\]                                if $k == j$:\\\\[\\\\begin{aligned} \\\\frac {\\\\partial y_{nk}}{\\\\partial a_j} &amp;= \\\\frac {\\\\exp(a_j)}{\\\\sum_i \\\\exp(a_i)} - \\\\frac {\\\\exp(a_j)^2}{(\\\\sum_i \\\\exp(a_i))^2} \\\\\\\\ &amp;= y_{nj}(1-y_{nj})   \\\\end{aligned}\\\\]                                Then $\\\\frac {\\\\partial y_{nk}}{\\\\partial a_j} = y_{nk}(I_{jk} - y_{nj})$                                $\\\\frac {\\\\partial \\\\ln y_{nk}}{\\\\partial \\\\boldsymbol{w}_j}$\\\\[\\\\begin{aligned} \\\\frac {\\\\partial \\\\ln y_{nk}}{\\\\partial \\\\boldsymbol{w}_j} &amp;= \\\\frac {\\\\partial \\\\ln y_{nk}}{\\\\partial y_{nk}}\\\\frac {\\\\partial y_{nk}}{\\\\partial a_j}\\\\frac {\\\\partial a_j}{\\\\partial \\\\boldsymbol{w}_j} \\\\\\\\ &amp;= \\\\frac {1}{y_{nk}} y_{nk}(I_{jk} - y_{nj})\\\\phi_n \\\\\\\\ &amp;= (I_{jk} - y_{nj})\\\\phi_n  \\\\end{aligned}\\\\]                                      $\\\\nabla_{\\\\boldsymbol{w}_j} E$\\\\[\\\\begin{aligned} \\\\nabla_{\\\\boldsymbol{w}_j} E(\\\\boldsymbol{w}_1, ..., \\\\boldsymbol{w}_K) &amp;= -\\\\sum_{n=1}^N\\\\sum_{k=1}^K t_{nk}\\\\frac {\\\\partial \\\\ln y_{nk}}{\\\\partial \\\\boldsymbol{w}_j} \\\\\\\\ &amp;= -\\\\sum_{n=1}^N\\\\sum_{k=1}^K t_{nk}(I_{jk} - y_{nj})\\\\phi_n \\\\\\\\ &amp;= \\\\sum_{n=1}^N\\\\sum_{k=1}^K t_{nk} y_{nj}\\\\phi_n - \\\\sum_{n=1}^N t_{nj}\\\\phi_n \\\\\\\\ &amp;= \\\\sum_{n=1}^N (y_{nk}-t_{nk})\\\\phi_n \\\\end{aligned}\\\\]                  Hessian Matrix:\\\\[\\\\begin{aligned} \\\\nabla_{\\\\boldsymbol{w}_k}\\\\nabla_{\\\\boldsymbol{w}_j} E(\\\\boldsymbol{w}_1, ..., \\\\boldsymbol{w}_K) &amp;= \\\\sum_{n=1}^N y_{nk}(I_{jk} - y_{nj})\\\\phi_n\\\\phi_n^T  \\\\end{aligned}\\\\]  Probit Regression      The latent variable interpretation:\\\\[\\\\begin{aligned} y = \\\\left\\\\{ \\\\begin{aligned} &amp;1\\\\qquad &amp;wx+w_0+\\\\varepsilon &gt;0 \\\\\\\\ &amp;0\\\\qquad &amp;\\\\text{else} \\\\end{aligned} \\\\right. \\\\end{aligned}\\\\]          where $\\\\varepsilon$ is an error distributed by the standard normal distribution.            In general, let $a_n = \\\\boldsymbol{w}^T\\\\phi_n$\\\\[\\\\begin{aligned} t_n = \\\\left\\\\{ \\\\begin{aligned} &amp;1\\\\qquad &amp;a_n &gt;\\\\theta \\\\\\\\ &amp;0\\\\qquad &amp;\\\\text{else} \\\\end{aligned} \\\\right. \\\\end{aligned}\\\\]                  If the value of $theta$ is drawn from a probability density $p(\\\\theta)$, then the corresponding activation function will be given by the cumulative distributoin function.\\\\[\\\\begin{aligned} f(a) = \\\\int_{-\\\\infty}^a p(\\\\theta) d\\\\theta \\\\end{aligned}\\\\]              As the specific case:          If $p(\\\\theta)$ is logistic distribution, then $f(a)$ would be logistic sigmoid function.              If $p(\\\\theta)$ is standard normal distribution, then $f(a)$ would be probit function\\\\[\\\\begin{aligned} \\\\Phi(a) = \\\\int_{-\\\\infty}^a \\\\mathcal{N}(\\\\theta\\\\mid 0,1)d\\\\theta \\\\end{aligned}\\\\]                  erf function:\\\\[\\\\begin{aligned} \\\\text{erf}(a) = \\\\frac {2}{\\\\sqrt{\\\\pi}} \\\\int_0^a \\\\exp(-\\\\theta^2/2)d\\\\theta \\\\\\\\ \\\\\\\\ \\\\Phi(a) = \\\\frac 12 \\\\left(1+\\\\frac {1}{\\\\sqrt{2}}\\\\text{erf}(a)\\\\right) \\\\end{aligned}\\\\]    The generalized linear model based on a probit activation function is known as probit regression."
  },
  
  {
    "title": "Discriminant Analysis",
    "url": "/posts/discriminant_analysis/",
    "categories": "Machine Learning, Discriminant Analysis",
    "tags": "FLDA, LDA",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "Discriminant AnalysisLeast Square for classification      Each class has a linear model:\\\\[\\\\begin{aligned} y_k = \\\\boldsymbol{w}_k^T\\\\boldsymbol{x}_k + w_0 \\\\end{aligned}\\\\]                  In total:\\\\[\\\\begin{aligned} \\\\boldsymbol{y}(\\\\boldsymbol{x}) = W^T\\\\boldsymbol{x} \\\\end{aligned}\\\\]                    where $W$ is a $M\\\\times K$ matrix                  Error function: Sum of Square error    \\\\(\\\\begin{aligned} E_D(W) = \\\\frac 12 \\\\text{Tr}[(XW-Y)^T(XW-Y)] \\\\end{aligned}\\\\)                  Where $X$ is a $N\\\\times M$ matrix, $Y$ is a $N\\\\times K$ matrix                    Solution:          \\\\[\\\\begin{aligned} \\\\hat{W} = (X^TX)^{-1}X^T Y \\\\end{aligned}\\\\]        The discriminant function:\\\\[\\\\begin{aligned} \\\\boldsymbol{y}(\\\\boldsymbol{x}) &amp;= \\\\hat{W}^T\\\\boldsymbol {x} \\\\\\\\&amp;= Y^TX(X^TX)^{-1}(\\\\boldsymbol{x})  \\\\end{aligned}\\\\]  Finsher Linear Discriminant      有监督降维降维方法    Assumptions:          normally distributed classes      equal class covariances            Project $\\\\boldsymbol{x}$ down to K-dimension, and maximizes the class separation          Maximize the projected class means ( maximize the between-calss variance) and minimize the within-class variance      Fisher criterion: The ratio of between-calss variance and within-class variance            Two class case:                  Project $\\\\boldsymbol{x}$ down to one dimension using:\\\\[\\\\begin{aligned} y = \\\\boldsymbol{w}^T\\\\boldsymbol{x} \\\\end{aligned}\\\\]                    Fisher Criterion\\\\[\\\\begin{aligned} J(\\\\boldsymbol{w}) &amp;= \\\\frac {[\\\\boldsymbol{w}^T(\\\\boldsymbol{m}_1-\\\\boldsymbol{m}_2)]^2}{s_1^2 + s_2^2} \\\\\\\\\\\\\\\\ s_k^2 &amp;= \\\\sum_{n\\\\in C_k} [\\\\boldsymbol{w}^T(\\\\boldsymbol{x}_n - \\\\boldsymbol{m}_k)]^2  \\\\end{aligned}\\\\]                    also, we can write:\\\\[\\\\begin{aligned} J(\\\\boldsymbol{w}) &amp;= \\\\frac {\\\\boldsymbol{w}^TS_B\\\\boldsymbol{w}}{\\\\boldsymbol{w}^TS_W\\\\boldsymbol{w}} \\\\\\\\ S_B &amp;= (\\\\boldsymbol{m}_2-\\\\boldsymbol{m}_1)(\\\\boldsymbol{m}_2-\\\\boldsymbol{m}_1) \\\\\\\\ S_W &amp;= \\\\sum_k \\\\sum_{n\\\\in C_K} (\\\\boldsymbol{x}_n-\\\\boldsymbol{m}_k) (\\\\boldsymbol{x}_n-\\\\boldsymbol{m}_k)^T \\\\end{aligned}\\\\]                    Differentiation with respect to $\\\\boldsymbol{w}$ and equate to zero, we can obtain:\\\\[\\\\begin{aligned} S_B\\\\boldsymbol{w} &amp;= \\\\lambda S_W \\\\boldsymbol{w} \\\\\\\\ \\\\boldsymbol{w} &amp;\\\\propto S_W^{-1}(\\\\boldsymbol{m}_2-\\\\boldsymbol{m}_1) \\\\end{aligned}\\\\]                              通常只在乎$\\\\boldsymbol{w}$的方向, 所以:\\\\[\\\\boldsymbol{w} = S_W^{-1}(\\\\boldsymbol{m}_2-\\\\boldsymbol{m}_1)\\\\]                                    Multiple class case                  Projection Matrix $W$ which maps from $D$ to $L$ so as to maximize:\\\\[\\\\begin{aligned}  J(W) &amp;= \\\\text{Tr}\\\\left\\\\{ (WS_W W^T)^{-1} (WS_BW^T) \\\\right\\\\} \\\\\\\\\\\\\\\\ S_B &amp;= \\\\sum_k \\\\frac {N_k}{N} (\\\\boldsymbol{m}_k-\\\\boldsymbol{m})(\\\\boldsymbol{m}_k-\\\\boldsymbol{m})^T \\\\\\\\ S_{k} &amp;= \\\\frac 1{N_k} \\\\sum_{n\\\\in C_k} (\\\\boldsymbol{x}_n-\\\\boldsymbol{m})(\\\\boldsymbol{x}_n-\\\\boldsymbol{m})^T \\\\\\\\ S_W &amp;= \\\\sum_k \\\\frac {N_k}{N} S_{k}  \\\\end{aligned}\\\\]                    Solution:\\\\[\\\\begin{aligned} W &amp;= S_W^{-1/2}U \\\\end{aligned}\\\\]                  $U$ are the $L$ leading eigenvactors of $S_W^{-1/2}S_BS_W^{-1/2}$, and assume $S_W$ is non-singular.                    Perceptron Algorithm\\\\[\\\\begin{aligned} y(\\\\boldsymbol{x}) = f(\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}))  \\\\end{aligned}\\\\]      $f(a)$ is a step function, if $a\\\\ge 0, f(a) = 1$, else $-1$        Error Function: Perceptron Criterion\\\\[\\\\begin{aligned} E(\\\\boldsymbol{w}) = -\\\\sum_{n\\\\in \\\\mathcal{M}}\\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x})t_n  \\\\end{aligned}\\\\]          Where $\\\\mathcal{M}$ denotes the set of all misclassified patterns.            Optimal: SGD\\\\[\\\\begin{aligned} \\\\boldsymbol{w}^{(n+1)} = \\\\boldsymbol{w}^{(n)} - \\\\eta \\\\nabla E = \\\\boldsymbol{w}^{(n)} + \\\\eta \\\\phi_n t_n \\\\end{aligned}\\\\]  "
  },
  
  {
    "title": "Graphical Models",
    "url": "/posts/graphical_model/",
    "categories": "Machine Learning, Graphical Models",
    "tags": "DAG",
    "date": "2022-02-20 23:00:00 +0000",
    





    "snippet": "Graphical ModelsDirected Graphical ModelAlso known as Bayesian Networks.Directed Acyclic Graphs (DAG): The directed graphs that we are considering are subject to an important restriction namely that there must be no directed cycles.D-Separation:      We consider all posible paths from any Node in $B$ o any node in $B$.    Any such path is said to be blocked if it includes a node such that either          The arrows on the path meet eigher head-to-tail or tail-to-tail at the node, and the node is in the set $C$      The arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in the set $C$.            then we can have:\\\\[A \\\\perp\\\\!\\\\!\\\\!\\\\perp B \\\\mid C\\\\]  Markov Blanket:  The set of nodes comprising the parents, the children and the co-parents is called the Markov Blanket.  We can think of the Markov Blanket of a node $x$ as being the minimal set of nodes that isolates $x$ from the rest of the Graph.Undirected Graphical ModelAlso known as Markov Network or Markov Random Field.Conditional Independence PropertiesIf all paths pass through one or mode nodes in set $C$, the nall such paths are ‘blocked’ and so the conditional independence property holds.\\\\[A \\\\perp\\\\!\\\\!\\\\!\\\\perp B \\\\mid C\\\\]Markov Blanket: for an undirected graph, the Markov Blacket of a node $x$ consists of the neighbouring nodes.Factorization Properties\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) = \\\\frac 1Z \\\\prod_{C} \\\\psi_C(\\\\boldsymbol{x}_C) \\\\end{aligned}\\\\]  $C$ denotes a clique  $\\\\boldsymbol{x}_C$ denotes the set of variables in clique $C$.  $\\\\psi_C$: called the potential functions, it should be strictly positive.      $Z$: called the partition function\\\\[\\\\begin{aligned} Z = \\\\sum_{\\\\boldsymbol{x}} \\\\prod_{C} \\\\psi_C(\\\\boldsymbol{x}_C) \\\\end{aligned}\\\\]  We can use Hammersley-Clifford theorem to proof this.Because we restricted to potential functions which are strictly positive it is convenient to express them as exponentials, so that:\\\\[\\\\begin{aligned} \\\\psi_C(\\\\boldsymbol{x}_C) = \\\\exp(-E(\\\\boldsymbol{x}_C)) \\\\end{aligned}\\\\]  where $E(\\\\boldsymbol{x}_C)$ is called an energy function, and the exponential representation is called the Boltzmann distribution.The join distributino is defined as the product of potentials, and so the total energy is obtained by adding the erergies of each of the maximal cliques.Moralization&quot;Moralization&quot;: to covert a directed graph into an undirected graph    - add additional undirected links between all pairs of parents of each node in the graph    - and then drop the arrows on the original links to give the moral graph.D-map、I-map and Perfect mapD-map (dependence):  A graph is said to be a D map of a distribution if every conditional independence statement satisfied by the distribution is reflected in the graph.I-map (independence)：A graph is said to be an I map of a distribution if every conditional independence statement implied by a graph is satisfied by that specific distribution.Perfect Map: It is both an I map and D map.Inference in Graphical ModelsInference on chains, treestrees:  Undirected tree  Directed tree  PolytreeFactor graphSum-Product algorithmFor any graph which has tree structure, we can use sum-product algorithm to sufficient compute exact marginal or joint probabilities of nodes.This is an exact inference method, and also known as belief propagatino on directed graphs without loops.Marginals\\\\[\\\\begin{aligned} p(x) = \\\\sum_{\\\\boldsymbol{x}\\\\smallsetminus x} p(\\\\boldsymbol{x}) \\\\end{aligned}\\\\]Using factor graph:\\\\[\\\\begin{aligned} p(x) &amp;= \\\\sum_{\\\\boldsymbol{x}\\\\smallsetminus x} p(\\\\boldsymbol{x}) \\\\\\\\ &amp;= \\\\sum_{\\\\boldsymbol{x}\\\\smallsetminus x} \\\\prod_{s\\\\in \\\\text{ne}(x)} F_s(x, \\\\mathbf{X}_s) \\\\\\\\ &amp;= \\\\prod_{s\\\\in \\\\text{ne}(x)}\\\\sum_{\\\\mathbf{X}_s} F_s(x, \\\\mathbf{X}_s) \\\\end{aligned}\\\\]  Note: here the graph must be a tree structure graph, and then we can interchange the sums and products.Then we defined:      message from factor node $f_s$ to variables node $x$:\\\\[\\\\begin{aligned} p(x) &amp;= \\\\sum_{\\\\boldsymbol{x}\\\\smallsetminus x} \\\\prod_{s\\\\in \\\\text{ne}(x)} F_s(x, \\\\mathbf{X}_s) \\\\\\\\ &amp;= \\\\prod_{s\\\\in \\\\text{ne}(x)} \\\\left[\\\\sum_{\\\\mathbf{X}_s} F_s(x, \\\\mathbf{X}_s)\\\\right] \\\\\\\\ &amp;= \\\\prod_{s\\\\in \\\\text{ne}(x)} \\\\mu_{f_s\\\\rightarrow x} (x) \\\\end{aligned}\\\\]        message from variables node $x_m$ to factor node $f_s$:\\\\[\\\\begin{aligned} \\\\mu_{x_m \\\\rightarrow f_s}(x_m) &amp;= \\\\sum_{\\\\mathbf{X}_{sm}} G_m(x_m, \\\\mathbf{X}_{sm}) \\\\\\\\ \\\\\\\\ G_m(x_m, \\\\mathbf{X}_{sm}) &amp;= \\\\prod_{l\\\\in \\\\text{ne}(x_m)\\\\smallsetminus f_s} F_l(x_m, \\\\mathbf{X}_{ml}) \\\\\\\\\\\\\\\\ \\\\mu_{x_m \\\\rightarrow f_s}(x_m) &amp;=  \\\\prod_{l\\\\in \\\\text{ne}(x_m)\\\\smallsetminus f_s} \\\\sum_{\\\\mathbf{X}_{sm}} F_l(x_m, \\\\mathbf{X}_{ml}) \\\\\\\\&amp;= \\\\prod_{l\\\\in \\\\text{ne}(x_m)\\\\smallsetminus f_s} \\\\mu_{f_l \\\\rightarrow x_m}(x_m) \\\\end{aligned}\\\\]        Leaf Node:\\\\[\\\\begin{aligned} \\\\mu_{x\\\\rightarrow f}(x) &amp;= 1 \\\\\\\\ \\\\mu_{f\\\\rightarrow x} &amp;= f(x) \\\\end{aligned}\\\\]  Joint distribution in factors\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}_s) = f_s(\\\\boldsymbol{x}_s) \\\\prod_{i\\\\in \\\\text{ne}(f_s)} \\\\mu_{x_i \\\\rightarrow f_s}(x_i) \\\\end{aligned}\\\\]Max-Sum algorithmPurpose: To find a setting of the variables that has the largest probability and to find the value of that probability\\\\[\\\\begin{aligned} \\\\boldsymbol{x}^{max} =  \\\\arg \\\\max_{\\\\boldsymbol{x}} p(\\\\boldsymbol{x}) \\\\end{aligned}\\\\]we can write:\\\\[\\\\begin{aligned} \\\\max_{\\\\boldsymbol{x}} p(\\\\boldsymbol{x}) &amp;= \\\\max_{x_1}...\\\\max_{x_N} p(\\\\boldsymbol{x}_N) \\\\\\\\&amp;= \\\\max_{x_1}...\\\\max_{x_N} \\\\prod_{C} \\\\psi_C({\\\\boldsymbol{x}_C}) \\\\\\\\ \\\\\\\\ \\\\ln( \\\\max_{\\\\boldsymbol{x}} p(\\\\boldsymbol{x})) &amp;= \\\\max_{\\\\boldsymbol{x}} \\\\ln p(\\\\boldsymbol{x})) \\\\\\\\ &amp;= \\\\max_{x_1}...\\\\max_{x_N} p(\\\\boldsymbol{x}) \\\\\\\\&amp;= \\\\max_{x_1}...\\\\max_{x_N} \\\\sum_{C} \\\\psi_C({\\\\boldsymbol{x}_C}) \\\\end{aligned}\\\\]then at the roor node:\\\\[\\\\begin{aligned} \\\\max p(\\\\boldsymbol{x}) &amp;= \\\\max \\\\left(\\\\sum_{s\\\\in \\\\text{ne}(x_{root})} \\\\mu_{f_s\\\\rightarrow x_{root}}(x_{root})\\\\right) \\\\end{aligned}\\\\]"
  },
  
  {
    "title": "Proximal Gradient Method",
    "url": "/posts/Proximal_Gradient_Method/",
    "categories": "Optimization",
    "tags": "Optimization, Gradient Method",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Proximal Gradient MethodProximal opeartor      Proximal Opeartor\\\\[{\\\\displaystyle \\\\operatorname {prox}_{f}(v)=\\\\arg \\\\min _{x\\\\in {\\\\mathcal {X}}}\\\\left(f(x)+{\\\\frac {1}{2}}\\\\|x-v\\\\|_{2}^{2}\\\\right).}\\\\]        Compute the Proximal Opeartor                  Let\\\\[g(x) = f(x) + {\\\\frac {1}{2}}\\\\|x-v\\\\|_{2}^{2}\\\\]                    Because $f$ is a covex , and also ${\\\\frac {1}{2}}|x-v|_{2}^{2}$, then this is true that $0\\\\in \\\\partial g(x)$, $\\\\partial g(x)$ is the subgradient of $g(x)$. Then we have:\\\\[\\\\begin{aligned} 0 \\\\in \\\\partial g(x) &amp;\\\\Longleftrightarrow 0 \\\\in \\\\partial f(x) + x-v \\\\end{aligned}\\\\]                    Then we need to find out what conditions $v$ must satisfy to make $\\\\partial f(x) + x-v=0$, and what value should $x$ take.                    Example:                              if $f(x)$ is non-differentiable, for example $f(\\\\boldsymbol{x}) = \\\\sum_i \\\\vert x_i\\\\vert$\\\\[\\\\begin{aligned} \\\\frac {\\\\partial f(\\\\boldsymbol{x})}{\\\\partial x_i} = \\\\left\\\\{ \\\\begin{aligned}&amp;1,\\\\qquad &amp;\\\\text{if} \\\\space x_i &gt;0 \\\\\\\\ &amp;-1, \\\\qquad &amp;\\\\text{if} \\\\space x_i &lt;0 \\\\\\\\ &amp;[-1, +1],\\\\qquad &amp;\\\\text{if} \\\\space x_i = 0 \\\\end{aligned}\\\\right. \\\\end{aligned}\\\\]                                          then let $\\\\frac {\\\\partial f(\\\\boldsymbol{x})}{\\\\partial x_i} + x_i-v_i = 0$, we have:\\\\[\\\\begin{aligned} x_i &amp;= \\\\left\\\\{ \\\\begin{aligned}&amp;v_i - 1, \\\\qquad &amp;v_i &gt; 1 \\\\\\\\ &amp; v_i + 1, \\\\qquad &amp;v_i &lt; -1 \\\\\\\\ &amp; 0 ,\\\\qquad &amp;v_i\\\\in [-1,1] \\\\end{aligned}\\\\right . \\\\\\\\ &amp;= \\\\text{sgn}(v_i)*\\\\max(0,|v_i|-1) \\\\end{aligned}\\\\]                                                          if $f(x)$ is differentiable, for example $f(\\\\boldsymbol{x}) =\\\\boldsymbol{x}^T\\\\boldsymbol{x}$, it is easy to find the result.                              Proximal Gradient method      $f: R^n \\\\rightarrow R$ is a differentiable convex function , $h: R^n\\\\rightarrow R$ is a convex function (maybe non-differentiable), $X$ is a closed convex set.\\\\[\\\\begin{aligned} \\\\text{minimize}&amp; \\\\space f(x) + h(x) \\\\\\\\ \\\\text{Subject\\\\space to}&amp;\\\\space x \\\\in X \\\\end{aligned}\\\\]        Proximal Gradient Descent algorithm combines ideas from the gradient projection method and the proximal method. It replaces $f$ with a linear approximation in the proximal minimization, i.e.,\\\\[\\\\begin{aligned} x_{k+1} \\\\in \\\\arg \\\\min_{x\\\\in X} \\\\left( \\\\nabla f(x_k)^T(x-x_k) + h(x) + \\\\frac {1}{2\\\\alpha_k}\\\\vert\\\\vert x-x_k\\\\vert\\\\vert^2 \\\\right) \\\\end{aligned}\\\\]                  $\\\\alpha_k &gt; 0$ is a parameter, we will see that it can be seen as stepsize. And note that there is an alternative/equivalent way to write this algorithm in the form:\\\\[\\\\begin{aligned} z_k &amp;= x_k - \\\\alpha_k \\\\nabla f(x_k) \\\\\\\\ x_{k+1} &amp;\\\\in \\\\arg\\\\min_{x\\\\in X} \\\\left( h(x)+\\\\frac {1}{2\\\\alpha_k}\\\\vert\\\\vert x-z_k\\\\vert\\\\vert^2 \\\\right) \\\\end{aligned}\\\\]                              This can be verified by expanding the quadratic:\\\\[\\\\begin{aligned} \\\\| x-z_k\\\\|_2 = \\\\| x-x_k + \\\\alpha_k\\\\nabla f(x_k)\\\\|^2 \\\\end{aligned}\\\\]                              Lasso regression problem      Using Proximal Gradient method to optimal (Note we write the $k$ at the top-right in this problem):\\\\[\\\\begin{aligned} \\\\min \\\\space \\\\frac 12 \\\\sum_{i=1}^N (y_i - \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_i))^2 + \\\\eta \\\\sum_{j=1}^M \\\\vert w_j\\\\vert \\\\end{aligned}\\\\]                  Let\\\\[\\\\begin{aligned} E(\\\\boldsymbol{w}) &amp;= \\\\frac {1}{2} \\\\sum_{i=1}^N (y_i - \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_i))^2 \\\\\\\\  p(\\\\boldsymbol{w}) &amp;= \\\\eta \\\\sum_{j=1}^M \\\\vert w_j\\\\vert \\\\end{aligned}\\\\]                    So we can write this problem in this way:\\\\[\\\\begin{aligned} \\\\boldsymbol{w}^{k+1} \\\\in \\\\arg \\\\min_{\\\\boldsymbol{w}} \\\\left(E(\\\\boldsymbol{w}) + p(\\\\boldsymbol{w})  + \\\\frac 1{2\\\\alpha^k} \\\\| \\\\boldsymbol{w}-\\\\boldsymbol{w}^k \\\\|^2 \\\\right) \\\\end{aligned}\\\\]                  Then according to Proximal Gradient method, we have:\\\\[\\\\begin{aligned} \\\\boldsymbol{z}^k &amp;= \\\\boldsymbol{w}^k - \\\\alpha^k \\\\nabla E(\\\\boldsymbol{w}^k) \\\\\\\\ \\\\boldsymbol{w}^{k+1} &amp;\\\\in \\\\arg \\\\min_{\\\\boldsymbol{w}} \\\\left(p(\\\\boldsymbol{w})  + \\\\frac 1{2\\\\alpha^k} \\\\| \\\\boldsymbol{w}-\\\\boldsymbol{z}^k \\\\|^2 \\\\right) \\\\\\\\ w_j^{k+1} &amp;= \\\\left\\\\{ \\\\begin{aligned} &amp;0\\\\qquad &amp;\\\\vert z_j^k \\\\vert \\\\le \\\\eta\\\\alpha^k \\\\\\\\  &amp;z_j^k - \\\\text{sgn}(z_j^k)\\\\alpha^k \\\\eta \\\\qquad &amp;\\\\vert z_j^k \\\\vert \\\\gt \\\\eta\\\\alpha^k \\\\end{aligned} \\\\right. \\\\\\\\ &amp;= \\\\text{sgn}(z_j^k) \\\\max(0, \\\\vert z_j^k \\\\vert - \\\\eta\\\\alpha^k) \\\\end{aligned}\\\\]  Elastic Net Regression Problem\\\\[\\\\begin{aligned} \\\\min \\\\space \\\\frac 12 \\\\sum_{i=1}^N (y_i - \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}_i))^2 + \\\\eta \\\\left((1-\\\\gamma)\\\\| \\\\boldsymbol{w} \\\\|_1 + \\\\gamma \\\\| \\\\boldsymbol{w} \\\\|_2^2 \\\\right)\\\\end{aligned}\\\\]      For $0\\\\le \\\\gamma \\\\le 1$ the penalty term $p(\\\\boldsymbol{w}) = \\\\eta \\\\left((1-\\\\gamma)| \\\\boldsymbol{w} |_1 + \\\\gamma | \\\\boldsymbol{w} |_2^2 \\\\right)$is now strictly convex, and hence the minimization problem now admits a unique solution.        Then also, according to Proximal Gradient method, we have:\\\\[\\\\begin{aligned} \\\\boldsymbol{z}^k &amp;= \\\\boldsymbol{w}^k - \\\\alpha^k \\\\nabla E(\\\\boldsymbol{w}^k) \\\\\\\\ \\\\boldsymbol{w}^{k+1} &amp;\\\\in \\\\arg \\\\min_{\\\\boldsymbol{w}} \\\\left(p(\\\\boldsymbol{w})  + \\\\frac 1{2\\\\alpha^k} \\\\| \\\\boldsymbol{w}-\\\\boldsymbol{z}^k \\\\|^2 \\\\right) \\\\end{aligned}\\\\]                  The result is:\\\\[\\\\begin{aligned}  w_j^{k+1} &amp;= \\\\left\\\\{ \\\\begin{aligned} &amp;0 \\\\qquad &amp;\\\\vert z_j^k \\\\vert \\\\le \\\\eta\\\\alpha^k(1-\\\\gamma) \\\\\\\\  &amp;\\\\frac{z_j^k - \\\\text{sgn}(z_j^k)\\\\eta\\\\alpha^k(1-\\\\gamma)}{1+\\\\eta\\\\alpha^k\\\\gamma} \\\\qquad &amp;\\\\vert z_j^k \\\\vert \\\\gt \\\\eta\\\\alpha^k(1-\\\\gamma) \\\\end{aligned} \\\\right. \\\\\\\\ &amp;= \\\\text{sgn}(z_j^k) \\\\max\\\\left(0, \\\\frac{|z_j^k| - \\\\eta\\\\alpha^k(1-\\\\gamma)}{1+\\\\eta\\\\alpha^k\\\\gamma} \\\\right)  \\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "Probabilistic PCA",
    "url": "/posts/probabilistic_pca/",
    "categories": "Machine Learning, PCA",
    "tags": "PCA, EM",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Probabilistic PCAWe now show that PCA can also be expressed as the maximum likelihood solution of a probabilistic latent variable model.Note: the probabilistic PCA model can be expressed as a directed graphSeveral Advantages of Probabilistic PCA      Probabilistic PCA represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set.        We can derive an EM algorithm for PCA that is computationally efficient in situations where only a few leading eigenvectors are required and that avoids having to evaluate the data covariance matrix as an intermediate step.        The combination of a probabilistic model and EM allows us to deal with missing values in the data set.        Mixtures of probabilistic PCA models can be formulated in a principled way and trained using the EM algorithm.        Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which the dimensionality of the principal subspace can be found automatically from the data.        Probabilistic PCA can be used to model class-conditional densities and hence be applied to classification problems.        The probabilistic PCA model can be run generatively to provide samples from the distribution.  DefinitionProbabilistic PCA is closely related to factor analysis.Probabilistic PCA is a simple example of the linear-Gaussian framework, in which all of the marginal and conditional distributions are Gaussian.We can formulate probabilistic PCA by  first introducing an explicit latent variable $z$ corresponding to the principal-component subspace.  Next we define a Gaussian prior distribution $p(z)$ over the latent variable, together with a Gaussian conditional distribution $p(x\\\\vert z)$ for the observed variable $x$ conditioned on the value of the latent variable.\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{z} \\\\vert 0, I) \\\\\\\\ p(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{z}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{x} \\\\vert \\\\mathbf{W}\\\\boldsymbol{z} + \\\\boldsymbol{\\\\mu}, \\\\sigma^2 I) \\\\end{aligned}\\\\]  because we set the covariance as $\\\\sigma^2I$, then this is an example of native Bayes model.  and we shall see that the columns of $\\\\mathbf{W}$ span a linear subspace with the data space that corresponds to the principal subspace.Generative Viewpoint of Probabilistic PCA  first, choosing a value of the latent variable $\\\\boldsymbol{z}$ from $p(\\\\boldsymbol{z})$  and then sampling $\\\\boldsymbol{x}$ from $p(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{z})$Specifically, the D-dimensional observed variable $\\\\boldsymbol{x}$ is defined by a linear transformation of the M-dimensional latent variable latent variables $$ plus additive Gaussian noise, to that:\\\\(\\\\begin{aligned} \\\\boldsymbol{x} = \\\\mathbf{W}\\\\boldsymbol{z} + \\\\boldsymbol{\\\\mu} + \\\\boldsymbol{\\\\epsilon} \\\\end{aligned}\\\\)  $\\\\boldsymbol{\\\\epsilon} \\\\sim \\\\mathcal{N}(\\\\boldsymbol{\\\\epsilon} \\\\vert 0, \\\\sigma^2 I)$Note that this framework is based on a mapping from latent space to data space, in contrast to the more conventional view of PCA.The Reverse mapping, from data space to the lantent space, will be obtained shortly using Bayes’ theorem.Predictive Distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) &amp;= \\\\int p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{z})p(\\\\boldsymbol{z})d\\\\boldsymbol{z} \\\\\\\\ &amp;= \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{C}) \\\\\\\\\\\\\\\\ C &amp;= \\\\mathbf{W}\\\\mathbf{W}^T + \\\\sigma^2 I  \\\\end{aligned}\\\\]Note: when we evaluate the predictive distributino, we require $\\\\mathbf{C}^{-1}$, which involves the inversion of a $D\\\\times D$ matrix.      But if we use the Woodbury matrix identity on $C$,\\\\[\\\\begin{aligned} \\\\left(A+UCV\\\\right)^{-1} &amp;= A^{-1}-A^{-1}U\\\\left(C^{-1}+VA^{-1}U\\\\right)^{-1}VA^{-1} \\\\end{aligned}\\\\]        we have\\\\[\\\\begin{aligned} \\\\mathbf C^{-1} &amp;= (\\\\mathbf{W}\\\\mathbf{W}^T + \\\\sigma^2)^{-1} \\\\\\\\&amp;= \\\\frac 1{\\\\sigma^2} \\\\left(I - \\\\frac 1{\\\\sigma^2} \\\\mathbf{W}(I+\\\\sigma^2\\\\mathbf{W}^T\\\\mathbf{W})^{-1}\\\\mathbf{W}^T \\\\right) \\\\end{aligned}\\\\]          then the cost of evaluating $\\\\mathbf C^{-1}$ is reduced from $O(D^3)$ to $O(M^3)$ by $(I+\\\\sigma^2W^TW)^{-1}$      Posterior DistributionLet $\\\\mathbf{M} = (I+\\\\sigma^2\\\\mathbf{W}^T\\\\mathbf{W})^{-1}$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z} \\\\vert \\\\boldsymbol{x}) = \\\\mathcal{N}(\\\\boldsymbol{z} \\\\vert \\\\mathbf{M}^{-1} \\\\mathbf{W}^T(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}), \\\\sigma^2 \\\\mathbf{M}) \\\\end{aligned}\\\\]      Proof:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z} \\\\vert \\\\boldsymbol{x}) &amp;= \\\\frac {p( \\\\boldsymbol{x} \\\\vert \\\\boldsymbol{z}) p(\\\\boldsymbol{z})}{p(\\\\boldsymbol{x})} \\\\\\\\&amp;\\\\propto \\\\exp\\\\left\\\\{-\\\\frac 12 \\\\boldsymbol{z}^T\\\\boldsymbol{z} - \\\\frac {1}{2\\\\sigma^2}\\\\left( \\\\boldsymbol{z}^T\\\\mathbf{W}^T\\\\mathbf{W}\\\\boldsymbol{z} -2\\\\boldsymbol{z}^T\\\\mathbf{W}^T(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}) \\\\right) \\\\right\\\\} \\\\\\\\ &amp;\\\\propto \\\\exp\\\\left\\\\{ -\\\\frac {1}{2\\\\sigma^2}(\\\\boldsymbol{z}^TM^{-1}\\\\boldsymbol{z} - 2\\\\boldsymbol{z}^T\\\\mathbf{W}^T(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})) \\\\right\\\\} \\\\\\\\ \\\\\\\\ p(\\\\boldsymbol{z} \\\\vert \\\\boldsymbol{x}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{z} \\\\vert \\\\mathbf{M}^{-1} \\\\mathbf{W}^T(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}), \\\\sigma^2 \\\\mathbf{M}) \\\\end{aligned}\\\\]  And we can see that poseterior covariance is independent of $\\\\boldsymbol{x}$Inference - Maximum LikelihoodUsing Maximum LikelihoodGiven a data set $\\\\mathbf{X}$, according to the prediction distribution $p(\\\\boldsymbol{x}) \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{C})$, the log likelihood function is:\\\\[\\\\begin{aligned} \\\\ln p(\\\\mathbf{X} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{W}, \\\\sigma^2) &amp;= \\\\sum_{n=1}^N \\\\mathcal{N}(\\\\boldsymbol{x}_n \\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{C}) \\\\\\\\ &amp;= -\\\\frac {ND}{2} \\\\ln (2\\\\pi) - \\\\frac N2 \\\\ln \\\\vert \\\\mathbf{C} \\\\vert -\\\\frac 12 \\\\sum_{n=1}^N (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\mathbf{C}^{-1} (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\end{aligned}\\\\]      setting the derivative with respect to $\\\\boldsymbol{\\\\mu}$ equal to zero, we can obtain $\\\\boldsymbol{\\\\mu} = \\\\overline{\\\\boldsymbol{x}}$, and then we have:\\\\[\\\\begin{aligned} \\\\ln p(\\\\mathbf{X} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{W}, \\\\sigma^2) &amp;= -\\\\frac {N}{2} (\\\\ln (2\\\\pi) + \\\\ln \\\\vert \\\\mathbf{C} \\\\vert + \\\\text{Tr}(C^{-1}X^TX) \\\\end{aligned}\\\\]        Maximization with respect to $\\\\mathbf{W}$ and $\\\\sigma^2$ is more complex but nonetheless has an exact closed-form solution It was shown by Tipping and Bishop (1999b) that all of the stationary points of the log likelihood function can be written as:\\\\[\\\\begin{aligned} \\\\mathbf{W}_{ML} = U_M(L_M - \\\\sigma^2I)^{1/2} R \\\\end{aligned}\\\\]          $U_M$ is a $D\\\\times M$ matrix whose columns are given by any subset of size $M$ of the eigenvectors of data covariance matrix $X^TX$      $L_M$ is the $M\\\\times M$ diagonal matrix with eigenvalues $\\\\lambda_i$      $R$ is an arbitrary $M\\\\times M$ orthogonal matrix.      Furthermore, Tipping and Bishop (1999b) showed that the maximum of the likelihood function is obtained when the M eigenvectors are chosen to be those whose eigenvalues are the M largest (all other solutions being saddle points).assume that the eigenvectors have been arranged in order of decreasing values of the corresponding eigenvalues, so that the M principal eigenvectors are $\\\\boldsymbol{\\\\mu}_1, …,\\\\boldsymbol{\\\\mu}_M$. In this case, the columns of W define the principal subspace of standard PCA, and :\\\\[\\\\begin{aligned} \\\\sigma^2_{ML} = \\\\frac 1{D-M} \\\\sum_{i=M+1}^D \\\\lambda_i \\\\end{aligned}\\\\]so that $\\\\sigma^2_{ML}$ is the average variance associated with the discarded dimensions.Rotation InvariantyIf we set a new $\\\\widetilde{\\\\mathbf W} = \\\\mathbf{WR}$, then:\\\\[\\\\widetilde{\\\\mathbf W}\\\\widetilde{\\\\mathbf W}^T = \\\\mathbf{WR}\\\\mathbf{R^T} \\\\mathbf{W^T} = \\\\mathbf{W}\\\\mathbf{W^T}\\\\]then we can see $\\\\mathbf{C}$ is independent of $\\\\mathbf{R}$This simply says that the predictive density is unchanged by rotations in the latent spaceand for the particular case of $\\\\mathbf{R} = I$, we see that the columns of $\\\\mathbf W$ are the principal component eigenvectors scaled by the variance parameters $\\\\lambda_i - \\\\sigma^2$ML Solution AnalysisIt is worth taking a moment to study the form of the covariance matrix given by(待更)EM algorithm for PCAComplete-Data log likelihood function\\\\[\\\\begin{aligned} \\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{W}, \\\\sigma^2) = \\\\sum_{n=1}^N \\\\{\\\\ln p(\\\\boldsymbol{x}_n \\\\vert \\\\boldsymbol{z}_n) + \\\\ln p(\\\\boldsymbol{z}_n)\\\\} \\\\end{aligned}\\\\]we already know that the exact maximum likelihood solution for $\\\\boldsymbol{\\\\mu}$ is given by the sample mean $\\\\overline{\\\\boldsymbol{x}}$, and it is convenient to substitute for $\\\\boldsymbol{\\\\mu}$ at this stage.Expectation step of Complete-Data LLtaking the expectation with respect to the posterior distribution over the latent variables, we obtain\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{W}, \\\\sigma^2)] \\\\space =&amp; -\\\\sum_{n=1}^N \\\\{\\\\frac 12 \\\\text{Tr}(\\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n^T]) + \\\\frac D2 \\\\ln(2\\\\pi\\\\sigma^2) +   \\\\\\\\ &amp; +\\\\frac {1}{2\\\\sigma^2}\\\\|\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu}\\\\|^2 -\\\\frac {1}{\\\\sigma^2}\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T\\\\mathbf{W}^T(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu}) \\\\\\\\ &amp; + \\\\frac {1}{2\\\\sigma^2}\\\\text{Tr}(\\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n]^T\\\\mathbf{W}^T\\\\mathbf{W}) \\\\}  \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[\\\\boldsymbol{z}_n] \\\\space =&amp; \\\\mathbf{M^{-1}}\\\\mathbf{W}^T(\\\\boldsymbol{x}_n-\\\\overline{\\\\boldsymbol{x}}) \\\\\\\\  \\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n^T] \\\\space =&amp; \\\\sigma^2\\\\mathbf{M^{-1}} + \\\\mathbb{E}[\\\\boldsymbol{z}_n]\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T \\\\end{aligned}\\\\]Maximization stepmaximize with respect to  $\\\\mathbf{W}_{new}$  $\\\\sigma_{new}^2$\\\\[\\\\begin{aligned} \\\\mathbf{W}_{new}&amp;= \\\\left[ \\\\sum_{n=1}^N \\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n]  \\\\right]^{-1} \\\\left[ \\\\sum_{n=1}^N (\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu})\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T  \\\\right] \\\\\\\\ \\\\sigma_{new}^2 &amp;= \\\\frac {1}{ND}\\\\sum_{n=1}^N \\\\left\\\\{ \\\\|\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu}\\\\|^2 -2\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T\\\\mathbf{W}_{new}^T(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu}) + \\\\text{Tr}(\\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n]^T\\\\mathbf{W}_{new}^T\\\\mathbf{W}_{new}) \\\\right\\\\} \\\\end{aligned}\\\\]Advantages of EM in PCA  One of the benefits of the EM algorithm for PCA is computational efficiency for large-scale applications  Note that this EM algorithm can be implemented in an on-line form in which each D-dimensional data point is read in and processed and then discarded before the next data point is considered.  Because we now have a fully probabilistic model for PCA, we can deal with missing data, provided that it is missing at random, by marginalizing over the distribution of the unobserved variables.EM for standard PCA  Another elegant feature ofthe EM approach is that we can take the limit $\\\\sigma^2 \\\\rightarrow 0$, corresponding to standard PCA, and still obtain a valid EM-like algorithm\\\\[\\\\begin{aligned} \\\\mathbf{M} &amp;= \\\\mathbf{W}^T\\\\mathbf{W}  \\\\\\\\ \\\\mathbf{\\\\Omega} &amp;= (\\\\mathbf{W}_{old}^T\\\\mathbf{W}_{old})^{-1}\\\\mathbf{W}_{old}^T\\\\widetilde{X} \\\\\\\\ \\\\mathbf{W}_{new}&amp;= \\\\widetilde{X}\\\\mathbf{\\\\Omega}^T(\\\\mathbf{\\\\Omega}\\\\mathbf{\\\\Omega}^T)^{-1} \\\\end{aligned}\\\\]Again these can be implemented in an on-line form.\\\\[\\\\begin{aligned} \\\\end{aligned}\\\\]"
  },
  
  {
    "title": "Principal Component Analysis",
    "url": "/posts/principal_component_analysis/",
    "categories": "Machine Learning, PCA",
    "tags": "PCA",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Principal Component AnalysisMaximum Variance formulationPCA can be defined as the Orthogonal projection of the data into a lower dimensional linear space, known as the principal subspace, such that the variance of the projected data is maximized.      for $M = 1$, suppose the direction of this space is $\\\\boldsymbol{u}_1$, and $|\\\\boldsymbol{u}_1|^2=1$\\\\[\\\\begin{aligned} &amp;\\\\overline{\\\\boldsymbol{x}} = \\\\frac 1N\\\\sum_{n=1}^N x_n \\\\\\\\ &amp;\\\\frac 1N \\\\sum_{n=1}^N \\\\{ \\\\boldsymbol{u}_1^T\\\\boldsymbol{x}_n - \\\\boldsymbol{u}_1^T\\\\overline{\\\\boldsymbol{x}}\\\\} = \\\\boldsymbol{u}_1^T S\\\\boldsymbol{u}_1 \\\\\\\\ &amp; S = \\\\frac 1N \\\\sum_{n=1}^N (\\\\boldsymbol{x_n}-\\\\overline{\\\\boldsymbol{x}})(\\\\boldsymbol{x_n}-\\\\overline{\\\\boldsymbol{x}})^T \\\\end{aligned}\\\\]        Note: the term $\\\\boldsymbol{u}_1^T S\\\\boldsymbol{u}_1$ is the variance of the projected data, to maximize it we need to introduce a Lagrange multiplier:    \\\\(\\\\begin{aligned} \\\\max \\\\qquad \\\\boldsymbol{u}_1^T + \\\\lambda S\\\\boldsymbol{u}_1 + \\\\lambda_1(1-\\\\boldsymbol{u}_1^T\\\\boldsymbol{u}_1) \\\\end{aligned}\\\\)                  By setting the derivative with respect $\\\\boldsymbol{u}_1$ to zero, we have:\\\\[\\\\begin{aligned} S\\\\boldsymbol{u}_1 = \\\\lambda_1 \\\\boldsymbol{u}_1 \\\\end{aligned}\\\\]                    this means that firstly the $\\\\boldsymbol{u}_1$ must be an eigenvector of $S$                    and further more, we also can see:\\\\[\\\\begin{aligned} \\\\boldsymbol{u}_1^TS\\\\boldsymbol{u}_1 = \\\\lambda_1 \\\\end{aligned}\\\\]                    this means that $\\\\lambda_1$ is the largest eigenvalue of $S$ and it corresponds to the eigenvector $\\\\boldsymbol{u}_1$                  Then we can easily draw the conclusion that orthogonal projection of the data into a lower $M$-dimensional linear space, from the viewpoint of maximizing the variance of the projected data, and this problem equals to find the $M$-largest eigenvalues and the corresponding eigenvectors.  Minimum-error formulationWe now discuss an alternative formulation of PCA based on projection error minimization.We define:  $X$: $N\\\\times D$ certered data set matrix  $Z$: $(D-M)\\\\times D$ linear transform matrixand if we also denote:  $\\\\mathcal{Y}$ represents the $M$-dimension Orthogonal Projection Subspace, i.e. principal subspace.  $\\\\boldsymbol{z}_n = Z\\\\boldsymbol{x}_n, \\\\boldsymbol{z}_n \\\\in \\\\mathcal{Y}^{\\\\perp}$in PCA, the best orthogonal projection problem can be writen as minimizing the distortion measure $J$:\\\\[\\\\begin{aligned} \\\\min &amp;\\\\qquad J(Z) = \\\\frac 1N \\\\sum_{n=1}^N \\\\|Z\\\\boldsymbol{x}_n\\\\|^2 = \\\\text{Tr}(XZ^TZX^T) \\\\\\\\ &amp;\\\\qquad \\\\|z_i\\\\|^2 = 1, i=M+1,...,D \\\\end{aligned}\\\\]      Introduce Lagrange multiplier, this is equals to\\\\[\\\\begin{aligned} \\\\min_{Z,\\\\boldsymbol{\\\\lambda}} \\\\qquad \\\\text{Tr}(XZ^TZX^T) + \\\\boldsymbol{\\\\lambda}^T(ZZ^T - I) \\\\end{aligned}\\\\]          $\\\\lambda$ is a $D-M$-dim vector            setting derivative respect to $Z$ equals to zero, we have:    \\\\(\\\\begin{aligned} X^TXZ^T &amp;= Z^T\\\\boldsymbol{\\\\lambda}\\\\\\\\ \\\\Longrightarrow X^TX \\\\boldsymbol{z}_i &amp;= \\\\lambda_i \\\\boldsymbol{z}_i, \\\\qquad i=M+1,...,D \\\\end{aligned}\\\\)          we can see that the set of ${\\\\lambda_i}$ are the eigenvaues of $X^TX$ and corresponding to eigenvectors  ${\\\\boldsymbol{z}_i}$            then we can obtain the minimum value of $J$ is :\\\\[\\\\begin{aligned} J^* =  \\\\text{Tr}(XZ^TZX^T) = \\\\text{Tr}(\\\\boldsymbol{\\\\lambda}I) = \\\\sum_{i=M+1}^D \\\\lambda_i \\\\end{aligned}\\\\]        that means ${\\\\lambda_i}$ are the $D-M$ smallest eigenvalues of $X^TX$  hence the eigenvectors defining the principal subspace are those corresponding to the $M$ largest eigenvalues.SVD for PCA\\\\[\\\\begin{aligned} \\\\mathbf{X} = \\\\mathbf{U}\\\\mathbf{D}\\\\mathbf{V}^T \\\\end{aligned}\\\\]  $\\\\mathbf{X}: N\\\\times D$: Center data set matrixthen the $M$-larget principal components are first $M$ columns of $\\\\mathbf{V}$Application of PCA  Data Compression  Data Pre-processing          in the general, the standardizing is made by a separate linear re-scaling of the individual variables such that each variable had zero mean and unit variance, and the correlation matrix(covariance matrix) is\\\\(\\\\begin{aligned} \\\\rho_{ij} = \\\\frac 1N \\\\sum_{n=1}^N \\\\frac {x_{ni}-\\\\overline{x}_i}{\\\\sigma_i}\\\\frac {x_{nj}-\\\\overline{x}_j}{\\\\sigma_j}  \\\\end{aligned}\\\\)      but if using PCA we can make a more substantial normalization of the data to give it zero mean and unit covariance, and different variables become decorrelated                              Eigenvector equation\\\\[\\\\begin{aligned} SU = UL \\\\end{aligned}\\\\]                    $L$ is a $D\\\\times D$ diagonal matrix with elements $\\\\lambda_i$, and $U$ is a $D\\\\times D$ orthogonal matrix with columns given by $\\\\boldsymbol{u}_i$                      next, we defined a transform on data given by:\\\\[\\\\begin{aligned} \\\\boldsymbol{y}_n = L^{-1/2}U^T(\\\\boldsymbol{x}_n -\\\\overline{\\\\boldsymbol{x}}) \\\\end{aligned}\\\\]                                then its covariance is given by identity matrix:\\\\[\\\\begin{aligned} \\\\boldsymbol{y}_n\\\\boldsymbol{y}_n^T = L^{-1/2}U^T(\\\\boldsymbol{x}_n -\\\\overline{\\\\boldsymbol{x}}) (\\\\boldsymbol{x}_n -\\\\overline{\\\\boldsymbol{x}})^TUL^{-1/2} \\\\end{aligned}\\\\]                    This operation is known as whitening or sphereing                      Data Visualization          Here each data point is projected onto a  two-dimensional $(M = 2)$ principal subspace      PCA for high-dimensional dataif $N&lt; D$, then the linear subspace whise dimensionality is at most $N-1$, and there is little point in applying PCA for values of $M$ that are greater than $N-1$Let $X$ be $N\\\\times D$ centred data matrix, then\\\\[\\\\begin{aligned} \\\\frac 1N X^TX\\\\boldsymbol{u}_i &amp;= \\\\lambda_i \\\\boldsymbol{u}_i \\\\\\\\ \\\\frac 1N XX^T(X\\\\boldsymbol{u}_i) &amp;= \\\\lambda_iX\\\\boldsymbol{u}_i \\\\\\\\ \\\\text{Let: }\\\\boldsymbol{v_i} &amp;= X\\\\boldsymbol{u}_i \\\\\\\\ \\\\frac 1N XX^T \\\\boldsymbol{v}_i &amp;= \\\\boldsymbol{v}_i \\\\\\\\ \\\\\\\\ \\\\frac 1N XX^T X^T\\\\boldsymbol{v}_i &amp;= \\\\lambda_i X^T\\\\boldsymbol{v}_i \\\\\\\\ \\\\boldsymbol{u}_i &amp;=  \\\\frac 1{(N\\\\lambda_i)^{1/2}}X^T\\\\boldsymbol{v}_i \\\\end{aligned}\\\\]  so we can firstly work on $R^N$ to find the $N$-dim eigenvectors of $XX^T$ and then compute the $D$-dim eigenvectors of $X^TX$"
  },
  
  {
    "title": "Kernel PCA",
    "url": "/posts/kernel_pca/",
    "categories": "Machine Learning, PCA",
    "tags": "PCA, Kernel",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Kernel PCACentered case analysis      Let $\\\\mathbf{S}$ be $D\\\\times D$ sample covariance matrix, as discussed at conventional PCA: (assume $\\\\mathbf{X}$ are Centered data set)        Recall that the principal components are defined by the eigenvectors $\\\\boldsymbol{\\\\mu}_i$ of the covariance matrix  \\\\[\\\\begin{aligned} \\\\mathbf{S}\\\\boldsymbol{\\\\mu}_i &amp;= \\\\lambda_i \\\\boldsymbol{\\\\mu}_i \\\\\\\\ \\\\mathbf{S} &amp;= \\\\frac 1N \\\\sum_{n=1}^N \\\\boldsymbol{x}_n\\\\boldsymbol{x}_n^T \\\\\\\\&amp;= \\\\frac 1N \\\\mathbf{X}\\\\mathbf{X}^T \\\\end{aligned}\\\\]      In the $M\\\\times M$ feature space, the sample covariance matrix is: (assume ${\\\\phi(\\\\boldsymbol{x}_n)}$ are also centred)\\\\[\\\\begin{aligned} \\\\mathbf{C} = \\\\frac 1N \\\\sum_{n=1}^N \\\\phi(\\\\boldsymbol{x}_n)\\\\phi(\\\\boldsymbol{x}_n)^T \\\\end{aligned}\\\\]                  eigenvector expansion:\\\\[\\\\begin{aligned} \\\\mathbf{C}\\\\boldsymbol{v}_i = \\\\lambda_i \\\\boldsymbol{v}_i \\\\end{aligned}\\\\]                    Then out goal is to solve this eigenvalue problem without having to work explicitly in the feature space.          \\\\[\\\\begin{aligned} \\\\because \\\\space\\\\space&amp; \\\\mathbf{C}\\\\boldsymbol{v}_i = \\\\lambda_i \\\\boldsymbol{v}_i \\\\\\\\ \\\\therefore \\\\space\\\\space&amp; \\\\frac 1N \\\\sum_{n=1}^N \\\\phi(\\\\boldsymbol{x}_n)[\\\\phi(\\\\boldsymbol{x}_n)^T \\\\boldsymbol{v}_i] = \\\\lambda_i \\\\boldsymbol{v}_i \\\\\\\\ &amp;\\\\text{then we can assume: it can be seen a linear combination} \\\\\\\\ &amp; \\\\boldsymbol{v}_i = \\\\sum_{n=1}^N a_{in}\\\\phi(\\\\boldsymbol{x}_n) \\\\\\\\ \\\\therefore \\\\space\\\\space&amp; \\\\frac 1N \\\\sum_{n=1}^N \\\\phi(\\\\boldsymbol{x}_n) \\\\sum_{m=1}^N a_{im}\\\\phi(\\\\boldsymbol{x}_n)^T\\\\phi(\\\\boldsymbol{x}_m) = \\\\lambda_i \\\\sum_{n=1}^N a_{in}\\\\phi(\\\\boldsymbol{x}_n) \\\\\\\\ &amp;\\\\frac 1N \\\\sum_{n=1}^N \\\\phi(\\\\boldsymbol{x}_l)^T\\\\phi(\\\\boldsymbol{x}_n) \\\\sum_{m=1}^N a_{im}\\\\phi(\\\\boldsymbol{x}_n)^T\\\\phi(\\\\boldsymbol{x}_m) = \\\\lambda_i \\\\sum_{n=1}^N a_{in} \\\\phi(\\\\boldsymbol{x}_l)^T\\\\phi(\\\\boldsymbol{x}_n) \\\\\\\\ &amp; \\\\frac 1N \\\\sum_{n=1}^N k(\\\\boldsymbol{x}_l, \\\\boldsymbol{x}_n) \\\\sum_{m=1}^N a_{im}k(\\\\boldsymbol{x}_n, \\\\boldsymbol{x}_m) = \\\\lambda_i \\\\sum_{n=1}^N a_{in} k(\\\\boldsymbol{x}_l, \\\\boldsymbol{x}_n)  \\\\end{aligned}\\\\]        Matrix Form:\\\\[\\\\begin{aligned} \\\\mathbf{K}^2 \\\\boldsymbol{a}_i = \\\\lambda_i N  \\\\mathbf{K} \\\\boldsymbol{a}_i \\\\end{aligned}\\\\]          \\\\[\\\\boldsymbol{a}_i = \\\\{a_{i1},...,a_{in}\\\\}\\\\]                    We can find soulution for $\\\\boldsymbol{a}_i$ by solving the following eigenvalue problem: (assume $\\\\lambda_i\\\\neq 0$)\\\\[\\\\begin{aligned} \\\\mathbf{K}\\\\boldsymbol{a}_i = \\\\lambda_i N \\\\boldsymbol{a}_i  \\\\end{aligned}\\\\]                    Then\\\\[\\\\begin{aligned} y_i(\\\\boldsymbol{x}) &amp;= \\\\phi(\\\\boldsymbol{x})^T\\\\boldsymbol{v}_i \\\\\\\\&amp;= \\\\sum+{n=1}^N a_{in} \\\\phi(\\\\boldsymbol{x})^T\\\\phi(\\\\boldsymbol{x}) \\\\\\\\&amp;= \\\\sum_{n=1}^N k(\\\\boldsymbol{x}, \\\\boldsymbol{x}_n) \\\\\\\\&amp;= \\\\mathbf{k}^T(\\\\boldsymbol{x})\\\\boldsymbol{a}_i \\\\end{aligned}\\\\]            We can see in the fact that Kernel PCA involves the eigenvector expansion of the $N\\\\times N$ matrix $\\\\mathbf{K}$using SVD to compute\\\\[\\\\begin{aligned} &amp; \\\\mathbf{K} = \\\\mathbf{\\\\Phi}\\\\mathbf{\\\\Phi}^T \\\\\\\\ &amp; \\\\mathbf{K} = \\\\mathbf{U}\\\\mathbf{\\\\Sigma}^2\\\\mathbf{U}^T \\\\end{aligned}\\\\]Then the Components is : $\\\\mathbf{U}$Uncentered case analysis根据前面的分析，已经知道，Kernel PCA 其实是先将 $D$-dim 数据根据 $\\\\phi(\\\\boldsymbol{x})$函数映射到$M\\\\times M$ 维的特征空间，然后再在特征空间进行PCA，而且假设特征空间的数据已经中心化但通常，我们并不希望直接在特征空间直接对数据中心化，因此现在假设 $\\\\widetilde{\\\\phi}(\\\\boldsymbol{x})$ 是未中心化的特征数据\\\\[\\\\begin{aligned} \\\\phi(\\\\boldsymbol{x}) &amp;= \\\\widetilde{\\\\phi}(\\\\boldsymbol{x}) - \\\\frac 1N \\\\sum_{l=1}^N \\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_l) \\\\end{aligned}\\\\]Then the Gram matrix element is:\\\\[\\\\begin{aligned} K_{nm} =&amp;\\\\space \\\\phi(\\\\boldsymbol{x}_n)^T\\\\phi(\\\\boldsymbol{x}_m) \\\\\\\\ &amp;= \\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_n)\\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_m) - \\\\frac 1N \\\\sum_{l=1}^N \\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_l)^T\\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_n) - \\\\frac 1N \\\\sum_{l=1}^N \\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_l)^T\\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_m) \\\\\\\\ &amp;\\\\space  + \\\\frac {1}{N^2} \\\\sum_{j=1}^N\\\\sum_{l=1} \\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_l)^T\\\\widetilde{\\\\phi}(\\\\boldsymbol{x}_j) \\\\end{aligned}\\\\]Then the matrix form is given by:\\\\[\\\\begin{aligned} \\\\mathbf{K} = \\\\widetilde{\\\\mathbf{K}} - \\\\mathbf{1}_N\\\\widetilde{\\\\mathbf{K}} - \\\\widetilde{\\\\mathbf{K}}\\\\mathbf{1}_N + \\\\mathbf{1}_N \\\\widetilde{\\\\mathbf{K}} \\\\mathbf{1}_N \\\\end{aligned}\\\\]这样就又回到前面的问题中，对$\\\\mathbf{K}$ 进行特征向量特征值求解，从而得到 Kernel Principal ComponentAttention: 在Kernel PCA中，maximum number of Principal Component is $N$，but not $D$."
  },
  
  {
    "title": "Factor Analysis",
    "url": "/posts/factor_analysis/",
    "categories": "Machine Learning, PCA",
    "tags": "PCA, EM",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Factor AnalysisFactor analysis is a linear-Gaussian latent variable model that is closely related to probabilistic PCA.      in probabilistic PCA:\\\\[\\\\begin{aligned} \\\\boldsymbol{x} &amp;= \\\\mathbf{W}\\\\boldsymbol{z} + \\\\boldsymbol{\\\\mu} + \\\\boldsymbol{\\\\epsilon} \\\\\\\\ \\\\boldsymbol{\\\\epsilon} &amp;\\\\sim \\\\mathcal{N}(\\\\boldsymbol{\\\\epsilon} \\\\vert 0, \\\\sigma^2 I) \\\\\\\\ \\\\\\\\ p(\\\\boldsymbol{z}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{z} \\\\vert 0, I) \\\\\\\\ p(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{z}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{x} \\\\vert \\\\mathbf{W}\\\\boldsymbol{z} + \\\\boldsymbol{\\\\mu}, \\\\sigma^2 I) \\\\end{aligned}\\\\]        but in Factor Analysis\\\\[\\\\begin{aligned} \\\\boldsymbol{x} &amp;= \\\\mathbf{W}\\\\boldsymbol{z} + \\\\boldsymbol{\\\\mu} + \\\\boldsymbol{\\\\epsilon} \\\\\\\\ \\\\boldsymbol{\\\\epsilon} &amp;\\\\sim \\\\mathcal{N}(\\\\boldsymbol{\\\\epsilon} \\\\vert 0, \\\\mathbf{\\\\Psi}) \\\\\\\\ \\\\\\\\ p(\\\\boldsymbol{z}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{z} \\\\vert 0, I) \\\\\\\\ p(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{z}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{x} \\\\vert \\\\mathbf{W}\\\\boldsymbol{z} + \\\\boldsymbol{\\\\mu}, \\\\mathbf{\\\\Psi}) \\\\end{aligned}\\\\]          Where $\\\\mathbf{\\\\Psi}$ is a diagonal matrix.      Also this is again an example of naive Bayes, but each component variable of $\\\\boldsymbol{x}$ has different variance.                  given the latent variable $\\\\boldsymbol{z}$, observed variables ${\\\\mathbf{x}_n}$ are independent.                    Note:The origins of factor analysis are as old as those of PCA. and discussions of factor analysis can be found in the books by Everitt (1984). Bartholomew (1987), and Basilevsky (1994).  Links between factor analysis and PCA were investigated by Lilwley (1953) and Anderson (1963) who showed that at stationary points of the likelihood function. for a faclOr analysis model with $\\\\mathbf{\\\\Psi} = \\\\sigma^2I$, the columns of $\\\\mathbf{W}$ are scaled eigenvectors of the sample covariance matrix, and $\\\\sigma^2$ is the average of the discarded eigenvalues.  Later. Tipping and Bishop (1999b) showed that the maximum of the log likelihood function occurs when the eigenvectors comprising $\\\\mathbf{W}$ are chosen to be the principal eigenvectors.Predictive Distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) &amp;= \\\\int p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{z})p(\\\\boldsymbol{z})d\\\\boldsymbol{z} \\\\\\\\ &amp;= \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{C}) \\\\\\\\\\\\\\\\ C &amp;= \\\\mathbf{W}\\\\mathbf{W}^T + \\\\mathbf{\\\\Psi}  \\\\end{aligned}\\\\]###EM for Factor analysiswe also can determine the parameters by ML and again optimized by EM.As the same discuss in the Probabilistic PCA, with wih some modified:      E-step:\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\ln p(\\\\mathbf{X}, \\\\mathbf{Z}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\mathbf{W}, \\\\mathbf{\\\\Psi})] \\\\space =&amp; \\\\mathbb{E}[\\\\ln p(\\\\mathbf{X}\\\\vert \\\\mathbf{Z}) + \\\\ln p(\\\\mathbf{Z})] \\\\\\\\ \\\\space =&amp; -\\\\sum_{n=1}^N \\\\{\\\\frac 12 \\\\text{Tr}(\\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n^T]) + \\\\frac D2 \\\\ln(2\\\\pi) + \\\\frac 12 \\\\ln \\\\vert \\\\mathbf{\\\\Psi} \\\\vert  \\\\\\\\ &amp; +\\\\frac {1}{2}(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu})^T\\\\mathbf{\\\\Psi}^{-1}(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu}) -\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T\\\\mathbf{W}^T\\\\mathbf{\\\\Psi}^{-1}(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu}) \\\\\\\\ &amp; + \\\\frac {1}{2}\\\\text{Tr}(\\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n]^T\\\\mathbf{W}^T\\\\mathbf{\\\\Psi}^{-1}\\\\mathbf{W}) \\\\} \\\\\\\\ \\\\\\\\  \\\\mathbf{G} =&amp; (I + \\\\mathbf{W}^T\\\\mathbf{\\\\Psi}^{-1}\\\\mathbf{W})^{-1} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[\\\\boldsymbol{z}_n] \\\\space =&amp; \\\\mathbf{G}\\\\mathbf{W}^T\\\\mathbf{\\\\Psi}^{-1}(\\\\boldsymbol{x}_n-\\\\overline{\\\\boldsymbol{x}}) \\\\\\\\  \\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n^T] \\\\space =&amp; \\\\mathbf{G} + \\\\mathbb{E}[\\\\boldsymbol{z}_n]\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T  \\\\end{aligned}\\\\]        M-step\\\\[\\\\begin{aligned} \\\\mathbf{W}_{new}&amp;= \\\\left[ \\\\sum_{n=1}^N \\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n]  \\\\right]^{-1} \\\\left[ \\\\sum_{n=1}^N (\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu})\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T  \\\\right] \\\\\\\\ \\\\mathbf{\\\\Psi} &amp;= \\\\frac 1N \\\\text{diag} \\\\left\\\\{ \\\\sum_{n=1}^N (\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu})(\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu})^T - \\\\mathbf{W}_{new}\\\\sum_{n=1}^N\\\\mathbb{E}[\\\\boldsymbol{z}_n](\\\\boldsymbol{x}_n - \\\\boldsymbol{\\\\mu})^T + N\\\\mathbf{W}_{new}\\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n^T]\\\\mathbf{W}_{new}^T \\\\right\\\\} \\\\end{aligned}\\\\]  Note: Another difference between probabilistic PCA and factor analysis concerns their different behaviour under transformations of the data set. For PCA and probabilistic PCA, if we rotate the coordinate system in data space, then we obtain exactly the same fit to the data but with the $\\\\mathbf{W}$ matrix transformed by the corresponding rotation matrix. However, for factor analysis, the analogous property is that if we make a component-wise re-scaling of the data vectors, then this is absorbed into a orresponding re-scaling of the elements of $\\\\mathbf{\\\\Psi}$."
  },
  
  {
    "title": "Bayesian PCA",
    "url": "/posts/Bayesian_pca/",
    "categories": "Machine Learning, PCA",
    "tags": "PCA",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Bayesian PCAGiven that we have a probabilistic formulation of PCA, it seems natural to seek a Bayesian approach to model selection.But to do this, we need to marginalize out the model parameters $\\\\boldsymbol{\\\\mu}$, $\\\\mathbf{W}$, and $\\\\sigma^2$ with respect to appropriate prior distributions.Variational ApproximationThis can be down by using Variational framework to approximation the analytically intractable marginalizations.The marginal likelihood values, given by the Variational lower bound, can then be compared for a range of different values of $M$ and the value giving the largest marginal likelihood selected.Evidence ApproximationBut here we consider a simpler approach “Evidence Approximation”, it is appropriate when the number of  data points is relatively large and the corresponding posterior distribution is tightly peaked.It involves a specific choice of prior over $\\\\mathbf{W}$ that allows surplus dimensions in the principal subspace to be pruned out of the model. (Just like a Sparisty Solution of $\\\\mathbf{W}$)  This corresponds to an example of automatic relevance determination.Independent Gaussian prior over $\\\\mathbf{W}$      We define an independent Gaussian Prior over each column of $\\\\mathbf{W}$, which represent the vectors defing the principal subspace.        Each such Gaussian has an independent variance giverned by a precision hyperparameter $\\\\alpha_i$ so that\\\\[\\\\begin{aligned} p(\\\\mathbf{W}\\\\vert \\\\boldsymbol{\\\\alpha}) = \\\\prod_{i=1}^M \\\\left (\\\\frac {\\\\alpha_i}{2}\\\\right)^{D/2} \\\\exp\\\\left\\\\{-\\\\frac 12 \\\\alpha_i \\\\mathbf{w}_i^T\\\\mathbf{w}_i \\\\right\\\\} \\\\end{aligned}\\\\]    As a result of this optimization, some of the $\\\\alpha_i$ may be driven to infinty, with the corresponding parameters vector $\\\\mathbf{w}_i$ being driven to zero given a spare solution.  Then the effective dimensionality of the principal subspace is the determined by the number of finite $\\\\alpha_i$ values, and the corresponding vectors $\\\\mathbf{w}_i$ can be thought of as ‘Relevant’ for modelling the data distribution.In this way, the Bayesian approach is automatically making the trade-off between  improving the fit to the data, by using a larger number of vectors $\\\\mathbf{w}_i$ with their corresponding eigenvalues $\\\\alpha_i$ each tuned to the data,  reducing the complexity of the model by suppressing some of the $\\\\mathbf{w}_i$ vectors.The origins of this sparsity were discussed earlier in the context of relevance vector machines.Re-Estimation the value of $\\\\alpha_i$Maximizing the log marginal likelihood\\\\[\\\\begin{aligned} p(\\\\mathbf{X} \\\\vert \\\\boldsymbol{\\\\alpha}, \\\\boldsymbol{\\\\mu}, \\\\sigma^2 ) = \\\\int p(\\\\mathbf{X} \\\\vert \\\\mathbf{W}, \\\\boldsymbol{\\\\mu}, \\\\sigma^2) p(\\\\mathbf{W} \\\\vert \\\\boldsymbol{\\\\alpha}) d\\\\mathbf{W} \\\\end{aligned}\\\\]Because this integration is intractable, we make use of the Laplace approximation.      If we assume that the posterior distribution is sharply peaked, as will occur for sufficiently large data sets, then the re-estimation equations obtained by maximizing the marginal likelihood with respect to $\\\\alpha_i$ take the simple form:\\\\[\\\\begin{aligned} \\\\alpha_i^{new} = \\\\frac {D}{\\\\mathbf{w}_i^T\\\\mathbf{w}_i^T} \\\\end{aligned}\\\\]  Then in the E-step in EM for probability PCA, we can easily see:\\\\[\\\\begin{aligned} \\\\mathbf{W}_{new}&amp;= \\\\left[ \\\\sum_{n=1}^N \\\\mathbb{E}[\\\\boldsymbol{z}_n\\\\boldsymbol{z}_n] + \\\\sigma^2 \\\\text{diag}[\\\\boldsymbol{\\\\alpha}]  \\\\right]^{-1} \\\\left[ \\\\sum_{n=1}^N (\\\\boldsymbol{x}_n-\\\\boldsymbol{\\\\mu})\\\\mathbb{E}[\\\\boldsymbol{z}_n]^T  \\\\right] \\\\end{aligned}\\\\]Note: The model described here involves a prior only over the matrix $\\\\mathbf{W}$, and this is not a full Bayesian, which can be solved usign variational methods."
  },
  
  {
    "title": "Bayesian Linear Regression (Single Output)",
    "url": "/posts/bayesian_linear_regression_single_output/",
    "categories": "Bayesian Inference",
    "tags": "Linear Regression, Bayesian",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Bayesian Linear Regression (Single Output)      The Linear Model of $f(\\\\boldsymbol{x}, \\\\boldsymbol{w}) = \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x})$, with Gaussian noise $p(\\\\epsilon) = \\\\mathcal{N}(\\\\epsilon \\\\vert 0, \\\\beta^{-1})$ is given by:\\\\[\\\\begin{aligned} y &amp;= \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}) + \\\\epsilon \\\\\\\\ p(y\\\\vert \\\\boldsymbol{x}, \\\\boldsymbol{w}, \\\\beta) &amp;= \\\\mathcal{N}(y\\\\vert \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}), \\\\beta^{-1}) \\\\end{aligned}\\\\]          $\\\\boldsymbol{w}$ is a $M$-dim parameter vector.            Likelihood Function\\\\[\\\\begin{aligned} p(D \\\\vert \\\\boldsymbol{w}) = \\\\prod_{i=1}^N \\\\mathcal{N}(y_i \\\\vert \\\\boldsymbol{w}^T\\\\phi(\\\\mathbf{x}_i), \\\\beta^{-1}) \\\\end{aligned}\\\\]  Known Precision  Assume the noise precision $\\\\beta$ is known, then the conjugate prior and posterior both are GaussianParameter Distribution      The Conjugate Prior Distribution  (Gaussian)\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}_0, \\\\boldsymbol{S}_0) = \\\\mathcal{N}(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}_0, \\\\boldsymbol{S}_0) \\\\end{aligned}\\\\]        The Posterior Distribution  (Gaussian)\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\vert \\\\mathbf{X}, \\\\mathbf{y}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}_N, \\\\boldsymbol{S}_N) \\\\\\\\ \\\\\\\\ \\\\boldsymbol{S}_N^{-1} &amp;= \\\\beta \\\\mathbf{\\\\Phi}^T\\\\mathbf{\\\\Phi} + \\\\boldsymbol{S}_0^{-1} \\\\\\\\ \\\\boldsymbol{\\\\mu}_N &amp;= \\\\boldsymbol{S}_N(\\\\beta \\\\mathbf{\\\\Phi}^T \\\\mathbf{y} + \\\\boldsymbol{S}_0^{-1} \\\\boldsymbol{\\\\mu}_0) \\\\end{aligned}\\\\]                  where $\\\\mathbf{X}$ and $\\\\mathbf{y}$ are observations corresponding to the N input and output values.                    where $\\\\mathbf{\\\\Phi}$ is a $N\\\\times m$ matrix, if $\\\\boldsymbol{w}$ is a $m$-dim vector, and its rows are $\\\\phi(\\\\mathbf{x}_i)^T,\\\\space i=1,2\\\\cdots,N$.                    The Proof:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\vert D) &amp;\\\\propto p(D\\\\vert \\\\boldsymbol{w})p(\\\\boldsymbol{w}) \\\\\\\\ &amp;= \\\\left[ \\\\prod_{i=1}^N \\\\mathcal{N}(y_i \\\\vert \\\\boldsymbol{w}^T\\\\phi(\\\\mathbf{x}_i), \\\\beta^{-1}) \\\\right]\\\\mathcal{N}(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}_0, \\\\boldsymbol{S}_0) \\\\\\\\ &amp; \\\\propto \\\\exp\\\\left(-\\\\frac \\\\beta 2 (\\\\mathbf{y}-\\\\mathbf{\\\\Phi}\\\\boldsymbol{w})^T(\\\\mathbf{y}-\\\\mathbf{\\\\Phi}\\\\boldsymbol{w})\\\\right) \\\\exp\\\\left( -\\\\frac 12 (\\\\boldsymbol{w}-\\\\boldsymbol{\\\\mu}_0)^TS_0^{-1}(\\\\boldsymbol{w}-\\\\boldsymbol{\\\\mu}_0) \\\\right) \\\\\\\\ &amp;= \\\\exp\\\\left(-\\\\frac 12 \\\\boldsymbol{w}^T(\\\\beta\\\\boldsymbol{\\\\Phi}^T\\\\boldsymbol{\\\\Phi} + S_0^{-1})\\\\boldsymbol{w} + \\\\boldsymbol{w}^T(\\\\beta\\\\boldsymbol{\\\\Phi}^T\\\\boldsymbol{y} + S_0^{-1}\\\\boldsymbol{\\\\mu}_0) \\\\right) + \\\\text{const} \\\\end{aligned}\\\\]                  If we consider the prior is a zero-mean and isotropic Gaussian distribution, then                  The Prior Distribution (let $\\\\alpha$ be its precision) can be written by:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\vert 0, \\\\alpha^{-1}I) \\\\end{aligned}\\\\]                    The Posterior Distribution:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\vert \\\\mathbf{X}, \\\\mathbf{y}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}_N, \\\\boldsymbol{S}_N) \\\\\\\\ \\\\\\\\ \\\\boldsymbol{S}_N^{-1} &amp;= \\\\beta \\\\mathbf{\\\\Phi}^T\\\\mathbf{\\\\Phi} + \\\\alpha I \\\\\\\\ \\\\boldsymbol{\\\\mu}_N &amp;= \\\\beta\\\\boldsymbol{S}_N\\\\mathbf{\\\\Phi}^T \\\\mathbf{y}  \\\\end{aligned}\\\\]                              Also, in this assumption:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\vert \\\\mathbf{X}, \\\\mathbf{y}) &amp;\\\\propto \\\\exp\\\\left(-\\\\frac \\\\beta 2 (\\\\mathbf{y}-\\\\mathbf{\\\\Phi}\\\\boldsymbol{w})^T(\\\\mathbf{y}-\\\\mathbf{\\\\Phi}\\\\boldsymbol{w})\\\\right) \\\\exp\\\\left( -\\\\frac \\\\alpha 2 \\\\boldsymbol{w}^T\\\\boldsymbol{w} \\\\right) \\\\\\\\ \\\\ln (p(\\\\boldsymbol{w} \\\\vert \\\\mathbf{X}, \\\\mathbf{y})) &amp;= -\\\\frac \\\\beta 2 (\\\\mathbf{y}-\\\\mathbf{\\\\Phi}\\\\boldsymbol{w})^T(\\\\mathbf{y}-\\\\mathbf{\\\\Phi}\\\\boldsymbol{w}) -\\\\frac \\\\alpha 2 \\\\boldsymbol{w}^T\\\\boldsymbol{w} + \\\\text{const} \\\\end{aligned}\\\\]                                Then we can see, maximization of this posterior distribution with respect to $\\\\boldsymbol{w}$ is therefore equivalent to the minimization of the sum-of-squares error function with the addition of a quadratic regularization term, corresponding $\\\\lambda = \\\\frac {\\\\alpha}{\\\\beta}$                              Predictive Distribution      In practice, we are not usually interested in the value of $\\\\boldsymbol{w}$ itself but rather to making predictions of $t$ for new values of $\\\\mathbf{x}$.        Then we can batained a predictive dsitribution by\\\\[\\\\begin{aligned} p(y \\\\vert \\\\boldsymbol{x}, D, \\\\alpha, \\\\beta) = \\\\int p(y\\\\vert \\\\boldsymbol{x}, \\\\boldsymbol{w}, \\\\beta) p(\\\\boldsymbol{w}\\\\vert D, \\\\alpha, \\\\beta) d\\\\boldsymbol{w} \\\\end{aligned}\\\\]        And we have already know:                  if the condition distribution $p(\\\\boldsymbol{x})$ is $\\\\mathcal{N}(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\Lambda^{-1})$ and the marginal distribution $p(\\\\boldsymbol{y} \\\\vert \\\\boldsymbol{x})$ is $\\\\mathcal{N}(\\\\boldsymbol{y} \\\\vert A\\\\boldsymbol{x} + b, L^{-1})$                    then the marginal distribution $p(y)$ is $\\\\mathcal{N}(y\\\\vert A\\\\boldsymbol{\\\\mu} +b, L^{-1}+A\\\\Lambda^{-1}A^T)$                  Thus in this case, (Linear Model with zero-mean Gaussian noise):\\\\[\\\\begin{aligned} p(y\\\\vert \\\\boldsymbol{x}, D, \\\\alpha, \\\\beta) &amp;= \\\\int p(y\\\\vert \\\\boldsymbol{x}, \\\\boldsymbol{w}, \\\\beta) p(\\\\boldsymbol{w}\\\\vert D, \\\\alpha, \\\\beta) \\\\boldsymbol{w} \\\\\\\\ &amp;= \\\\int \\\\mathcal{N}(y \\\\vert \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}), \\\\beta^{-1}) \\\\mathcal{N}(\\\\boldsymbol{w}\\\\vert \\\\boldsymbol{\\\\mu}_N, \\\\boldsymbol{S}_N) d\\\\boldsymbol{w} \\\\\\\\ &amp;= \\\\mathcal{N}(y\\\\vert \\\\mu, \\\\sigma^2) \\\\\\\\ \\\\\\\\ \\\\mu &amp;= \\\\boldsymbol{\\\\mu}_N^T\\\\phi(\\\\boldsymbol{x}) \\\\\\\\ \\\\sigma^2 &amp;=  \\\\beta^{-1}+\\\\phi(\\\\boldsymbol{x})^T \\\\boldsymbol{S}_N \\\\phi(\\\\boldsymbol{x}) \\\\\\\\\\\\\\\\  \\\\end{aligned}\\\\]        Note:          if we use localized basis functions such as Gaussian, then in regions away from the basis function centres, the contribution from the second term in the predictive variance will go to zero, leaving only the noise contribution $\\\\beta^{-1}$. Thus the model becomes very confident in its predictions when extrapolation outside the region occuiped by the basis functions, which is generaly an undersirable behaviour.      Unknown Precision  If both $w$ and $\\\\beta$ are treated as unknown, then the conjugate prior will be a Gaussian–Gamma distribution, because $\\\\beta$ is the precision. In this case, the predictive distribution is a Student’s t-distribution.Parameter Distribution      Conjugate Prior Distribution (Gaussian Gamma)\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w}, \\\\beta \\\\vert \\\\boldsymbol{\\\\mu}_0, V_0^{-1}, a_0, b_0) &amp;= \\\\mathcal{N}\\\\left(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}_0, \\\\frac 1\\\\beta V_0\\\\right)\\\\text{Gamma}(\\\\beta\\\\vert a_0, b_0) \\\\end{aligned}\\\\]          $V_0$ is a $M\\\\times M$ symmetric definite matrix.            Posterior Distribution (Gaussian Gamma)\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w}, \\\\beta \\\\vert \\\\boldsymbol{\\\\mu}, V^{-1}, a, b) = \\\\mathcal{N}\\\\left(\\\\boldsymbol{w} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\frac 1\\\\beta V\\\\right)\\\\text{Gamma}(\\\\beta\\\\vert a, b)\\\\end{aligned}\\\\]                  Proof:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w}, \\\\beta) \\\\propto&amp; p(D \\\\vert \\\\boldsymbol{w})p(\\\\boldsymbol{w}, \\\\beta \\\\vert \\\\boldsymbol{\\\\mu}_0, V_0^{-1}, a_0, b_0) \\\\\\\\\\\\propto&amp; \\\\beta^{\\\\frac {MN}{2}} \\\\exp\\\\left(\\\\sum_{i=1}^N -\\\\frac {\\\\beta}{2}(y_i-\\\\boldsymbol{w}^T\\\\phi_i)^2 \\\\right) \\\\exp\\\\left(-\\\\frac \\\\beta 2 (\\\\boldsymbol{w}-\\\\boldsymbol{\\\\mu}_0)^TV_0^{-1}(\\\\boldsymbol{w}-\\\\boldsymbol{\\\\mu}_0)\\\\right) \\\\beta^{a_0-1}\\\\exp(-b_0\\\\beta) \\\\\\\\ \\\\propto&amp; \\\\exp\\\\left( -\\\\frac \\\\beta 2 \\\\boldsymbol{w}^T(\\\\Phi^T\\\\Phi + V_0^{-1})\\\\boldsymbol{w} + \\\\beta  (\\\\Phi^T\\\\boldsymbol{y} + V_0^{-1}\\\\boldsymbol{\\\\mu}_0)^T\\\\boldsymbol{w} \\\\right) \\\\cdot \\\\\\\\ &amp;\\\\beta^{\\\\frac {MN}{2}+a_0-1}\\\\exp\\\\left( -\\\\beta\\\\left(b_0+\\\\frac 12 (\\\\boldsymbol{y}^T\\\\boldsymbol{y} + \\\\boldsymbol{\\\\mu}_0^TV_0^{-1}\\\\boldsymbol{\\\\mu}_0) \\\\right) \\\\right) \\\\end{aligned}\\\\]                              Then we have:\\\\[\\\\begin{aligned} V^{-1} &amp;= \\\\Phi^T\\\\Phi + V_0^{-1} \\\\\\\\ \\\\boldsymbol{\\\\mu} &amp;= V(\\\\Phi^T\\\\boldsymbol{y} + V_0^{-1}\\\\boldsymbol{\\\\mu}_0) \\\\\\\\ a&amp;= \\\\frac {MN}{2}+a_0 \\\\\\\\ b&amp;= b_0+\\\\frac 12 (\\\\boldsymbol{y}^T\\\\boldsymbol{y} + \\\\boldsymbol{\\\\mu}_0^TV_0^{-1}\\\\boldsymbol{\\\\mu}_0) \\\\end{aligned}\\\\]                              Predictive Distribution\\\\[\\\\begin{aligned} p(y\\\\mid \\\\boldsymbol{x}) &amp;= \\\\int_0^\\\\infty\\\\int p(y\\\\mid \\\\boldsymbol{x}, \\\\boldsymbol{w}, \\\\beta) p(\\\\boldsymbol{w}, \\\\beta \\\\mid \\\\boldsymbol{\\\\mu}, V^{-1}, a, b) d\\\\boldsymbol{w}d\\\\beta  \\\\\\\\&amp;= \\\\int_0^\\\\infty\\\\int \\\\mathcal{N}(y \\\\mid \\\\boldsymbol{w}^T\\\\phi(\\\\boldsymbol{x}), \\\\beta^{-1})\\\\mathcal{N}\\\\left(\\\\boldsymbol{w} \\\\mid \\\\boldsymbol{\\\\mu}, \\\\frac 1\\\\beta V\\\\right)\\\\text{Gamma}(\\\\beta\\\\mid a, b) d\\\\boldsymbol{w} d\\\\beta \\\\\\\\ &amp;= \\\\int_0^\\\\infty \\\\mathcal{N}\\\\left(y \\\\mid \\\\boldsymbol{\\\\mu}^T\\\\phi(\\\\boldsymbol{x}), \\\\frac {\\\\sigma^2}{\\\\beta}\\\\right)\\\\text{Gamma}(\\\\beta\\\\mid a, b)d\\\\beta \\\\\\\\ &amp;= (2\\\\pi\\\\sigma^2)^{-1/2}\\\\frac {b^a}{\\\\Gamma(a)} \\\\int_0^\\\\infty \\\\beta^{1/2}\\\\exp\\\\left( -\\\\frac {\\\\beta z^2} {2\\\\sigma^2} \\\\right)\\\\beta^{a-1}\\\\exp(-b\\\\beta)  d\\\\beta \\\\\\\\ &amp;= (2\\\\pi\\\\sigma^2)^{-1/2} \\\\frac {b^a}{\\\\Gamma(a)} \\\\Gamma(a+1/2) \\\\left[b+ \\\\frac {z^2}{2\\\\sigma^2}\\\\right]^{-a-1/2} \\\\\\\\ &amp;= (2\\\\pi\\\\sigma^2 b)^{-1/2} \\\\frac {\\\\Gamma(a+1/2)}{\\\\Gamma(a)}  \\\\left[1+ \\\\frac {z^2}{2b\\\\sigma^2}\\\\right]^{-a-1/2}  \\\\end{aligned}\\\\]      where we have let:\\\\[\\\\begin{aligned} z &amp;= y - \\\\boldsymbol{\\\\mu}^T\\\\phi(\\\\boldsymbol{x}) \\\\\\\\ \\\\sigma^2 &amp;= 1+\\\\phi(\\\\boldsymbol{x})^TV\\\\phi(\\\\boldsymbol{x}) \\\\end{aligned}\\\\]        if we have let:\\\\[\\\\begin{aligned} \\\\mu_s &amp;= \\\\boldsymbol{\\\\mu}^T\\\\phi(\\\\boldsymbol{x}) \\\\\\\\ \\\\lambda_s &amp;= \\\\frac {a}{b\\\\sigma^2} \\\\\\\\ \\\\nu_s &amp;= 2a \\\\end{aligned}\\\\]        finally, we have:\\\\[\\\\begin{aligned} p(y\\\\mid \\\\boldsymbol{x}) &amp;= \\\\left( \\\\frac {\\\\lambda_s}{\\\\pi\\\\nu_s}\\\\right)^{1/2} \\\\frac {\\\\Gamma(\\\\nu_s/2+1/2)}{\\\\Gamma(\\\\nu_s/2)}  \\\\left[1+ \\\\frac {\\\\lambda_s(y-\\\\mu_s)^2}{\\\\nu_s}\\\\right]^{-\\\\nu_s/2-1/2}  \\\\\\\\ &amp;= \\\\text{S.t} \\\\space (y\\\\mid \\\\mu_s, \\\\lambda_s, \\\\nu_s) \\\\end{aligned}\\\\]                  and:\\\\[\\\\begin{aligned} \\\\mathbb{E}[y] &amp;= \\\\mu_s  \\\\\\\\&amp;= \\\\boldsymbol{\\\\mu}^T\\\\phi(\\\\boldsymbol{x}) \\\\\\\\&amp;= \\\\phi(\\\\boldsymbol{x})^T(\\\\Phi^T\\\\Phi + V_0^{-1})^{-1}(\\\\Phi^T\\\\boldsymbol{y} + V_0^{-1}\\\\boldsymbol{\\\\mu}_0) \\\\\\\\ \\\\text{cov}[y] &amp;= \\\\frac {\\\\nu_s}{\\\\nu_s - 2} \\\\lambda_s^{-1} \\\\\\\\&amp;= \\\\frac {b\\\\sigma^2}{a-1} \\\\\\\\ \\\\text{mode}[y] &amp;= \\\\mu_s \\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "Bayesian Linear Regression (Multiple Outputs)",
    "url": "/posts/bayesian_linear_regression_multiple_output/",
    "categories": "Bayesian Inference",
    "tags": "Linear Regression, Bayesian",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "Bayesian Linear Regression (Multiple Output)      The Linear Model of $f(\\\\boldsymbol{x}, W) = W\\\\phi(\\\\boldsymbol{x})$, with Gaussian noise $p(\\\\boldsymbol{\\\\epsilon}) = \\\\mathcal{N}(\\\\boldsymbol{\\\\epsilon} \\\\vert 0, \\\\Lambda_{\\\\varepsilon}^{-1})$ is given by:\\\\[\\\\begin{aligned} \\\\boldsymbol{y} &amp;= W^T\\\\phi(\\\\boldsymbol{x}) + \\\\boldsymbol{\\\\epsilon} \\\\\\\\ p( \\\\boldsymbol{y}\\\\mid \\\\boldsymbol{x}, W, \\\\Lambda_{\\\\varepsilon}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{y} \\\\mid W^T\\\\phi(\\\\boldsymbol{x}), \\\\Lambda_{\\\\varepsilon}^{-1}) \\\\end{aligned}\\\\]          $W$ is a $M\\\\times K$ parameter matrix      $\\\\Lambda_{\\\\varepsilon}$ is a $K\\\\times K$ percision matrix of a gaussian white noisy            Likelihood Function\\\\[\\\\begin{aligned} p(D \\\\vert W) = \\\\prod_{i=1}^N \\\\mathcal{N}(\\\\boldsymbol{y}_i \\\\vert W^T\\\\phi(\\\\mathbf{x}_i), \\\\Lambda_{\\\\varepsilon}^{-1}) \\\\end{aligned}\\\\]        Note: now, the gaussian prior distribution of paramater matrix $W$ is Matrix Gaussian distribution $\\\\mathcal{N}(W \\\\mid \\\\mu_{W_0},V_0, U_0)$.\\\\[\\\\begin{aligned} p(W\\\\mid M_0, U_0, V_0) = \\\\frac {1}{(2\\\\pi)^{\\\\frac {KM}{2}} \\\\vert V_0\\\\vert^{\\\\frac K2} \\\\vert U_0\\\\vert^{\\\\frac M2}} \\\\exp\\\\left( -\\\\frac 12 \\\\mathrm{Tr}\\\\left[ V_0^{-1}(W-M_0)^TU_0^{-1}(W-M_0) \\\\right] \\\\right) \\\\end{aligned}\\\\]          $M_0$ is $M\\\\times K$      $V_0$ is $K \\\\times K$              $U_0$ is $M \\\\times M$                    and this can be writen as vectorize $\\\\boldsymbol{w} = \\\\text{vec}(W)$, which is a $MK$-dim vector.\\\\[\\\\boldsymbol{w} \\\\sim \\\\mathcal{N}(\\\\boldsymbol{w} \\\\mid \\\\text{vec}(M_0), V_0 \\\\otimes U_0)\\\\]                  where $\\\\otimes$ denotes the Kronecker product wiki.                    Known Precision Matrix  Let $\\\\boldsymbol{w} = \\\\text{vec}(W)$Parameter Distribution      Prior distribution: a Matrix Gaussian distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\mid \\\\boldsymbol{m}_0, V_0\\\\otimes U_0) = \\\\mathcal{N}(\\\\boldsymbol{w} \\\\mid \\\\text{vec}(M_0), V_0 \\\\otimes U_0) \\\\end{aligned}\\\\]        Posterior distribution: a Matrix Gaussian distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w} \\\\mid \\\\boldsymbol{m}_N, V_N\\\\otimes U_N) &amp;\\\\propto p(D \\\\vert W) p(\\\\boldsymbol{w} \\\\mid \\\\boldsymbol{m}_0, V_0\\\\otimes U_0) \\\\end{aligned}\\\\]                  the terms in the $exp$:\\\\[\\\\begin{aligned} &amp;\\\\left(\\\\sum_{i=1}^N -\\\\frac 12 [\\\\boldsymbol{y}_i - W^T\\\\phi(\\\\boldsymbol{x}_i)]^T\\\\Lambda_{\\\\varepsilon}[\\\\boldsymbol{y}_i - W^T\\\\phi(\\\\boldsymbol{x}_i)] \\\\right) - \\\\frac 12 (\\\\boldsymbol{w}-\\\\boldsymbol{m}_0)^T(V_0 \\\\otimes U_0)^{-1}(\\\\boldsymbol{w}-\\\\boldsymbol{m}_0) \\\\\\\\ =&amp; -\\\\frac 12 \\\\text{Tr}[(Y-\\\\Phi W)^T(Y-\\\\Phi W)\\\\Lambda_{\\\\varepsilon}]- \\\\frac 12 (\\\\boldsymbol{w}-\\\\boldsymbol{m}_0)^T(V_0 \\\\otimes U_0)^{-1}(\\\\boldsymbol{w}-\\\\boldsymbol{m}_0) \\\\\\\\=&amp; -\\\\frac 12 \\\\text{vec}(W)^T\\\\text{vec}(\\\\Phi^T\\\\Phi W \\\\Lambda_{\\\\varepsilon}) -\\\\frac 12 \\\\boldsymbol{w}^T(V_0 \\\\otimes U_0)^{-1}\\\\boldsymbol{w} \\\\\\\\ &amp;+ \\\\text{vec}(W)^T\\\\text{vec}(\\\\Phi^TY\\\\Lambda_{\\\\varepsilon}) + \\\\boldsymbol{w}^T(V_0 \\\\otimes U_0)^{-1}\\\\boldsymbol{m}_0 + \\\\text{const}  \\\\\\\\ =&amp; -\\\\frac 12 \\\\boldsymbol{w}^T(\\\\Lambda_{\\\\varepsilon}\\\\otimes \\\\Phi^T\\\\Phi) \\\\boldsymbol{w} -\\\\frac 12 \\\\boldsymbol{w}^T(V_0 \\\\otimes U_0)^{-1}\\\\boldsymbol{w} \\\\\\\\ &amp;+ \\\\boldsymbol{w}^T\\\\text{vec}(\\\\Phi^TY\\\\Lambda_{\\\\varepsilon}) + \\\\boldsymbol{w}^T(V_0 \\\\otimes U_0)^{-1}\\\\boldsymbol{m}_0 + \\\\text{const} \\\\\\\\=&amp; -\\\\frac 12 \\\\boldsymbol{w}^T(\\\\Lambda_{\\\\varepsilon}\\\\otimes \\\\Phi^T\\\\Phi+(V_0 \\\\otimes U_0)^{-1}) \\\\boldsymbol{w} + \\\\boldsymbol{w}^T(\\\\text{vec}(\\\\Phi^TY\\\\Lambda_{\\\\varepsilon}) + (V_0 \\\\otimes U_0)^{-1}\\\\boldsymbol{m}_0) + \\\\text{const} \\\\end{aligned}\\\\]                    Then we have:\\\\[\\\\begin{aligned} V_N \\\\otimes U_N &amp;= (\\\\Lambda_{\\\\varepsilon}\\\\otimes \\\\Phi^T\\\\Phi+(V_0 \\\\otimes U_0)^{-1})^{-1} \\\\\\\\ \\\\boldsymbol{m}_N &amp;= (V_N \\\\otimes U_N)(\\\\text{vec}(\\\\Phi^TY\\\\Lambda_{\\\\varepsilon})+(V_0 \\\\otimes U_0)^{-1}\\\\boldsymbol{m}_0)  \\\\end{aligned}\\\\]                  Note: in the prior if we let $V_0 = \\\\Lambda_{\\\\varepsilon}^{-1}$, we can obtain:\\\\[\\\\begin{aligned} V_N \\\\otimes U_N &amp;= (\\\\Lambda_{\\\\varepsilon}\\\\otimes \\\\Phi^T\\\\Phi+(\\\\Lambda_{\\\\varepsilon}^{-1} \\\\otimes U_0)^{-1})^{-1} \\\\\\\\ &amp;= \\\\Lambda_{\\\\varepsilon}^{-1} \\\\otimes (\\\\Phi^T\\\\Phi+U_0^{-1})^{-1} \\\\\\\\ \\\\boldsymbol{m}_N &amp;= (V_N \\\\otimes U_N)(\\\\text{vec}(\\\\Phi^TY\\\\Lambda_{\\\\varepsilon})+(\\\\Lambda_{\\\\varepsilon}^{-1} \\\\otimes U_0)^{-1}\\\\boldsymbol{m}_0) \\\\\\\\ &amp;= (V_N \\\\otimes U_N)\\\\text{vec}[(\\\\Phi^TY+U_0^{-1}M_0)\\\\Lambda_{\\\\varepsilon}] \\\\\\\\ &amp;= \\\\text{vec}[(\\\\Phi^T\\\\Phi+U_0^{-1})^{-1}(\\\\Phi^TY+U_0^{-1}M_0)] \\\\\\\\ M_N&amp;= (\\\\Phi^T\\\\Phi+U_0^{-1})^{-1}(\\\\Phi^TY+U_0^{-1}M_0) \\\\end{aligned}\\\\]          it also means: $V_N = \\\\Lambda_{\\\\varepsilon}^{-1}$ and $U_N = (\\\\Phi^T\\\\Phi+U_0^{-1})^{-1}$, we see now, the mean of posterior do not depend on the $\\\\Lambda_{\\\\varepsilon}^{-1}$.      Unknown Precision Matrix  Also let $\\\\boldsymbol{w} = \\\\text{vec}(W)$Parameter distribution      Prior distribution: Matrix-Gaussian–Wishart distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w}\\\\mid \\\\boldsymbol{m}_0, \\\\Lambda_{\\\\varepsilon}, \\\\Lambda_0 ) &amp;= \\\\mathcal{N}( \\\\boldsymbol{w}\\\\mid \\\\boldsymbol{m}_0, \\\\Lambda_{\\\\varepsilon} \\\\otimes \\\\Lambda_0) \\\\\\\\ p(\\\\Lambda_{\\\\varepsilon}  \\\\mid V_0, \\\\nu_0) &amp;= \\\\mathcal{W}(\\\\Lambda_{\\\\varepsilon} \\\\mid V_0, \\\\nu_0) \\\\end{aligned}\\\\]        Posterior distribution: Matrix-Gaussian–Wishart distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w}\\\\mid \\\\boldsymbol{m}_N, \\\\Lambda_{\\\\varepsilon}, \\\\Lambda_N ) &amp;= \\\\mathcal{N}( \\\\boldsymbol{w}\\\\mid \\\\boldsymbol{m}_N, \\\\Lambda_{\\\\varepsilon} \\\\otimes \\\\Lambda_N) \\\\\\\\ p(\\\\Lambda_{\\\\varepsilon}  \\\\mid V_N, \\\\nu_N) &amp;= \\\\mathcal{W}(\\\\Lambda_{\\\\varepsilon} \\\\mid V_N, \\\\nu_N) \\\\\\\\ \\\\\\\\ p(\\\\boldsymbol{w}, \\\\Lambda_{\\\\varepsilon}) &amp;= \\\\mathcal{N}( \\\\boldsymbol{w}\\\\mid \\\\boldsymbol{m}_N, \\\\Lambda_{\\\\varepsilon} \\\\otimes \\\\Lambda_N)\\\\mathcal{W}(\\\\Lambda_{\\\\varepsilon} \\\\mid V_N, \\\\nu_N) \\\\end{aligned}\\\\]                  Inference:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{w}, \\\\Lambda_{\\\\varepsilon}) \\\\propto&amp; p(D \\\\vert W) \\\\mathcal{N}( \\\\boldsymbol{w}\\\\mid \\\\boldsymbol{m}_0, \\\\Lambda_{\\\\varepsilon} \\\\otimes \\\\Lambda_0)\\\\mathcal{W}(\\\\Lambda_{\\\\varepsilon} \\\\mid V_0, \\\\nu_0) \\\\end{aligned}\\\\]                    We can get:\\\\[\\\\begin{aligned} \\\\Lambda_N &amp;= \\\\Lambda_0 + (Y-\\\\Phi M_N)^T(Y-\\\\Phi W M_N) + (M_N - M_0)^T\\\\Lambda_0(M_N-M_0) \\\\\\\\ \\\\nu_n &amp;= \\\\nu_0 + n \\\\\\\\ \\\\Lambda_N &amp;= \\\\Phi^T\\\\Phi + \\\\Lambda_0 \\\\\\\\ M_N &amp;= (\\\\Phi^T\\\\Phi+\\\\Lambda_0)^{-1}(\\\\Phi^TY + \\\\Lambda_0M_0) \\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "Bayesian Inference for Gaussian",
    "url": "/posts/bayesian_inference_for_gaussian/",
    "categories": "Bayesian Inference",
    "tags": "",
    "date": "2022-02-20 00:00:00 +0000",
    





    "snippet": "UnivariateUnknown Mean, Known Variance/Precision  GaussianKnown Mean, Unknown Variance  Gammaknown Mean, Unknown Precision  Inverse-GammaUnknown Mean, Unknown Precision  Gauss-GammaUnknown Mean, Unknown Variance  Gauss-inverse-GammaMultivariateUnknown Mean, Known Covariance/Precision  GaussianKnown Mean, Unknown Precision  WishartKnown Mean, Unknown Covariance  inverse-WishartUnknown Mean, Unknown Precision  Gauss-WishartUnknown Mean, Unknown Covariance  Gauss-inverse-Wishart"
  },
  
  {
    "title": "Gaussian-Gamma Distribution",
    "url": "/posts/gaussian_gamma_distribution/",
    "categories": "Probability, Distribution",
    "tags": "Gaussian Distribution, Gamma Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Gaussian-Gamma distribution\\\\[\\\\begin{aligned} \\\\mathrm{GausGam}(\\\\mu, \\\\lambda  \\\\vert \\\\mu_0, \\\\tau , a, b) &amp;= \\\\mathcal{N}(\\\\mu \\\\vert \\\\mu_0, (\\\\tau\\\\lambda)^{-1}) \\\\mathrm{Gam}(\\\\lambda \\\\vert a,b)  \\\\\\\\ &amp;= \\\\left(\\\\frac {\\\\lambda\\\\tau}{2\\\\pi}\\\\right)^{1/2}\\\\exp\\\\left(-\\\\frac {\\\\lambda\\\\tau}{2} (\\\\mu -\\\\mu_0)^2\\\\right)  \\\\frac {1}{\\\\Gamma(a)} b^a\\\\lambda^{a-1}\\\\exp(-b\\\\lambda) \\\\end{aligned}\\\\]      The integral\\\\[\\\\begin{aligned} \\\\int_0^{+\\\\infty} \\\\int_{-\\\\infty}^{+\\\\infty}  \\\\mathrm{GausGam}(\\\\mu, \\\\lambda  \\\\vert \\\\mu_0, \\\\tau , a, b) d\\\\mu d\\\\lambda &amp;=  \\\\int_0^{+\\\\infty} \\\\int_{-\\\\infty}^{+\\\\infty}  \\\\left(\\\\frac {\\\\lambda\\\\tau}{2\\\\pi}\\\\right)^{1/2}\\\\exp\\\\left(-\\\\frac {\\\\lambda\\\\tau}{2} (\\\\mu -\\\\mu_0)^2\\\\right)  \\\\frac {1}{\\\\Gamma(a)} b^a\\\\lambda^{a-1}\\\\exp(-b\\\\lambda) d\\\\mu d\\\\lambda \\\\\\\\&amp;= \\\\int_0^{+\\\\infty}  \\\\frac {1}{\\\\Gamma(a)} b^a\\\\lambda^{a-1}\\\\exp(-b\\\\lambda) \\\\left[ \\\\int_{-\\\\infty}^{+\\\\infty}  \\\\left(\\\\frac {\\\\lambda\\\\tau}{2\\\\pi}\\\\right)^{1/2}\\\\exp\\\\left(-\\\\frac {\\\\lambda\\\\tau}{2} (\\\\mu -\\\\mu_0)^2\\\\right)  d\\\\mu \\\\right]d\\\\lambda \\\\\\\\&amp;=  \\\\int_0^{+\\\\infty}  \\\\frac {1}{\\\\Gamma(a)} b^a\\\\lambda^{a-1}\\\\exp(-b\\\\lambda) d\\\\lambda \\\\\\\\&amp;= 1  \\\\end{aligned}\\\\]        The Moments:\\\\[\\\\begin{aligned} \\\\mathbb{E}_{\\\\mu,\\\\lambda}[\\\\mu] &amp;= \\\\mu_0  \\\\\\\\   \\\\mathbb{E}_{\\\\mu,\\\\lambda}[\\\\lambda] &amp;= \\\\frac ab \\\\\\\\  \\\\mathbb{E}_{\\\\mu,\\\\lambda}[\\\\mu\\\\lambda] &amp;= \\\\mu_0\\\\frac ab \\\\\\\\  \\\\mathbb{E}_{\\\\mu,\\\\lambda}[\\\\mu^2] &amp;= \\\\mu_0^2 + \\\\frac {b}{(a-1)\\\\tau} \\\\\\\\  \\\\mathbb{E}_{\\\\mu,\\\\lambda}[\\\\lambda \\\\mu^2] &amp;= \\\\mu_0^2\\\\frac ab + \\\\frac 1\\\\tau      \\\\end{aligned}\\\\]        The Variance:\\\\[\\\\begin{aligned} \\\\mathrm{var}[\\\\mu] &amp;=  \\\\frac {b}{(a-1)\\\\tau} \\\\\\\\   \\\\mathrm{var}[\\\\lambda] &amp;=  \\\\frac {a}{b^2}  \\\\end{aligned}\\\\]  # Gaussian-Gamma distributionimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib import cmfrom scipy.stats import gamma, multivariate_normaldef gaussian_gamma(x,y, mu, tau, a,b):    &quot;&quot;&quot;the pdf of Gaussian Gamma distribution&quot;&quot;&quot;    return multivariate_normal.pdf(x, mean=mu, cov=1/(y*tau) ) * gamma.pdf(y, a=a, scale=1/b)mu, tau, a,b = 0,2,5,6m = 100n=50x = np.linspace(mu-2,mu+2, m)y = np.linspace(0.01, 2.01, n)x1, x2 = np.meshgrid(x,y)points = np.dstack([x1,x2]).reshape(m*n, 2)  fig1, ax1 = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;}, figsize=(8,8))z = []for x,y in points:    z.append(gaussian_gamma(x,y, mu, tau, a,b))z = np.asarray(z)ax1.plot_surface(x1, x2, z.reshape(n, m), cmap=cm.turbo)fig2, ax2 = plt.subplots(figsize=(8,8))ax2.contour(x1, x2, z.reshape(n, m), cmap=cm.turbo)plt.show()"
  },
  
  {
    "title": "Gaussian Distribution (3)",
    "url": "/posts/gaussian_distribution_3/",
    "categories": "Probability, Distribution",
    "tags": "Gaussian Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Conditional Gaussian Distribution      we partition $\\\\boldsymbol{x}$ into disjoint subsets $\\\\boldsymbol{x}_a, \\\\boldsymbol{x}_b$    \\\\(\\\\begin{aligned} \\\\boldsymbol{x} &amp;= \\\\binom{\\\\boldsymbol{x}_a}{\\\\boldsymbol{x}_b} \\\\\\\\ \\\\boldsymbol{\\\\mu} &amp;= \\\\binom{\\\\boldsymbol{\\\\mu}_a}{\\\\boldsymbol{\\\\mu}_b} \\\\\\\\ \\\\Sigma &amp;= \\\\binom{\\\\Sigma_{aa} \\\\space\\\\space \\\\Sigma_{ab}}{\\\\Sigma_{ba} \\\\space\\\\space \\\\Sigma_{bb}} \\\\end{aligned}\\\\)                  Note, $\\\\Sigma_{ab} = \\\\Sigma_{ba}^T$, and $\\\\Sigma_{aa},\\\\Sigma_{bb}$ alse are symmetric matrix                    Precision Matrix\\\\[\\\\begin{aligned} \\\\Lambda = \\\\Sigma^{-1} = \\\\binom{\\\\Lambda_{aa} \\\\space\\\\space \\\\Lambda_{ab}}{\\\\Lambda_{ba} \\\\space\\\\space \\\\Lambda_{bb}} \\\\end{aligned}\\\\]                  The Quadratic Term                  Firstly, the general quadratic term can be written by:\\\\[\\\\begin{aligned} -\\\\frac 12(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})  = -\\\\frac 12\\\\boldsymbol{x}^T\\\\Sigma^{-1}\\\\boldsymbol{x} + \\\\boldsymbol{x}^T\\\\Sigma^{-1}\\\\boldsymbol{\\\\mu} + \\\\mathrm{const}  \\\\end{aligned}\\\\]                    The term $\\\\mu_{a\\\\vert b}$, $\\\\Sigma_{a\\\\vert b}$ in the $p(\\\\boldsymbol{x}_a\\\\vert \\\\boldsymbol{x}_b)$ can be obtained in the following:\\\\[\\\\begin{aligned} -\\\\frac 12(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) &amp;= -\\\\frac 12(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Lambda(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\\\\\&amp;= -\\\\frac 12(\\\\boldsymbol{x}_a - \\\\boldsymbol{\\\\mu}_a)^T\\\\Lambda_{aa}(\\\\boldsymbol{x}_a - \\\\boldsymbol{\\\\mu}_a) \\\\\\\\ &amp;\\\\qquad -\\\\frac 12(\\\\boldsymbol{x}_a - \\\\boldsymbol{\\\\mu}_a)^T\\\\Lambda_{ab}(\\\\boldsymbol{x}_b - \\\\boldsymbol{\\\\mu}_b) \\\\\\\\ &amp;\\\\qquad -\\\\frac 12(\\\\boldsymbol{x}_b - \\\\boldsymbol{\\\\mu}_b)^T\\\\Lambda_{ba}(\\\\boldsymbol{x}_a - \\\\boldsymbol{\\\\mu}_a) \\\\\\\\ &amp;\\\\qquad -\\\\frac 12(\\\\boldsymbol{x}_b - \\\\boldsymbol{\\\\mu}_b)^T\\\\Lambda_{bb}(\\\\boldsymbol{x}_b - \\\\boldsymbol{\\\\mu}_b) \\\\\\\\ &amp;= -\\\\frac 12 \\\\boldsymbol{x}_a^T\\\\Lambda_{aa}\\\\boldsymbol{x}_a + \\\\boldsymbol{x}_a^T\\\\Lambda_{aa}\\\\boldsymbol{\\\\mu}_a - \\\\boldsymbol{x}_a^T\\\\Lambda_{ab}(\\\\boldsymbol{x}_b-\\\\boldsymbol{\\\\mu}_b) + \\\\mathrm{const} \\\\\\\\ &amp;= -\\\\frac 12 \\\\boldsymbol{x}_a^T\\\\Lambda_{aa}\\\\boldsymbol{x}_a + \\\\boldsymbol{x}_a^T(\\\\Lambda_{aa}\\\\boldsymbol{\\\\mu}_a - \\\\Lambda_{ab}(\\\\boldsymbol{x}_b-\\\\boldsymbol{\\\\mu}_b)) + \\\\mathrm{const}   \\\\end{aligned}\\\\]                              Then the result have been expressed in terms of partitioned precision matrix of the original joint distribution $p(\\\\boldsymbol{x}_a, \\\\boldsymbol{x}_b)$:\\\\[\\\\begin{aligned} \\\\Sigma_{a\\\\vert b} &amp;= \\\\Lambda_{aa}^{-1} \\\\\\\\ \\\\mu_{a\\\\vert b} &amp;= \\\\boldsymbol{\\\\mu}_a - \\\\Lambda_{aa}^{-1}\\\\Lambda_{ab}(\\\\boldsymbol{x}_b-\\\\boldsymbol{\\\\mu}_b) \\\\end{aligned}\\\\]                                Using the inverse of a partitioned matrix equation, the result can be expressed in terms of covariance matrix.                                          The inverse of a partitioned matrix equation                \\\\(\\\\begin{aligned} \\\\binom{A \\\\space\\\\space B}{C \\\\space\\\\space D} &amp;= \\\\binom{M \\\\space\\\\space -MBD^{-1}}{-D^{-1}CM \\\\space\\\\space D^{-1}(I+CMBD^{-1})} \\\\\\\\ \\\\\\\\ M &amp;= (A-BD^{-1}C)^{-1} \\\\end{aligned}\\\\)                                  Note, the $M^{-1}$ is known as the Schur Complement.                                                            Then we have:\\\\[\\\\begin{aligned} \\\\Lambda_{aa} &amp;= (\\\\Sigma_{aa} - \\\\Sigma_{ab}\\\\Sigma_{bb}^{-1}\\\\Sigma_{ba})^{-1} \\\\\\\\ \\\\Lambda_{ab} &amp;= -(\\\\Sigma_{aa} - \\\\Sigma_{ab}\\\\Sigma_{bb}^{-1}\\\\Sigma_{ba})^{-1}\\\\Sigma_{ab}\\\\Sigma_{bb}^{-1} \\\\end{aligned}\\\\]                                            Thus we obtain:\\\\[\\\\begin{aligned} \\\\Sigma_{a\\\\vert b} &amp;= \\\\Sigma_{aa} - \\\\Sigma_{ab}\\\\Sigma_{bb}^{-1}\\\\Sigma_{ba} \\\\\\\\ \\\\mu_{a\\\\vert b} &amp;= \\\\boldsymbol{\\\\mu}_a + \\\\Sigma_{ab}\\\\Sigma_{bb}^{-1}(\\\\boldsymbol{x}_b-\\\\boldsymbol{\\\\mu}_b) \\\\end{aligned}\\\\]                                  We can see that:                                          $\\\\Sigma_{a\\\\vert b}$ is independent of $\\\\boldsymbol{x}_a$;                      and $\\\\boldsymbol{\\\\mu}_{a\\\\vert b}$ is a linear function of $\\\\boldsymbol{\\\\mu}_b$.                                                                                                              Marginal Gaussian Distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}_a) = \\\\int p(\\\\boldsymbol{x}_a, \\\\boldsymbol{x}_b) d\\\\boldsymbol{x}_b \\\\end{aligned}\\\\]      According the partition form of the joint distribution $p(\\\\boldsymbol{x}_a, \\\\boldsymbol{x}_b)$, we can see:                  The terms about $\\\\boldsymbol{x}_b$:\\\\[\\\\begin{aligned}  -\\\\frac 12 \\\\boldsymbol{x}_b^T\\\\Lambda_{bb}\\\\boldsymbol{x}_b + \\\\boldsymbol{x}_b^T(\\\\Lambda_{bb}\\\\boldsymbol{\\\\mu}_b - \\\\Lambda_{ba}(\\\\boldsymbol{x}_a-\\\\boldsymbol{\\\\mu}_a))   \\\\end{aligned}\\\\]                              because:\\\\[\\\\begin{aligned} -\\\\frac 12 (\\\\boldsymbol{x}_b - \\\\tilde{\\\\boldsymbol{\\\\mu}}_b)^T\\\\Lambda_{bb}(\\\\boldsymbol{x}_b - \\\\tilde{\\\\boldsymbol{\\\\mu}}_b) = -\\\\frac 12 \\\\boldsymbol{x}_b^T\\\\Lambda_{bb}\\\\boldsymbol{x}_b + \\\\boldsymbol{x}_b^T\\\\Lambda_{bb}\\\\tilde{\\\\boldsymbol{\\\\mu}}_b -\\\\frac 12 \\\\tilde{\\\\boldsymbol{\\\\mu}}_b^T\\\\Lambda_{bb} \\\\tilde{\\\\boldsymbol{\\\\mu}}_b\\\\end{aligned}\\\\]                                so to let the integrating be easily performed, we need the addtional term $-\\\\frac 12 \\\\tilde{\\\\boldsymbol{\\\\mu}}b^T\\\\Lambda{bb}$, and $\\\\tilde{\\\\boldsymbol{\\\\mu}}_b$ can be obtained by:\\\\[\\\\begin{aligned} \\\\tilde{\\\\boldsymbol{\\\\mu}}_b &amp;= \\\\Lambda_{bb}^{-1}(\\\\Lambda_{bb}\\\\boldsymbol{\\\\mu}_b - \\\\Lambda_{ba}(\\\\boldsymbol{x}_a-\\\\boldsymbol{\\\\mu}_a)) \\\\end{aligned}\\\\]                                then:\\\\[\\\\begin{aligned} -\\\\frac 12 \\\\tilde{\\\\boldsymbol{\\\\mu}}_b^T\\\\Lambda_{bb}\\\\tilde{\\\\boldsymbol{\\\\mu}}_b = - \\\\frac 12 (\\\\Lambda_{bb}\\\\boldsymbol{\\\\mu}_b - \\\\Lambda_{ba}(\\\\boldsymbol{x}_a-\\\\boldsymbol{\\\\mu}_a))^T\\\\Lambda_{bb}^{-1}(\\\\Lambda_{bb}\\\\boldsymbol{\\\\mu}_b - \\\\Lambda_{ba}(\\\\boldsymbol{x}_a-\\\\boldsymbol{\\\\mu}_a))  \\\\end{aligned}\\\\]                                      Then the $p(\\\\boldsymbol{x}_a)$ can be obtained by :\\\\[\\\\begin{aligned} -\\\\frac 12 \\\\boldsymbol{x}_a^T\\\\Lambda_{aa}\\\\boldsymbol{x}_a + \\\\boldsymbol{x}_a^T(\\\\Lambda_{aa}\\\\boldsymbol{\\\\mu}_a + \\\\Lambda_{ab}\\\\boldsymbol{\\\\mu}_b)) + \\\\frac 12 (\\\\Lambda_{bb}\\\\boldsymbol{\\\\mu}_b - \\\\Lambda_{ba}(\\\\boldsymbol{x}_a-\\\\boldsymbol{\\\\mu}_a))^T\\\\Lambda_{bb}^{-1}(\\\\Lambda_{bb}\\\\boldsymbol{\\\\mu}_b - \\\\Lambda_{ba}(\\\\boldsymbol{x}_a-\\\\boldsymbol{\\\\mu}_a)) \\\\\\\\ = -\\\\frac 12 \\\\boldsymbol{x}_a^T(\\\\Lambda_{aa} - \\\\Lambda_{ab}\\\\Lambda_{bb}^{-1}\\\\Lambda_{ba})\\\\boldsymbol{x}_a + \\\\boldsymbol{x}_a^T(\\\\Lambda_{aa}-\\\\Lambda_{ab}\\\\Lambda_{bb}^{-1}\\\\Lambda_{ba})\\\\boldsymbol{\\\\mu}_a \\\\end{aligned}\\\\]                    Thus,                              The covariance matrix of $p(\\\\boldsymbol{x}_a)$ is given by\\\\[\\\\begin{aligned} \\\\mathrm{cov}[\\\\boldsymbol{x}_a] &amp;= (\\\\Lambda_{aa}-\\\\Lambda_{ab}\\\\Lambda_{bb}^{-1}\\\\Lambda_{ba})^{-1} \\\\\\\\ &amp;= \\\\Sigma_{aa} \\\\end{aligned}\\\\]                                The mean of the $p(\\\\boldsymbol{x}_a)$ is still $\\\\boldsymbol{\\\\mu}_a$\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\boldsymbol{x}_a] = \\\\boldsymbol{\\\\mu}_a \\\\end{aligned}\\\\]                              "
  },
  
  {
    "title": "Gaussian Distribution (2)",
    "url": "/posts/gaussian_distribution_2/",
    "categories": "Probability, Distribution",
    "tags": "Gaussian Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Maximum Likelihood for the Gaussian      Given a data set $\\\\mathbf{X} = {\\\\mathbf{x}_1,\\\\mathbf{x}_2,\\\\cdots, \\\\mathbf{x}_N}$ which are drawn independently from a multivariate Gaussian distribution, then we can estimate the parameters of the distribution by maximum likelihood.\\\\[\\\\begin{aligned} \\\\ln p(\\\\mathbf{X} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\Sigma) = -\\\\frac 12\\\\left(ND\\\\ln(2\\\\pi) + N\\\\ln(\\\\vert\\\\Sigma\\\\vert) + \\\\sum_{i=1}^N(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}) \\\\right) \\\\end{aligned}\\\\]        We can see, the likelihood function depends on the data set only through the two quantities:\\\\[\\\\begin{aligned} \\\\sum_{i=1}^N x_i, \\\\qquad \\\\sum_{i=1}^N x_ix_i^T  \\\\end{aligned}\\\\]          these are known as the sufficient statistics for the Gaussian distribution            The Maximum Likelihood solutions:  \\\\[\\\\begin{aligned} \\\\frac {\\\\partial \\\\ln p(\\\\mathbf{X} \\\\vert \\\\mathbf{\\\\mu}, \\\\Sigma)}{\\\\partial \\\\boldsymbol{\\\\mu}} &amp;= \\\\sum_{i=1}^N \\\\Sigma^{-1}(\\\\mathbf{x}_i - \\\\boldsymbol{\\\\mu}) \\\\\\\\ \\\\\\\\ \\\\Rightarrow\\\\qquad \\\\boldsymbol{\\\\mu}_{ml} &amp;= \\\\frac{1}{N} \\\\sum_{i=1}^N \\\\mathbf{x}_i \\\\end{aligned}\\\\]\\\\[\\\\begin{aligned} \\\\frac {\\\\partial \\\\ln p(\\\\mathbf{X} \\\\vert \\\\mathbf{\\\\mu}, \\\\Sigma)}{\\\\partial \\\\Sigma} &amp;= -\\\\frac 12\\\\left(N\\\\frac {\\\\partial}{\\\\partial \\\\Sigma}\\\\ln(\\\\vert\\\\Sigma\\\\vert) + \\\\frac {\\\\partial}{\\\\partial \\\\Sigma}\\\\sum_{i=1}^N(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}) \\\\right) \\\\\\\\&amp;= -\\\\frac 12\\\\left(N\\\\Sigma^{-1} + \\\\sum_{i=1}^N -\\\\Sigma^{-1}(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu})(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}  \\\\right) \\\\\\\\ &amp;= -\\\\frac 12 \\\\left(NI - \\\\Sigma^{-1}\\\\sum_{i=1}^N(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu})(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu})^T\\\\right)\\\\Sigma^{-1} \\\\\\\\ \\\\\\\\  \\\\Rightarrow\\\\qquad \\\\Sigma_{ml} &amp;= \\\\frac 1N \\\\sum_{i=1}^N(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})^T\\\\end{aligned}\\\\]      The Expectation of Maximum Likelihood solutions:\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\boldsymbol{\\\\mu}_{ml}] &amp;= \\\\frac 1N \\\\sum_{i=1}^N \\\\mathbb{E}[\\\\mathbf{x}_i] =\\\\boldsymbol{\\\\mu} \\\\\\\\ \\\\mathbb{E}[\\\\Sigma_{ml}] &amp;= \\\\frac 1N \\\\sum_{i=1}^N \\\\mathbb{E}[(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})^T] = \\\\frac {N-1}{N} \\\\Sigma \\\\end{aligned}\\\\]                  We the ml estimate of covariance has expeectation is less than the true value, and hence it is biased. But we can let $\\\\tilde{\\\\Sigma} = \\\\frac {1}{N-1} \\\\Sigma_{ml}$ to correct this bias.\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\tilde{\\\\Sigma}] = \\\\frac {N}{N-1} \\\\mathbb{E}[\\\\Sigma_{ml}] = \\\\Sigma \\\\end{aligned}\\\\]                    Proof of the $\\\\mathbb{E}[\\\\Sigma_{ml}]$:\\\\[\\\\begin{aligned} \\\\frac 1N \\\\sum_{i=1}^N\\\\mathbb{E}[(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})^T] &amp;=  \\\\frac 1N \\\\sum_{i=1}^N\\\\left(\\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{x}_i^T] -\\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{\\\\mu}_{ml}^T]-\\\\mathbb{E}[\\\\mathbf{\\\\mu}_{ml}\\\\mathbf{x}_i^T]+ \\\\mathbb{E}[\\\\mathbf{\\\\mu}_{ml}\\\\mathbf{\\\\mu}_{ml}^T] \\\\right) \\\\end{aligned}\\\\]                              $\\\\mathbb{E}[\\\\mathbf{x}i\\\\mathbf{\\\\mu}{ml}^T]$, according to the iid property of the data set, (means: $\\\\forall i\\\\neq j,\\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{x}_j^T]=\\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T; \\\\forall i=j, \\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{x}_j^T]=\\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{x}_i^T]=\\\\Sigma+\\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T$):   \\\\(\\\\begin{aligned} \\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{\\\\mu}_{ml}^T] = \\\\frac 1N \\\\sum_{j=1}^N \\\\mathbb{E}[\\\\mathbf{x}_i\\\\mathbf{x}_j] = \\\\frac 1N [(N-1)\\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T + \\\\Sigma+\\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T] = \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T+\\\\frac 1N \\\\Sigma \\\\end{aligned}\\\\)                                With the same procdure:\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\mathbf{\\\\mu}_{ml}\\\\mathbf{x}_i^T] = \\\\mathbb{E}[\\\\mathbf{\\\\mu}_{ml}\\\\mathbf{\\\\mu}_{ml}^T] = \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T+\\\\frac 1N \\\\Sigma  \\\\end{aligned}\\\\]                                Thus:\\\\[\\\\begin{aligned} \\\\frac 1N \\\\sum_{i=1}^N\\\\mathbb{E}[(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})(\\\\mathbf{x}_i-\\\\boldsymbol{\\\\mu}_{ml})^T] = \\\\frac 1N \\\\sum_{i=1}^N (\\\\Sigma + \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T - (\\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T+\\\\frac 1N \\\\Sigma)) = \\\\frac {N-1}{N} \\\\Sigma \\\\end{aligned}\\\\]                              Sequential Estimation      the sequential estimation of the Maximum Likelihood solution\\\\[\\\\begin{aligned} \\\\boldsymbol{\\\\mu}_{ml}^{(N)} &amp;= \\\\frac 1N \\\\sum_{i=1}^N \\\\mathbf{x}_i \\\\\\\\&amp;= \\\\frac 1N \\\\mathbf{x}_N + \\\\frac 1N \\\\sum_{i=1}^{N-1} \\\\mathbf{x}_i \\\\\\\\&amp;= \\\\frac 1N \\\\mathbf{x}_N + \\\\frac {N-1}{N}\\\\boldsymbol{\\\\mu}_{ml}^{(N-1)}  \\\\\\\\&amp;= \\\\boldsymbol{\\\\mu}_{ml}^{(N-1)} + \\\\frac 1N (\\\\mathbf{x}_N-\\\\boldsymbol{\\\\mu}_{ml}^{(N-1)})\\\\end{aligned}\\\\]          But this is not always be able to derive a sequential algorithm by this route.            Robbins-Monro Algorithm: this is a stochastic approximation.                  Consider the function:\\\\[\\\\begin{aligned} f(\\\\boldsymbol{\\\\theta}) = \\\\mathbb{E}_\\\\boldsymbol{x}[\\\\phi(\\\\boldsymbol{\\\\theta}, \\\\boldsymbol{x})\\\\vert \\\\boldsymbol{\\\\theta}] , \\\\qquad \\\\boldsymbol{\\\\theta} \\\\in R^D \\\\end{aligned}\\\\]                  $\\\\boldsymbol{x}$ is a random vector of unknown statistics.          the goal is to compute a root of $f(\\\\boldsymbol{\\\\theta})$                            The Robbins-Monro Algorithm\\\\[\\\\begin{aligned} \\\\boldsymbol{\\\\theta}_{n} &amp;= \\\\boldsymbol{\\\\theta}_{n-1}+ \\\\alpha_{n-1} \\\\phi(\\\\boldsymbol{\\\\theta}_{n-1}, \\\\mathbf{x}_{n})  \\\\end{aligned}\\\\]                  the convergence conditions  \\\\(\\\\begin{aligned} \\\\lim_{N \\\\rightarrow \\\\infty} \\\\alpha_{N-1} &amp;= 0 \\\\\\\\ \\\\sum_{N=1}^\\\\infty a_N &amp;= \\\\infty \\\\\\\\ \\\\sum_{N=1}^\\\\infty a_N^2 &amp; \\\\lt \\\\infty \\\\end{aligned}\\\\)                          Using Robbins-Monro Algorithm to solve a general maximum likelihood problem sequentially\\\\[\\\\begin{aligned} \\\\frac {\\\\partial}{\\\\partial \\\\boldsymbol{\\\\theta}} \\\\left.\\\\left( \\\\frac 1N \\\\sum_{i=1}^N \\\\ln p(\\\\mathbf{x}_i\\\\vert \\\\boldsymbol{\\\\theta}) \\\\right)\\\\right\\\\vert_{\\\\boldsymbol{\\\\theta}_{ml}} &amp;= 0 \\\\\\\\ \\\\\\\\  \\\\lim_{N\\\\rightarrow \\\\infty} \\\\frac 1N \\\\sum_{i=1}^N \\\\frac {\\\\partial}{\\\\partial \\\\boldsymbol{\\\\theta}} \\\\ln p(\\\\boldsymbol{x}_i\\\\vert \\\\boldsymbol{\\\\theta}) &amp;= \\\\mathbb{E}_{\\\\boldsymbol{x}}[\\\\frac {\\\\partial}{\\\\partial \\\\boldsymbol{\\\\theta}}\\\\ln p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\theta})] \\\\end{aligned}\\\\]                  Then:\\\\[\\\\begin{aligned} \\\\boldsymbol{\\\\theta}_{n} &amp;= \\\\boldsymbol{\\\\theta}_{n-1} + \\\\alpha_{n-1}\\\\frac {\\\\partial}{\\\\partial \\\\boldsymbol{\\\\theta}}\\\\ln p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\theta}) \\\\end{aligned}\\\\]                  the sequential estimation of the mean of a gaussian distribution    \\\\(\\\\begin{aligned}  \\\\frac {\\\\partial}{\\\\partial \\\\boldsymbol{\\\\mu}_{ml}}\\\\ln p(\\\\boldsymbol{x}_n\\\\vert \\\\boldsymbol{\\\\mu}_{ml}, \\\\Sigma) = \\\\Sigma^{-1}(\\\\mathbf{x}_i - \\\\boldsymbol{\\\\mu})  \\\\end{aligned}\\\\)          then, we let $\\\\alpha_{n-1}=\\\\frac \\\\Sigma N$, we can also obtain the sequential estimation of the Maximum Likelihood solution.      Parameter Estimate of Gaussian Distribution\\\\[p(x) = \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) = \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}} \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} (x-\\\\mu)^2\\\\right)\\\\]      Suppose we have an i.i.d data set of observation $\\\\mathbf{X} = {x_1, x_2, \\\\cdots, x_N }$, we shall determine values from the unknown parameters $\\\\mu$ and $\\\\sigma^2$ in Gaussian by maximizing the likelihood function.        Likelihood function:\\\\[L(\\\\mu, \\\\sigma^2) = p(\\\\mathbf{X} \\\\vert \\\\mu, \\\\sigma^2) = \\\\prod_{i=1}^N \\\\mathcal{N}(x_i\\\\vert \\\\mu, \\\\sigma^2)\\\\]        The log likelihood funciton:\\\\[LL(\\\\mu, \\\\sigma^2) = \\\\ln p(\\\\mathbf{X} \\\\vert \\\\mu, \\\\sigma^2)  = -\\\\frac {1}{2\\\\sigma^2} \\\\sum_{i=1}^N (x_i - \\\\mu)^2 - \\\\frac N2 \\\\ln \\\\sigma^2 - \\\\frac N2 \\\\ln (2\\\\pi)\\\\]        The partial derivative\\\\[\\\\begin{aligned} \\\\frac {\\\\partial LL}{\\\\partial \\\\mu} &amp;= \\\\frac {1}{\\\\sigma^2} \\\\sum_{i=1}^N (x_i -\\\\mu) \\\\\\\\ \\\\frac {\\\\partial LL}{\\\\partial \\\\sigma^2} &amp;= \\\\frac {1}{2\\\\sigma^2}( \\\\frac {1}{\\\\sigma^2}\\\\sum_{i=1}^N (x_i - \\\\mu)^2  - N)\\\\end{aligned}\\\\]        Then we have:\\\\[\\\\begin{aligned} \\\\mu_{ml} &amp;= \\\\arg \\\\max_{\\\\mu} LL(\\\\mu, \\\\sigma^2) = \\\\frac 1N \\\\sum_{i=1}^N x_i \\\\\\\\ \\\\sigma^2_{ml} &amp;= \\\\arg \\\\max_{\\\\sigma^2} LL(\\\\mu, \\\\sigma^2) = \\\\frac 1N \\\\sum_{i=1}^N (x_i - \\\\mu)^2 = \\\\frac 1N \\\\sum_{i=1}^N (x_i - \\\\mu_{ml})^2 \\\\end{aligned}\\\\]          They are respectively the sample mean and sample variance.            The expectation of $\\\\mu_{ml}$ and $\\\\sigma^2_{ml}$.\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\mu_{ml}] &amp;= \\\\frac 1N \\\\sum_{i=1}^N \\\\mathbb{E}[x_i] \\\\\\\\ &amp;= \\\\frac 1N N\\\\mu  \\\\\\\\ &amp;= \\\\mu \\\\\\\\ \\\\mathbb{E}[\\\\sigma^2_{ml}] &amp;= \\\\frac 1N \\\\sum_{i=1}^N \\\\mathbb{E}[(x_i-\\\\mu_{ml})^2]  \\\\\\\\&amp; = \\\\frac 1N \\\\sum_{i=1}^N \\\\mathbb{E}[x_i^2] - \\\\frac 2N \\\\sum_{i=1}^N \\\\mathbb{E}[x_i\\\\mu_{ml}] + \\\\frac 1N \\\\sum_{i=1}^N\\\\mathbb{E}[\\\\mu_{ml}^2] \\\\\\\\&amp;= \\\\sigma^2 + \\\\mu^2 - \\\\frac 2N \\\\mathbb{E}[\\\\sum_{i=1}^Nx_i \\\\mu_{ml}] + \\\\mathbb{E}[\\\\mu_{ml}^2] \\\\\\\\&amp;= \\\\sigma^2 + \\\\mu^2 - \\\\frac 2N \\\\mathbb{E}[N\\\\mu_{ml}^2] + \\\\mathbb{E}[\\\\mu_{ml}^2] \\\\\\\\&amp;= \\\\sigma^2 + \\\\mu^2 - \\\\mathbb{E}[\\\\mu_{ml}^2] \\\\\\\\&amp;= \\\\sigma^2 + \\\\mu^2 - \\\\mathbb{E}[(\\\\frac 1N \\\\sum_{i=1}^N x_i)^2] \\\\\\\\ &amp;= \\\\sigma^2 + \\\\mu^2 - \\\\frac {1}{N^2} [(N^2-N)\\\\mu^2 + N(\\\\mu^2+\\\\sigma^2)] \\\\\\\\ &amp;= \\\\sigma^2 - \\\\frac 1N \\\\sigma^2 \\\\\\\\&amp;= \\\\frac {N-1}{N} \\\\sigma^2 \\\\end{aligned}\\\\]          The result is attained according to the i.i.d assuming of the data set.              Then we see that the estimate for mean is unbiased, but for variance is not. The maximum likelihood approach systematically underestimates the variance of the distribution. On average the maximum likelihood estiamte will obtain the correct mean but will underestimate the true variance by a factor $\\\\frac {N-1}{N}$.                              But if let the estiamte for the variance be\\\\[\\\\tilde{\\\\sigma}^2 = \\\\frac {N}{N-1} \\\\sigma_{ml}^2 = \\\\frac 1{N-1} \\\\sum_{i=1}^N (x_i -\\\\mu_{ml})^2\\\\]            then it will be unbiased.                                Actually we also can see, in the limit $N\\\\rightarrow \\\\infty$ the maximum likelihhod solution for the variance equals the true variance of the distribution that generated the data.                              # Parameter Estimateimport numpy as npclass GaussianParameterEstimator(object):        def __init__(self):        pass        def estimate_mean_by_ml(self, x):        &quot;&quot;&quot;estimate mean by maximum likehood: obtained by simple mean&quot;&quot;&quot;        return np.mean(x, axis=0)        def estimate_variance_by_ml(self, x, mu):        &quot;&quot;&quot;estiamte variance by maximum likelihood: obtained by simple variance&quot;&quot;&quot;        return np.mean((x-mu)**2, axis=0)true_mean = 1true_sigma = 10estimator = GaussianParameterEstimator()print(&quot;True mean: {} and True variance: {}&quot;.format(true_mean, true_sigma**2))for N in [10, 100, 1e4, 1e6, 1e8]:    iid_data = np.random.normal(true_mean, true_sigma, int(N)) # generate data    mean = estimator.estimate_mean_by_ml(iid_data)    variance = estimator.estimate_variance_by_ml(iid_data, mean)    print(&quot;mean_error: {}, variance_error: {}, N: {}, &quot;.format(round(abs(mean-true_mean), 4), round(abs(variance-true_sigma**2), 4), N))# Output&quot;&quot;&quot;True mean: 1 and True variance: 100mean_error: 4.166, variance_error: 41.0061, N: 10, mean_error: 1.2388, variance_error: 15.1728, N: 100, mean_error: 0.0142, variance_error: 0.7322, N: 10000.0, mean_error: 0.0045, variance_error: 0.1071, N: 1000000.0, mean_error: 0.0003, variance_error: 0.0037, N: 100000000.0, &quot;&quot;&quot;  Then we can see as $N\\\\rightarrow \\\\infty$, the maximum solution of mean and variance getting closer the mean and variance of the distribution that generated the data."
  },
  
  {
    "title": "Gaussian Distribution (1)",
    "url": "/posts/gaussian_distribution_1/",
    "categories": "Probability, Distribution",
    "tags": "Gaussian Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Gaussian DistributionUnivariate\\\\[\\\\begin{aligned} \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) = \\\\frac {1}{\\\\sqrt{2\\\\pi \\\\sigma^2}} \\\\exp\\\\left(- \\\\frac {1}{2\\\\sigma^2} (x-\\\\mu)^2 \\\\right) \\\\end{aligned}\\\\]      The integral of Gaussian distribution\\\\[\\\\begin{aligned} \\\\int_{-\\\\infty}^{+\\\\infty} \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2)  dx &amp;=  \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}}  \\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} (x-\\\\mu)^2\\\\right)dx \\\\\\\\ &amp;= \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}}  \\\\int_{-\\\\infty}^{+\\\\infty} \\\\exp\\\\left(-\\\\left(\\\\frac {x-\\\\mu}{(2\\\\sigma^2)^{1/2}}\\\\right)^2\\\\right)dx\\\\qquad y=\\\\frac {x-\\\\mu}{(2\\\\sigma^2)^{1/2}} \\\\\\\\ &amp;= \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}}  \\\\int_{-\\\\infty}^{+\\\\infty} \\\\exp\\\\left(-y^2\\\\right) (2\\\\sigma^2)^{1/2}dy \\\\qquad \\\\frac {dx}{dy} = (2\\\\sigma^2)^{1/2} \\\\\\\\ &amp;= \\\\frac {1}{\\\\sqrt{\\\\pi}}\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp\\\\left(-y^2\\\\right)dy \\\\\\\\ &amp;= 1 \\\\end{aligned}\\\\]                  we have uesed the property $\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp(-x^2)dx =\\\\sqrt{\\\\pi} $, the proof is given by:                              Firstly, if we let $I =\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp(-x^2)dx $, then we have $\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp(-x^2-y^2)dxdy = I^2$, and\\\\[\\\\begin{aligned} \\\\mathrm{Let} \\\\space I^2 &amp;= \\\\int_{-\\\\infty}^{+\\\\infty}\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp(-x^2-y^2)dxdy \\\\qquad x=r\\\\cos \\\\theta, y=r\\\\sin \\\\theta \\\\\\\\ &amp;=  \\\\int_{0}^{+\\\\infty}\\\\int_{0}^{2\\\\pi} \\\\exp(-r^2)\\\\left\\\\vert\\\\frac {\\\\partial(x,y)}{\\\\partial(r,\\\\theta)} \\\\right\\\\vert d\\\\theta dr \\\\\\\\&amp;=    \\\\int_{0}^{+\\\\infty}\\\\int_{0}^{2\\\\pi} \\\\exp(-r^2)\\\\left\\\\vert\\\\begin{matrix} \\\\cos\\\\theta &amp; -r\\\\sin\\\\theta  \\\\\\\\\\\\sin\\\\theta &amp; r\\\\cos\\\\theta  \\\\end{matrix} \\\\right\\\\vert d\\\\theta dr \\\\\\\\ &amp;=  \\\\int_{0}^{+\\\\infty}\\\\int_{0}^{2\\\\pi} \\\\exp(-r^2)r d\\\\theta dr \\\\\\\\&amp;=   2\\\\pi \\\\int_{0}^{+\\\\infty} \\\\exp(-r^2)r  dr \\\\\\\\ &amp;= 2\\\\pi \\\\left.(-\\\\frac {1}{2}\\\\exp(-r^2)  \\\\right\\\\vert_{0}^{\\\\infty} \\\\\\\\ &amp;= 2\\\\pi (0-(-\\\\frac 12)) \\\\\\\\&amp;= \\\\pi \\\\end{aligned}\\\\]                                Then we have: $I = \\\\sqrt{\\\\pi}$                                    The Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[x] &amp;= \\\\int_{-\\\\infty}^{+\\\\infty} x\\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2)  dx \\\\\\\\ &amp;=   \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}}\\\\int_{-\\\\infty}^{+\\\\infty} x \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} (x-\\\\mu)^2 \\\\right)  dx \\\\\\\\&amp;= \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}}\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} y^2 \\\\right)(y+\\\\mu) dy \\\\\\\\&amp;= \\\\frac {1}{(2\\\\pi \\\\sigma^2)^{1/2}} \\\\left(0 + \\\\mu\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} y^2 \\\\right) dy  \\\\right)  \\\\\\\\&amp;= \\\\mu \\\\end{aligned}\\\\]          This is given by $f(y) =  \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} y^2 \\\\right)y$ is an odd function on interval $(-\\\\infty, \\\\infty)$, and $\\\\int_{-\\\\infty}^{+\\\\infty}  \\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2} y^2 \\\\right) dy = \\\\sqrt{2\\\\pi \\\\sigma^2}$            The Variance\\\\[\\\\begin{aligned} \\\\qquad&amp; \\\\frac {\\\\partial \\\\left(\\\\int \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) dx\\\\right)} {\\\\partial \\\\sigma^2} = \\\\frac {\\\\partial 1}{\\\\partial \\\\sigma^2} \\\\\\\\ \\\\Longrightarrow \\\\qquad&amp; \\\\int (-\\\\frac 1{2\\\\sigma^2} + \\\\frac {(x-\\\\mu)^2}{2\\\\sigma^4}) \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) dx = 0 \\\\\\\\ \\\\Longrightarrow \\\\qquad&amp; \\\\int (x-\\\\mu)^2 \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) dx = \\\\sigma^2 \\\\\\\\ \\\\Longrightarrow \\\\qquad&amp; \\\\mathrm{var}[x] = \\\\int (x-\\\\mathbb{E}[x])^2  \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) dx = \\\\sigma^2  \\\\end{aligned}\\\\]  Multivariate\\\\[\\\\begin{aligned} \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\Sigma) = \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\exp\\\\left(-\\\\frac 12 (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\right) \\\\end{aligned}\\\\]      Mahalanobis Distance\\\\[\\\\begin{aligned} D_{x,y} = \\\\sqrt{(x-y)^TS^{-1}(x-y)} \\\\end{aligned}\\\\]        The Gaussian distribution will be constant on surfaces in $x$-space for which this quadratic form is constant.\\\\[\\\\begin{aligned} \\\\Delta^2 = (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\end{aligned}\\\\]        The Covariance Matrix $\\\\Sigma$                  Firstly, the matrix $\\\\Sigma$ can be taken to be symmetric, without loss of generality, because any antisymmetric component would disappear from the exponent.                              Suppose an arbitrary percision matrix $\\\\Lambda$ can be written as $\\\\Lambda^S + \\\\Lambda^A$, where they statify:\\\\[\\\\begin{aligned} \\\\Lambda^S = \\\\frac {\\\\Lambda_{ij}+\\\\Lambda_{ji}}{2}, \\\\qquad \\\\Lambda^A = \\\\frac {\\\\Lambda_{ij}-\\\\Lambda_{ji}}{2} \\\\end{aligned}\\\\]                          Then $\\\\Lambda^S = (\\\\Lambda^S)^T$, $\\\\Lambda^A = -(\\\\Lambda^A)^T$              So in the quadratic form, the value of $(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Lambda^A(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})$ is zero, and then only the symmetric matrix $\\\\Lambda^S$ left.                                                  Then because $\\\\Sigma$ is rela, symmetric matrix, its eigenvalues will be real, and its eigenvectors can be chosen to form an orthonormal set, so that:\\\\[\\\\begin{aligned} \\\\boldsymbol{u}_i^T\\\\boldsymbol{u}_j = \\\\left\\\\{\\\\begin{aligned}1\\\\qquad &amp;i=j \\\\\\\\ 0 \\\\qquad &amp;i\\\\neq j \\\\end{aligned} \\\\right. \\\\end{aligned}\\\\]                    And then, we have:\\\\[\\\\begin{aligned} \\\\Sigma &amp;= \\\\sum_{i=1}^D \\\\lambda_i \\\\boldsymbol{u}_i\\\\boldsymbol{u}_i^T \\\\\\\\ \\\\Sigma^{-1} &amp;= \\\\sum_{i=1}^D \\\\frac 1 \\\\lambda_i \\\\boldsymbol{u}_i\\\\boldsymbol{u}_i^T \\\\\\\\ \\\\vert \\\\Sigma \\\\vert &amp;= \\\\prod_{i=1}^D \\\\lambda_i  \\\\end{aligned}\\\\]                    Substituting this reuslt into the quadratic form, we obtain:\\\\[\\\\begin{aligned} \\\\Delta^2 &amp;= (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T \\\\left(\\\\sum_{i=1}^D \\\\frac 1 \\\\lambda_i \\\\boldsymbol{u}_i\\\\boldsymbol{u}_j^T \\\\right)(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\\\\\&amp;= \\\\sum_{i=1}^D \\\\frac {y_i^2} {\\\\lambda_i} \\\\qquad let \\\\space y_i = \\\\boldsymbol{u}^T(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\end{aligned}\\\\]            Interpreting ${y_i}$ as a new corrdinate system defined by the otthonormal vectors $\\\\boldsymbol{u}_i$ that are shifted and rotated with respect to the original $x_i$ coordinates.                              Forming the vector $\\\\boldsymbol{y}=(y_1,y_2,\\\\cdots,y_D)^T$, we have\\\\[\\\\boldsymbol{y} = U(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})\\\\]                          where the row of $U$ is $\\\\boldsymbol{u}_i^T$, moreover $U$ is an orthogonal matrix, $UU^T = U^TU=I$                                                  For the Gaussian distribution to be well defined, it is necessary for all of the eigenvalues $\\\\lambda_i$ of the covariance matrix to be strictly positive, oterwise the distribution cannot be properly normalized.            In going from $\\\\boldsymbol{x}$ to the $\\\\boldsymbol{y}$ corrdinate system, we have a Jacobian matrix $\\\\boldsymbol{J}$, and :    \\\\[\\\\begin{aligned} \\\\frac {\\\\partial y_i}{\\\\partial x_j} = mu_ij = U_{ij} = J_{ij} \\\\end{aligned}\\\\]                  Then we can see $\\\\boldsymbol{J} = U$, and $\\\\vert J \\\\vert = \\\\vert U \\\\vert = 1$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{y}) = p(\\\\boldsymbol{x})\\\\vert J \\\\vert &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}}  \\\\frac {1}{\\\\prod_{i=1}^D \\\\lambda_i^{1/2}} \\\\exp(-\\\\frac 12 \\\\sum_{i=1}^D\\\\frac {y_i^2}{\\\\lambda_i}) \\\\\\\\&amp;= \\\\prod_{i=1}^D \\\\frac 1{(2\\\\pi\\\\lambda_i)^{1/2}} \\\\exp(-\\\\frac {y_i^2}{2\\\\lambda_i}) \\\\end{aligned}\\\\]                  this is the product of $D$ independent univariate Gaussian distribution.                          The Moments of Gaussian distribution                  First Order Moments\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\boldsymbol{x}] &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\right)\\\\boldsymbol{x} d\\\\boldsymbol{x} \\\\\\\\ &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\boldsymbol{z}^T\\\\Sigma^{-1}\\\\boldsymbol{z} \\\\right)(\\\\boldsymbol{z}+\\\\boldsymbol{\\\\mu}) d\\\\boldsymbol{z} \\\\end{aligned}\\\\]                  and because $f(\\\\boldsymbol{z}) = \\\\exp\\\\left(-\\\\frac 12 \\\\boldsymbol{z}^T\\\\Sigma^{-1}\\\\boldsymbol{z} \\\\right)\\\\boldsymbol{z}$ is an odd function, then the integrals over the range $(-\\\\infty, \\\\infty)$ will be zero. Thus:        \\\\[\\\\begin{aligned}  \\\\mathbb{E}[\\\\boldsymbol{x}] = \\\\boldsymbol{\\\\mu} \\\\end{aligned}\\\\]                    Second Order Moments\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\boldsymbol{x}\\\\boldsymbol{x}^T] &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 (\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu})^T\\\\Sigma^{-1}(\\\\boldsymbol{x} - \\\\boldsymbol{\\\\mu}) \\\\right)\\\\boldsymbol{x}\\\\boldsymbol{x}^T d\\\\boldsymbol{x} \\\\\\\\ &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\boldsymbol{z}^T\\\\Sigma^{-1}\\\\boldsymbol{z} \\\\right)(\\\\boldsymbol{z}+\\\\boldsymbol{\\\\mu})(\\\\boldsymbol{z}+\\\\boldsymbol{\\\\mu})^T d\\\\boldsymbol{z} \\\\\\\\ &amp;= \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T + \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\boldsymbol{z}^T\\\\Sigma^{-1}\\\\boldsymbol{z} \\\\right)\\\\boldsymbol{z}\\\\boldsymbol{z}^T d\\\\boldsymbol{z} \\\\end{aligned}\\\\]                              and because $\\\\boldsymbol{z} = (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}) = U\\\\boldsymbol{y} = \\\\sum_{j=1}^D y_j \\\\boldsymbol{u}_j$. Thus:\\\\[\\\\begin{aligned} \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\boldsymbol{z}^T\\\\Sigma^{-1}\\\\boldsymbol{z} \\\\right)\\\\boldsymbol{z}\\\\boldsymbol{z}^T d\\\\boldsymbol{z} &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\sum_{j=1}^D y_j \\\\boldsymbol{u}_j^T \\\\left( \\\\sum_{i=1}^D\\\\frac {1}{\\\\lambda_i}\\\\boldsymbol{u}_i\\\\boldsymbol{u}_i^T \\\\right)\\\\sum_{k=1}^D y_k \\\\boldsymbol{u}_k \\\\right)\\\\sum_{m=1}^D y_m \\\\boldsymbol{u}_m \\\\sum_{n=1}^D y_n \\\\boldsymbol{u}_n^T d\\\\boldsymbol{y} \\\\\\\\ &amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\sum_{m=1}^D\\\\sum_{n=1}^D \\\\boldsymbol{u}_m\\\\boldsymbol{u}_n^T\\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\left( \\\\sum_{i=1}^D\\\\frac {y_i}{\\\\lambda_i} \\\\boldsymbol{u}_i^T \\\\right)\\\\sum_{k=1}^D y_k \\\\boldsymbol{u}_k \\\\right) y_m y_n d\\\\boldsymbol{y} \\\\\\\\&amp;= \\\\frac {1}{(2\\\\pi)^{D/2}} \\\\frac {1}{\\\\vert\\\\Sigma\\\\vert^{1/2}} \\\\sum_{m=1}^D\\\\sum_{n=1}^D \\\\boldsymbol{u}_m\\\\boldsymbol{u}_n^T\\\\int \\\\exp\\\\left(-\\\\frac 12 \\\\sum_{i=1}^D\\\\frac {y_i^2}{\\\\lambda_i} \\\\right) y_m y_n d\\\\boldsymbol{y} \\\\\\\\ &amp;= \\\\sum_{m=1}^D\\\\sum_{n=1}^D \\\\boldsymbol{u}_m\\\\boldsymbol{u}_n^T \\\\prod_{i=1}^D \\\\int \\\\frac {1}{(2\\\\pi \\\\lambda_i)^{1/2}}\\\\exp(-\\\\frac {y_i^2}{2\\\\lambda_i})y_my_n d\\\\boldsymbol{y}\\\\end{aligned}\\\\]                                          The integrals $\\\\forall i\\\\neq m$, and $i\\\\neq n$ will be 1,so:\\\\[\\\\begin{aligned} &amp; \\\\left\\\\{\\\\begin{aligned} &amp;\\\\sum_{m=1}^D\\\\sum_{n=1}^D \\\\boldsymbol{u}_m\\\\boldsymbol{u}_n^T \\\\int \\\\frac {1}{(2\\\\pi \\\\lambda_m)^{1/2}}\\\\exp(-\\\\frac {y_m^2}{2\\\\lambda_m})y_m dy_m y_ndy_n \\\\qquad m\\\\neq n, i = m \\\\\\\\ &amp;\\\\sum_{m=1}^D\\\\sum_{n=1}^D \\\\boldsymbol{u}_m\\\\boldsymbol{u}_n^T \\\\int \\\\frac {1}{(2\\\\pi \\\\lambda_n)^{1/2}}\\\\exp(-\\\\frac {y_n^2}{2\\\\lambda_n})y_ndy_n y_m dy_m \\\\qquad m\\\\neq n, i=n \\\\\\\\ &amp;\\\\sum_{i=1}^D \\\\boldsymbol{u}_i\\\\boldsymbol{u}_i^T \\\\int \\\\frac {1}{(2\\\\pi \\\\lambda_i)^{1/2}}\\\\exp(-\\\\frac {y_i^2}{2\\\\lambda_i})y_i^2 dy_i \\\\qquad i=m=n \\\\end{aligned}\\\\right. \\\\\\\\ =&amp; \\\\left\\\\{\\\\begin{aligned} &amp;0 \\\\qquad m\\\\neq n, i = m \\\\\\\\ &amp;0 \\\\qquad m\\\\neq n, i = n \\\\\\\\ &amp;\\\\sum_{i=1}^D \\\\boldsymbol{u}_i\\\\boldsymbol{u}_i^T (0+\\\\lambda_i) \\\\qquad i=m=n \\\\end{aligned}\\\\right. \\\\end{aligned}\\\\]                                            The final step, we have use the $\\\\mathbb{E}[x^2] = \\\\mu^2 + \\\\sigma^2$, thus:\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\boldsymbol{x}\\\\boldsymbol{x}^T] &amp;= \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T + 0 + 0 + \\\\sum_{i=1}^D \\\\boldsymbol{u}_i\\\\boldsymbol{u}_i^T \\\\lambda_i \\\\\\\\ &amp;= \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T+ \\\\Sigma\\\\end{aligned}\\\\]                                                                And then :\\\\[\\\\begin{aligned} \\\\mathrm{cov}[\\\\boldsymbol{x}] = \\\\mathbb{E}[\\\\boldsymbol{x}\\\\boldsymbol{x}^T] - \\\\mathbb{E}[\\\\boldsymbol{x}]\\\\mathbb{E}[\\\\boldsymbol{x}]^T = \\\\Sigma \\\\end{aligned}\\\\]                  The number of free parameters in the Gaussian distribution:                  A general symmetric covariance matrix $\\\\Sigma$ will have $\\\\frac {D^2-D}{2} + D = \\\\frac {D(D+1)}{2}$ independent parameters, and there  another $D$ independent parameters in $\\\\boldsymbol{\\\\mu}$, then giving $\\\\frac {D(D+3)}{2}$ parameters in total. Thus we can see, it is grows quadratically with $D$                    If the covariance matrix $\\\\Sigma$ is diagonal, then the number of parameters will be $2D$ in total.                    Especially, if we consider the covariance matrix $\\\\Sigma$ with $\\\\Sigma = \\\\sigma^2 I$, isotropic covariance, then the number of parameters will be $D+1$ in total.            "
  },
  
  {
    "title": "Gamma Distribution",
    "url": "/posts/gamma_distribution/",
    "categories": "Probability, Distribution",
    "tags": "Gamma Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Gamma Distribution\\\\[\\\\begin{aligned} \\\\mathrm{Gam}(\\\\lambda \\\\vert a,b) &amp;= \\\\frac {1}{\\\\Gamma(a)} b^a \\\\lambda^{a-1}\\\\exp(-b\\\\lambda) \\\\end{aligned}\\\\]      To verity the integral over $\\\\lambda$ is 1\\\\[\\\\begin{aligned} \\\\int_0^\\\\infty \\\\mathrm{Gam}(\\\\lambda \\\\vert a,b) &amp;= \\\\int_0^\\\\infty \\\\frac {1}{\\\\Gamma(a)} b^a \\\\lambda^{a-1}\\\\exp(-b\\\\lambda)d\\\\lambda \\\\\\\\&amp;= \\\\frac {1}{\\\\Gamma(a)}b^a  \\\\int_0^\\\\infty (\\\\frac tb)^{a-1}\\\\exp(-t)d(\\\\frac tb) \\\\\\\\&amp;= \\\\frac {1}{\\\\Gamma(a)} \\\\int_0^\\\\infty t^{a-1}\\\\exp(-t)dt \\\\\\\\ &amp;= \\\\frac {1}{\\\\Gamma(a)} \\\\Gamma(a)  \\\\\\\\&amp;= 1 \\\\end{aligned}\\\\]        The Mean and Variance of Gamma distribution\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\lambda] &amp;= \\\\int_0^\\\\infty \\\\lambda  \\\\mathrm{Gam}(\\\\lambda \\\\vert a,b) d\\\\lambda \\\\\\\\&amp;= \\\\int_0^\\\\infty \\\\lambda  \\\\frac {1}{\\\\Gamma(a)} b^a \\\\lambda^{a-1}\\\\exp(-b\\\\lambda) d\\\\lambda  \\\\\\\\&amp;=    \\\\frac {1}{\\\\Gamma(a)} b^a  \\\\int_0^\\\\infty (\\\\frac tb)^{a}\\\\exp(-t)d(\\\\frac tb) \\\\\\\\ &amp;=  \\\\frac {1}{\\\\Gamma(a)} b^a \\\\frac {1}{b^{a+1}} \\\\Gamma(a+1) \\\\\\\\ &amp;= \\\\frac ab  \\\\end{aligned}\\\\]\\\\[\\\\begin{aligned}  \\\\mathrm{var}[\\\\lambda] &amp;=  \\\\int_0^\\\\infty (\\\\lambda - \\\\frac ab)^2  \\\\mathrm{Gam}(\\\\lambda \\\\vert a,b) d\\\\lambda \\\\\\\\&amp;=  \\\\frac {1}{\\\\Gamma(a)} b^a  \\\\left( \\\\frac {1}{b^{a+2}} \\\\Gamma(a+2) - \\\\frac {a^2}{b^2}   \\\\right) \\\\\\\\ &amp;= \\\\frac {a} {b^2}   \\\\end{aligned}\\\\]        Gamma distribution can be parameterized with just one parameter                  Let $\\\\lambda = \\\\frac xb$, we can obtain:\\\\[\\\\begin{aligned}  \\\\mathrm{Gam}(\\\\frac xb \\\\vert a,b)  &amp;=  \\\\frac {1}{\\\\Gamma(a)} b^a (\\\\frac xb)^{a-1}\\\\exp(-b\\\\frac xb)  \\\\\\\\&amp;=  b \\\\frac {1}{\\\\Gamma(a)} x^{a-1}\\\\exp(-x)  \\\\end{aligned}\\\\]                              Define $ \\\\tilde{\\\\mathrm{Gam}}(x \\\\vert a) = \\\\frac {1}{\\\\Gamma(a)} x^{a-1}\\\\exp(-x)$, then we have  \\\\(\\\\begin{aligned} \\\\tilde{\\\\mathrm{Gam}}(x \\\\vert a) = \\\\frac 1b \\\\mathrm{Gam}(\\\\frac xb \\\\vert a,b)  \\\\end{aligned}\\\\)                          equally, we have:  \\\\(\\\\begin{aligned} \\\\mathrm{Gam}(x \\\\vert a,b) = b\\\\tilde{\\\\mathrm{Gam}}(bx \\\\vert a)  \\\\end{aligned}\\\\)                                            Then we can see, the one parameter form $\\\\tilde{\\\\mathrm{Gam}}(x \\\\vert a)$  is a scaled verison of the two parameter form $\\\\mathrm{Gam}(\\\\lambda \\\\vert a,b)$ with a scaler $\\\\frac 1b$                              # Gamma Distributionsimport numpy as npimport matplotlib.pyplot as pltfrom scipy.special import gamma as gam_fnfrom scipy.stats import gammadef gamma1(x, a, b):    &quot;&quot;&quot;Gamma distribution:  parameterized with two parameter&quot;&quot;&quot;    return 1/gam_fn(a) * b**a * x**(a-1) *np.exp(-b*x)def gamma2(x,a):    &quot;&quot;&quot;Gamma distribution:  parameterized with one parameter&quot;&quot;&quot;    return 1/gam_fn(a) * x**(a-1) *np.exp(-x)parameters = [(0.1,0.1), (1,1), (4,6),  (10,15)]fig, axs = plt.subplots(1,4, figsize=(24,6))for i in range(4):    a, b = parameters[i]    ax = axs[i]        x = np.linspace(gamma.ppf(0.0001, a), gamma.ppf(0.9999, a), 10000)/b#     x = np.linspace(0.01, 1.99, 100)#     ax.plot(x, gamma.pdf(x, a, scale=1/b), &#39;r&#39;, lw=3, alpha=0.8)#     ax.plot(x, gamma1(x,a,b), &#39;r&#39;, lw=3, alpha=0.8)    ax.plot(x, gamma2(b*x,a)*b, &#39;r&#39;, lw=3, alpha=0.8)    ax.set_xlim((0,2))    ax.set_ylim((0,2))    ax.set_title(&quot;a={}, b={}&quot;.format(a,b))    ax.set_xlabel(&quot;$\\\\lambda$&quot;)plt.show()"
  },
  
  {
    "title": "Dirichlet Distribution",
    "url": "/posts/dirichlet_distribution/",
    "categories": "Probability, Distribution",
    "tags": "Dirichlet Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Dirichlet Distribution      we now introduce a prior distribution  for the parameters $\\\\boldsymbol{\\\\mu} = {\\\\mu_1, \\\\mu_2, \\\\cdots, \\\\mu_K}$ of the multinomial distribution.        By inspection of the form of the multinomial distribution, we see that the conjugate prior is given by\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) \\\\propto \\\\prod_{k=1}^K \\\\mu_{k}^{\\\\alpha_k-1} \\\\end{aligned}\\\\]          Because of the summation constraint $\\\\sum_{k}\\\\mu_k = 1$, the distribution over the space of the ${\\\\mu_1, \\\\mu_2, \\\\cdots, \\\\mu_K}$ is confined to a simplex of dimensionality $K-1$.            The normalized form fot this distribution is by\\\\[\\\\begin{aligned} \\\\mathrm{Dir}(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) = \\\\frac {\\\\Gamma(\\\\sum_k \\\\alpha_k)}{\\\\prod_k \\\\Gamma(\\\\alpha_k)} \\\\prod_{k=1}^K \\\\mu_{k}^{\\\\alpha_k-1} \\\\end{aligned}\\\\]                  so that:\\\\[\\\\begin{aligned} \\\\int_0^1 \\\\mathrm{Dir}(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) d\\\\boldsymbol{\\\\mu} = 1 \\\\end{aligned}\\\\]                  Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\mu_i] = \\\\frac {\\\\alpha_i}{\\\\sum_k \\\\alpha_k}  \\\\end{aligned}\\\\]        Variance    \\\\(\\\\begin{aligned} \\\\mathrm{var}[\\\\mu_i] &amp;= \\\\frac {\\\\alpha_i((\\\\sum_k \\\\alpha^k) - \\\\alpha_i)}{((\\\\sum_k \\\\alpha^k)+1)\\\\left(\\\\sum_k \\\\alpha^k\\\\right)^2} \\\\\\\\ &amp;= \\\\frac {\\\\alpha_i(\\\\alpha_0 - \\\\alpha_i)}{\\\\alpha_0^2(\\\\alpha_0 + 1)} \\\\end{aligned}\\\\)          Here we have let $\\\\alpha_0 = \\\\sum_k \\\\alpha_k$            Covariance          $\\\\forall \\\\space i\\\\neq j$    \\\\[\\\\begin{aligned} \\\\mathrm{Cov}[\\\\mu_i, \\\\mu_j] &amp;= \\\\mathbb{E}[(\\\\mu_i - \\\\frac {\\\\alpha_i}{\\\\alpha_0})(\\\\mu_j - \\\\frac {\\\\alpha_j}{\\\\alpha_0})] \\\\\\\\ &amp;= \\\\frac {\\\\alpha_i\\\\alpha_j}{\\\\alpha_0(\\\\alpha_0+1) - \\\\frac {\\\\alpha_i\\\\alpha_j}{\\\\alpha_0^2}} \\\\\\\\ &amp;=  \\\\frac {-\\\\alpha_i\\\\alpha_j}{\\\\alpha_0^2(\\\\alpha_0+1)} \\\\end{aligned}\\\\]        The Posterior Distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu} \\\\vert D, \\\\boldsymbol{\\\\alpha}) \\\\propto p(D\\\\vert \\\\boldsymbol{\\\\mu}) p(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) \\\\propto \\\\prod_{k=1}^K \\\\mu_k^{m_k+\\\\alpha_k-1}  \\\\end{aligned}\\\\]                  Then we see the posterior distribution again takes the form of dirichlet distribution, so it is indeed a conjugate prior for the multinomial.\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu} \\\\vert D, \\\\boldsymbol{\\\\alpha}) &amp;= \\\\mathrm{Dir} (\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha} + \\\\boldsymbol{m}) \\\\\\\\ &amp;= \\\\frac {\\\\Gamma(\\\\sum_k (\\\\alpha_k +m_k))}{\\\\prod_k \\\\Gamma(\\\\alpha_k+m_k)} \\\\prod_{k=1}^K \\\\mu_k^{m_k+\\\\alpha_k-1} \\\\\\\\&amp;= \\\\frac {\\\\Gamma(N + \\\\sum_k \\\\alpha_k )}{\\\\prod_k \\\\Gamma(\\\\alpha_k+m_k)} \\\\prod_{k=1}^K \\\\mu_k^{m_k+\\\\alpha_k-1} \\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "Beta Distribution",
    "url": "/posts/beta_distribution/",
    "categories": "Probability, Distribution",
    "tags": "Beta Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Beta Distribution      New we introduce a prior distribution of $p(\\\\mu)$ over the parameter $\\\\mu$ in the Bernouli and Binomial distribution        The beta distribution as the prior distribution will have a simple interpretation as well as some useful analytical properties.        conjugacy: In this case, if we let the posterior distribution is priportional to the product of the prior and the likelihood function, then it will have the same functional form as the prior. Then we will see the posterior distribution is also a beta distribution. This property is called conjugacy.    \\\\(\\\\begin{aligned} \\\\mathrm{Beta}(\\\\mu \\\\vert a, b) = \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1} \\\\end{aligned}\\\\)                  where $\\\\Gamma(x)$ is the Gamma function defined by:\\\\[\\\\begin{aligned}\\\\Gamma(x) = \\\\int_0^{\\\\infty} u^{x-1}e^{-u}du \\\\end{aligned}\\\\]                              $\\\\Gamma(x+1) = x\\\\Gamma(x)$, and then $\\\\Gamma(x+1) = x!$ when $x$ is an integer.            $$\\\\begin{aligned} \\\\Gamma(x+1) &amp;= \\\\int_0^{\\\\infty} u^{x}e^{-u}du \\\\&amp;= \\\\int_0^{\\\\infty} -u^{x}d(e^{-u}) \\\\&amp;=-u^{x}e^{-u} - \\\\int_0^\\\\infty x u^{x-1} e^{-u} du \\\\qquad\\\\qquad (using: \\\\space \\\\int u(x)d[v(x)] = u(x)v(x)                          \\\\int v(x)d[u(x)]) \\\\&amp;= -u^{x}e^{-u} + x\\\\Gamma(x)  \\\\end{aligned}$$                              according $u \\\\in [0,+\\\\infty)$, if $u=0$, $-u^x e^{-u}=0$; and if $u\\\\rightarrow +\\\\infty$:\\\\[\\\\lim_{u\\\\rightarrow +\\\\infty} -\\\\frac {u^x}{e^u} = \\\\lim_{u\\\\rightarrow +\\\\infty} -\\\\frac {x!}{e^u} = 0 \\\\qquad\\\\qquad (using:\\\\space \\\\mathrm{L&#39;Hôpital&#39;s \\\\space rule})\\\\]                            then we have $\\\\Gamma(x+1) = x\\\\Gamma(x)$.                                                  The coefficient ensures that the beta distribution is normalized.\\\\[\\\\begin{aligned} \\\\int_0^1 \\\\mathrm{Beta}(\\\\mu \\\\vert a, b) du = 1 \\\\end{aligned}\\\\]                  Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\mu] &amp;= \\\\int_0^1 \\\\mu \\\\mathrm{Beta}(\\\\mu \\\\vert a, b) d\\\\mu \\\\\\\\&amp;= \\\\int_0^1 \\\\mu \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1}d\\\\mu \\\\\\\\&amp;= \\\\frac {a\\\\Gamma(a+1+b)}{(a+b)\\\\Gamma(a+1)\\\\Gamma(b)} \\\\int_0^1  \\\\mu^{a}(1-\\\\mu)^{b-1}d\\\\mu \\\\\\\\ &amp;= \\\\frac {a}{a+b} \\\\int_0^1 \\\\frac {\\\\Gamma(c+b)}{\\\\Gamma(c)\\\\Gamma(b)} \\\\mu^{c-1}(1-\\\\mu)^{b-1}d\\\\mu \\\\\\\\ &amp;= \\\\frac {a}{a+b}\\\\end{aligned}\\\\]        Variance\\\\[\\\\begin{aligned} \\\\mathrm{var}[\\\\mu]  &amp;= \\\\mathbb{E}[(\\\\mu - \\\\frac {a}{a+b})^2] \\\\\\\\&amp;= \\\\int_0^1 (\\\\mu - \\\\frac {a}{a+b})^2 \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1}  d\\\\mu \\\\\\\\&amp;= \\\\frac {a}{a+b} \\\\left(\\\\int_0^1 \\\\mu \\\\frac {\\\\Gamma(a+b+1)}{\\\\Gamma(a+1)\\\\Gamma(b)} \\\\mu^{a}(1-\\\\mu)^{b-1}\\\\right) - 2\\\\frac {a}{a+b} * \\\\frac {a}{a+b} + \\\\left(\\\\frac {a}{a+b}\\\\right)^2 \\\\\\\\&amp;= \\\\frac {a}{a+b} \\\\frac {a+1}{a+b+1} - \\\\left(\\\\frac {a}{a+b}\\\\right)^2 \\\\\\\\ &amp;= \\\\frac {ab}{(a+b)^2(a+b+1)} \\\\end{aligned}\\\\]        The Posterior Distribution\\\\[\\\\begin{aligned} p(\\\\mu \\\\vert D) &amp;\\\\propto p(D\\\\vert \\\\mu)p(\\\\mu) \\\\\\\\ p(\\\\mu \\\\vert m, N, a, b) &amp;\\\\propto \\\\mu^{m+a-1}(1-\\\\mu)^{N-m+b-1} \\\\end{aligned}\\\\]                  Then we have:\\\\[\\\\begin{aligned} p(\\\\mu \\\\vert m, N, a, b) = \\\\frac {\\\\Gamma(N+a+b)}{\\\\Gamma(m+a)\\\\Gamma(N-m+b)} \\\\mu^{m+a-1}(1-\\\\mu)^{N-m+b-1} \\\\end{aligned}\\\\]                  The Predictive Distribution\\\\[\\\\begin{aligned} p(x\\\\vert D) &amp;= \\\\int_0^1 p(x\\\\vert \\\\mu) p(\\\\mu\\\\vert D)d\\\\mu \\\\\\\\ &amp;=\\\\left\\\\{\\\\begin{aligned}&amp;\\\\int_0^1 \\\\mu p(\\\\mu\\\\vert D)d\\\\mu \\\\qquad \\\\qquad &amp;x=1 \\\\\\\\ &amp; \\\\int_0^1 (1-\\\\mu) p(\\\\mu\\\\vert D)d\\\\mu \\\\qquad \\\\qquad &amp;x=0 \\\\end{aligned} \\\\right. \\\\\\\\ &amp;= \\\\left\\\\{\\\\begin{aligned}&amp;\\\\mathbb{E}_{\\\\mu}[\\\\mu \\\\vert D] \\\\qquad \\\\qquad &amp;x=1 \\\\\\\\ &amp; 1-\\\\mathbb{E}_{\\\\mu}[\\\\mu \\\\vert D] \\\\qquad \\\\qquad &amp;x=0 \\\\end{aligned} \\\\right.  \\\\end{aligned}\\\\]                  Using the posterior distribution result, we obtain:\\\\[\\\\begin{aligned} p(x=1 \\\\vert D) &amp;= \\\\int_0^1 \\\\mu \\\\frac {\\\\Gamma(N+a+b)}{\\\\Gamma(m+a)\\\\Gamma(N-m+b)} \\\\mu^{m+a-1}(1-\\\\mu)^{N-m+b-1} d\\\\mu &amp;= \\\\frac {m+a}{N+a+b} \\\\\\\\ p(x=0\\\\vert D) &amp;= 1 - p(x=1 \\\\vert D) &amp;= \\\\frac {N-m+b}{N+a+b}  \\\\end{aligned}\\\\]                    Then we can see, in the limit of an infinitely large data set $m,N\\\\rightarrow \\\\infty$, this result reduces to the maximum likelihood result $\\\\frac mN$.                    It is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set.                    For a finite data set, the posterior distribution mean for $\\\\mu$ always lie between prior mean and the maximum likelihood estimate for $\\\\mu$ corresponding to the relative frequencies of events.            # Beta Distributionsimport numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import betaparameters = [(0.1,0.3), (1,1), (8,4), (200,150)]fig, axs = plt.subplots(1,4, figsize=(24,6))for i in range(4):    a, b = parameters[i]    ax = axs[i]        # mu = np.linspace(beta.ppf(0, a, b), beta.ppf(1, a, b), 100)    mu = np.linspace(0,1, 100)    ax.plot(mu, beta.pdf(mu, a, b),           &#39;r&#39;, lw=3, alpha=0.8, label=&#39;beta pdf&#39;)    ax.set_xlim((0,1))    ax.set_title(&quot;a={}, b={}&quot;.format(a,b))    ax.set_xlabel(&quot;$\\\\mu$&quot;)plt.show()      We can see that as $a\\\\rightarrow \\\\infty$, $b\\\\rightarrow \\\\infty$, the beta distribution becomes more sharply peaked, because the variance goes to zero for $a\\\\rightarrow \\\\infty$ or $b\\\\rightarrow \\\\infty$.        Also in the posterior distribution of $\\\\mu$, as the number of observations increases, the posterior distribution becomes more sharply peaked. And the variance of posterior distribution decreases, that means the uncertainty represented by the posterior distribution will also decrease.        But is that a general property of Bayesian learning, we can see in the following:                  To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold.            Consider a gengeral Bayesian inference problem for a parameter $\\\\theta$ for which we have observed a data set $D$, described by the joint distribution $p(\\\\theta, D)$.              Firstly: we can see that the posterior mean of $\\\\theta$, averaged over the distribution generation the data, is equal to the prior mean of $\\\\theta$.\\\\[\\\\begin{aligned} \\\\mathbb{E}_\\\\theta[\\\\theta] &amp;= \\\\mathbb{E}_D[\\\\mathbb{E}_\\\\theta[\\\\theta\\\\vert D]] \\\\\\\\ Proof: \\\\qquad&amp;  \\\\\\\\ \\\\mathbb{E}_D[\\\\mathbb{E}_\\\\theta[\\\\theta\\\\vert D]] &amp;= \\\\int \\\\left[ \\\\int \\\\theta p(\\\\theta\\\\vert D)d\\\\theta \\\\right]p(D) dD \\\\\\\\ &amp;= \\\\int \\\\theta p(\\\\theta) d\\\\theta \\\\\\\\ &amp;= \\\\mathbb{E}_\\\\theta[\\\\theta] \\\\end{aligned}\\\\]                    And then, we can see the prior variance of $\\\\theta$ is equal to the average posterior variance of $\\\\theta$ plus to the variance of posterior mean of $\\\\theta$.\\\\[\\\\begin{aligned} \\\\mathrm{var}_{\\\\theta}[\\\\theta] &amp;= \\\\mathbb{E}_{D}[\\\\mathrm{var}_\\\\theta[\\\\theta\\\\vert D]] + \\\\mathrm{var}_{D}[\\\\mathbb{E}_\\\\theta[\\\\theta\\\\vert D]] \\\\\\\\ &amp;\\\\ge \\\\mathbb{E}_{D}[\\\\mathrm{var}_\\\\theta[\\\\theta\\\\vert D]] \\\\end{aligned}\\\\]                    It means that, on average, the variance of posterior distribution is smaller than the prior variance. And the reduction is greater if the variance in the posterior mean is greater.            Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance.      "
  },
  
  {
    "title": "Bernoulli and Binomial Distribution",
    "url": "/posts/bernoulli_and_binomial_distribution/",
    "categories": "Probability, Distribution",
    "tags": "Bernoulli, Binomial",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Bernoulli Distributions      Bernoulli Distributions is the discrete probability distribution of a random variable which takes the value 1 with probability $\\\\mu$ and the value 0 with probability ${\\\\displaystyle 1-\\\\mu}$        $x\\\\in {0, 1}, \\\\mu \\\\in [0,1]$\\\\[\\\\begin{aligned} \\\\mathrm{Bern}(x\\\\vert \\\\mu) &amp;= \\\\mu^x(1-\\\\mu)^{1-x} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[x] &amp;= \\\\sum_{x} xp(x) = \\\\mu  \\\\\\\\ \\\\mathrm{var}[x] &amp;= \\\\mathbb{E}[(x-\\\\mu)^2] \\\\\\\\ &amp;= \\\\sum_{x} (x-\\\\mu)^2p(x)\\\\\\\\&amp;= \\\\mu^2(1-\\\\mu) + (1-\\\\mu)^2\\\\mu \\\\\\\\&amp;= \\\\mu(1-\\\\mu) \\\\end{aligned}\\\\]        Likelihood Function                  Let $D = {x_1, x_2, \\\\cdots, x_N }$ of observed data sets of $x$, and $m = \\\\sum_{i=1}^N x_i$.\\\\[\\\\begin{aligned} p(D\\\\vert \\\\mu) &amp;= \\\\prod_{i=1}^N \\\\mu^{x_i}(1-\\\\mu)^{1-x_i} \\\\\\\\ &amp;= \\\\mu^m(1-\\\\mu)^{N-m} \\\\end{aligned}\\\\]            Then we can see $m = \\\\sum_{i=1}^N x_i$ is a sufficient statistic for the data under this distribution.              And using maximum likelihood approach, we obtain:\\\\[\\\\mu_{ml} = \\\\frac mN\\\\]            Binomial Distributions      The binomial distribution with parameters $N$ and $\\\\mu$ is the discrete probability distribution of the number of successes $m$ in a sequence of $N$ independent experiments. It is the distribution of the number $m$ of  ovservations of $x=1$ in bernoulli distributions given that the data set has size $N$.\\\\[\\\\begin{aligned} \\\\mathrm{Bin}(m\\\\vert N,\\\\mu) &amp;= \\\\left( \\\\begin{matrix}N\\\\\\\\m\\\\end{matrix} \\\\right) \\\\mu^m(1-\\\\mu)^{N-m} \\\\\\\\ &amp;= \\\\frac {N!}{(N-m)!m!} \\\\mu^m(1-\\\\mu)^{N-m}  \\\\end{aligned}\\\\]                  Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[m] &amp;= \\\\sum_{m=0}^N m\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\&amp;= \\\\sum_{m=1}^N m\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\ &amp;= \\\\sum_{m=1}^N m \\\\frac {N!}{(N-m)!m!} \\\\mu^m(1-\\\\mu)^{N-m} \\\\\\\\ &amp;= N\\\\mu \\\\sum_{m=1}^N \\\\frac {(N-1)!}{((N-1)-(m-1))!(m-1)!} \\\\mu^{m-1}(1-\\\\mu)^{(N-1)-(m-1)} \\\\\\\\ &amp;= N\\\\mu \\\\sum_{k=0}^{N-1} \\\\frac {(N-1)!}{((N-1)-k)!k!} \\\\mu^{k}(1-\\\\mu)^{(N-1)-k} \\\\\\\\ &amp;= N\\\\mu \\\\sum_{k=0}^{N-1} \\\\mathrm{Bin}(k\\\\vert N-1, \\\\mu) \\\\\\\\ &amp;= N\\\\mu  \\\\end{aligned}\\\\]                    Variance\\\\[\\\\begin{aligned} \\\\mathrm{var}[m] &amp;= \\\\mathbb{E}[(m-\\\\mathbb{E}[m])^2] \\\\\\\\ &amp;= \\\\sum_{m=0}^N (m-\\\\mathbb{E}[m])^2 \\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\&amp;= \\\\sum_{m=0}^N (m-N\\\\mu)^2 \\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\ &amp;= \\\\sum_{m=0}^N m^2\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) -2N\\\\mu \\\\sum_{m=0}^N m\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) + N^2\\\\mu^2 \\\\sum_{m=0}^N  \\\\\\\\ &amp;= \\\\left[\\\\sum_{m=1}^N m^2\\\\frac {N!}{(N-m)!m!} \\\\mu^m(1-\\\\mu)^{N-m} \\\\right]  -2N\\\\mu N\\\\mu + N^2 \\\\mu^2 \\\\\\\\ &amp;= -N^2\\\\mu^2 + N\\\\mu \\\\sum_{m=1}^N m \\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\\\\\&amp;= -N^2\\\\mu^2 + N\\\\mu \\\\sum_{m=1}^N (1+m-1) \\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\\\\\ &amp;=  -N^2\\\\mu^2 + N\\\\mu\\\\left( \\\\sum_{m=1}^N\\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} +  \\\\sum_{m=1}^N (m-1) \\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\right) \\\\\\\\ &amp;= -N^2\\\\mu^2 + N\\\\mu\\\\left(1+\\\\sum_{m=2}^N \\\\frac {(N-1)!}{(N-m)!(m-2)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\right)  \\\\\\\\ &amp;= -N^2\\\\mu^2 + N\\\\mu \\\\left(1+(N-1)\\\\mu \\\\sum_{m=2}^N \\\\frac {(N-2)!}{((N-2)-(m-2))!(m-2)!}\\\\mu^{m-2}(1-\\\\mu)^{(N-2)-(m-2)} \\\\right) \\\\\\\\&amp;= -N^2\\\\mu^2 + N\\\\mu\\\\left(1+ (N-1)\\\\mu\\\\sum_{k=0}^{N-2} \\\\mathrm{Bin}(k\\\\vert N-2, \\\\mu) \\\\right) \\\\\\\\&amp;= -N^2\\\\mu^2 + N\\\\mu(1+N\\\\mu-\\\\mu) \\\\\\\\&amp;= N\\\\mu(1-\\\\mu)\\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "A Quick Look of Discrete Distribution",
    "url": "/posts/quick_look_discrete_distributions/",
    "categories": "Probability, Distribution",
    "tags": "Bernoulli, Binomial, Multinomial",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Bernoulli distribution      $x\\\\sim Bern(x\\\\vert \\\\mu)$\\\\[\\\\begin{aligned}  p(x) = Bern(x\\\\vert \\\\mu) &amp;= \\\\mu^x(1-\\\\mu)^{1-x} \\\\\\\\ &amp;= \\\\mu^{\\\\mathbb{I}(x=1)}(1-\\\\mu)^{\\\\mathbb{I}(x=0)} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[x] &amp;= 1*\\\\mu + 0*(1-\\\\mu) = \\\\mu \\\\\\\\ var[x] &amp;= \\\\mathbb{E}[x^2]-\\\\mathbb{E}[x]^2 = \\\\mu-\\\\mu^2 = \\\\mu(1-\\\\mu) \\\\end{aligned}\\\\]        Likelihood Function:\\\\[\\\\begin{aligned} L(D) = p(D\\\\vert \\\\mu) &amp;= \\\\prod_{n=1}^N p(x_n) \\\\\\\\ &amp;= \\\\prod_{n=1}^N \\\\mu^{x_n}(1-\\\\mu)^{x_n} \\\\end{aligned}\\\\]        Log-Likelihood\\\\[\\\\begin{aligned} LL(D) = \\\\ln p(D\\\\vert \\\\mu)  &amp;= \\\\sum_{n=1}^N \\\\ln p(x_n) \\\\\\\\ &amp;= \\\\sum_{n=1}^N \\\\{x_n\\\\ln\\\\mu+ (1-x_n)\\\\ln(1-\\\\mu)\\\\} \\\\end{aligned}\\\\]  Binomial distribution      $x \\\\sim Bin(x\\\\vert n, \\\\mu)$  \\\\(\\\\begin{aligned} p(x=k) = Bin(k \\\\vert n, \\\\mu) &amp;= \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{n-k} \\\\\\\\ \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) &amp;\\\\equiv \\\\frac {n!}{(n-k)!k!} \\\\end{aligned}\\\\)        Expection\\\\[\\\\begin{aligned} \\\\mathbb{E}[x] &amp;= \\\\sum_{k=0}^n k \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{n-k} \\\\\\\\ &amp;= \\\\sum_{k=1}^n k \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{(n-1)-(k-1)} \\\\\\\\ &amp;= n\\\\mu \\\\sum_{k=1}^n \\\\left(\\\\begin{matrix} n-1 \\\\\\\\ k-1 \\\\end{matrix}\\\\right) \\\\mu^{k-1}(1-\\\\mu)^{(n-1)-(k-1)} \\\\\\\\ &amp;= n\\\\mu(\\\\mu + 1-\\\\mu)^{n-1} \\\\\\\\ &amp;= n\\\\mu \\\\end{aligned}\\\\]        Variance\\\\[\\\\begin{aligned} var[x] &amp;= \\\\mathbb{E}[x^2] - \\\\mathbb{E}[x]^2 \\\\\\\\ &amp;= \\\\sum_{k=0}^n k^2 \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{n-k} - n^2\\\\mu^2 \\\\\\\\&amp;= \\\\sum_{k=0}^n (k(k-1)+k) \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{n-k} - n^2\\\\mu^2 \\\\\\\\ &amp;= \\\\sum_{k=2}^n k(k-1) \\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{n-k} + \\\\sum_{k=1}^n k\\\\left(\\\\begin{matrix} n \\\\\\\\ k \\\\end{matrix}\\\\right) \\\\mu^k(1-\\\\mu)^{n-k} - n^2\\\\mu^2  \\\\\\\\ &amp;= n(n-1)\\\\mu^2\\\\sum_{k=2}^n \\\\left(\\\\begin{matrix} n-2 \\\\\\\\ k-2 \\\\end{matrix}\\\\right) \\\\mu^{k-2}(1-\\\\mu)^{(n-2)-(k-2)} + n\\\\mu - n^2\\\\mu^2 \\\\\\\\&amp;= n\\\\mu - n\\\\mu^2 \\\\\\\\&amp;= n\\\\mu(1-\\\\mu) \\\\end{aligned}\\\\]  Categorical distribution (Multinoulli distributions)      $\\\\mathcal{X}={1,2,…,K }, x\\\\in X$        $\\\\boldsymbol{x} \\\\sim Cate(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu})$, and where $\\\\boldsymbol{\\\\mu}: {\\\\mu_1, \\\\mu_2,…,\\\\mu_K }$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) = Cate(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}) &amp;= \\\\prod_{k}^K \\\\mu_k^{x_k} \\\\end{aligned}\\\\]        If using one-hot encoding of $k$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) = Cate(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}) &amp;= \\\\sum_{k=1}^K \\\\mu_k^\\\\mathbb{I}(x_k=1) \\\\\\\\ \\\\sum_{k=1}^k \\\\mu_k &amp;= 1 \\\\end{aligned}\\\\]        Expectation: $\\\\mu_k$\\\\[\\\\begin{aligned}  \\\\mathbb{E}[x_k] &amp;= \\\\sum_k^K x_k\\\\prod_k^K \\\\mu_k^{x_k} \\\\\\\\&amp;= \\\\mu_k \\\\\\\\ \\\\mathbb{E}[\\\\boldsymbol{x}] &amp;= \\\\sum^K \\\\boldsymbol{x}\\\\prod_k^K \\\\mu_k^{x_k} \\\\\\\\ &amp;= \\\\boldsymbol{\\\\mu} \\\\end{aligned}\\\\]        Variance:  $\\\\mu_k(1-\\\\mu_k)$\\\\[\\\\begin{aligned} var[x_k] &amp;= \\\\mathbb{E}[x_k^2] - \\\\mathbb{E}[x_k]^2 \\\\\\\\ &amp;= \\\\mu_k - \\\\mu_k^2 \\\\\\\\&amp;= \\\\mu_k(1-\\\\mu_k) \\\\end{aligned}\\\\]        Covariance: $-\\\\mu_j\\\\mu_k$\\\\[\\\\begin{aligned} cov[x_j, x_k] &amp;= \\\\mathbb{E}[x_jx_k] - \\\\mathbb{E}[x_j]\\\\mathbb{E}[x_k] \\\\\\\\ &amp;= \\\\mathbb{E}[0]-\\\\mu_j\\\\mu_k \\\\\\\\&amp;= -\\\\mu_j\\\\mu_k\\\\end{aligned}\\\\]  Multinomial distribution      $\\\\mathcal{X}={0,1,2,…,n }, x_i\\\\in X, \\\\boldsymbol{x}=[x_1, x_2,…,x_K]$        $\\\\boldsymbol{x} \\\\backsim Mult(\\\\boldsymbol{x}\\\\vert n, \\\\boldsymbol{\\\\mu})$, and where $\\\\boldsymbol{\\\\mu}: {\\\\mu_1, \\\\mu_2,…,\\\\mu_K }$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) = Mult(\\\\boldsymbol{x}\\\\vert n, \\\\boldsymbol{\\\\mu}) &amp;=  \\\\left(\\\\begin{matrix} n \\\\\\\\ x_1,x_2,...,x_K \\\\end{matrix}\\\\right) \\\\prod_{k=1}^K\\\\mu_k^{x_k} \\\\\\\\ \\\\left(\\\\begin{matrix} n \\\\\\\\ x_1,x_2,...,x_K \\\\end{matrix}\\\\right) &amp;\\\\equiv \\\\frac {n!}{x_1!x_2!···x_K!} \\\\end{aligned}\\\\]    Expectation: $n\\\\mu_k$                  the marginonal distribution of $\\\\boldsymbol{x}$ is binomial distribution, then\\\\[\\\\mathbb{E}[x_k] = n\\\\mu_k\\\\]                    Proof:\\\\[\\\\begin{aligned} p(x_k) = \\\\sum_{j\\\\neq k}^K p(x_1, x_2,\\\\cdots,x_K) &amp;= \\\\binom{n}{x_k} \\\\mu_k^{x_k} \\\\binom{n-x_k}{x_{j\\\\neq k}}\\\\prod_{j\\\\neq k} \\\\mu_j^{x^j} \\\\end{aligned}\\\\]                              The second term:  \\\\(\\\\binom{n-x_k}{x_{j\\\\neq k}}\\\\prod_{j\\\\neq k} \\\\mu_j^{x^j}\\\\)                                It can be seen as the probability that none of the $n-x_k$ trials is state $k$, so we have:\\\\[(1-\\\\mu_k)^{n-x_k} = \\\\binom{n-x_k}{x_{j\\\\neq k}}\\\\prod_{j\\\\neq k} \\\\mu_j^{x^j}\\\\]                                then:  \\\\(p(x_k) = \\\\binom{n}{x_k} \\\\mu_k^{x_k} (1-\\\\mu_k)^{n-x_k}\\\\)                                      Vector form:  \\\\(\\\\mathbb{E}[\\\\boldsymbol{x}] = n\\\\boldsymbol{\\\\mu}\\\\)                  Variance: $n\\\\mu_k(1-\\\\mu_k)$\\\\[\\\\text{var}[x_k] = n\\\\mu_k(1-\\\\mu_k)\\\\]        Covariance: $-n\\\\mu_j\\\\mu_k$          Proof:                              $\\\\mathbb{E}[x_j\\\\vert x_k]$:\\\\[\\\\begin{aligned} p(x_j\\\\vert x_k) &amp;= \\\\binom{n-x_k}{x_1,\\\\cdots,x_{k-1}, x_{k+1},\\\\cdots,x_K} \\\\prod_{j\\\\neq k}^K \\\\left(\\\\frac {\\\\mu_j}{1-\\\\mu_k}\\\\right)^{x_j} = \\\\text{Mult}(\\\\boldsymbol{x_{j\\\\neq k}}\\\\vert n-x_k, \\\\mu_{j\\\\neq k}/(1-\\\\mu_k)) \\\\\\\\ \\\\mathbb{E}[x_j\\\\vert x_k] &amp;= (n-x_k)\\\\frac {\\\\mu_j}{1-\\\\mu_k} \\\\end{aligned}\\\\]                                $\\\\mathbb{E}[x_jx_k]$\\\\[\\\\begin{aligned} \\\\mathbb{E}[x_jx_k] = \\\\mathbb{E}[x_k \\\\mathbb{E}[x_j\\\\vert x_k]] &amp;= n\\\\frac {\\\\mu_j}{1-\\\\mu_k} \\\\mathbb{E}[x_k] - \\\\frac {\\\\mu_j}{1-\\\\mu_k} \\\\mathbb{E}[x_k^2] \\\\\\\\&amp;= n^2\\\\frac {\\\\mu_j\\\\mu_k}{1-\\\\mu_k} - \\\\frac {\\\\mu_j}{1-\\\\mu_k} (n^2\\\\mu_k^2+n\\\\mu_k(1-\\\\mu_k)) \\\\\\\\&amp;= n(n-1)\\\\mu_j\\\\mu_k  \\\\end{aligned}\\\\]                                $\\\\text{cov}[x_j,x_k]$\\\\[\\\\begin{aligned} \\\\text{cov}[x_j,x_k] &amp;= \\\\mathbb{E}[x_jx_k] - \\\\mathbb{E}[x_j] \\\\mathbb{E}[x_k] \\\\\\\\&amp;= n(n-1)\\\\mu_j\\\\mu_k - n^2 \\\\mu_j\\\\mu_k \\\\\\\\&amp;= -n\\\\mu_j\\\\mu_k \\\\end{aligned}\\\\]                                    Variance-Covariance Matrix:\\\\[\\\\text{var}[\\\\boldsymbol{x}] = n\\\\{\\\\text{diag}(\\\\boldsymbol{\\\\mu}) - \\\\boldsymbol{\\\\mu}\\\\boldsymbol{\\\\mu}^T\\\\}\\\\]  Poission Distribution      $x\\\\sim \\\\text{Poi}(x\\\\vert \\\\lambda)$, $x\\\\in \\\\mathbb{Z}, \\\\lambda &gt; 0$\\\\[\\\\begin{aligned} p(x) = \\\\text{Poi}(x\\\\vert \\\\lambda) = e^{-\\\\lambda} \\\\frac{\\\\lambda^x}{x!} \\\\end{aligned}\\\\]    The Poisson distribution can be applied to systems with a large number of possible events, each of which is rare. The number of such events that occur during a fixed time interval is, under the right circumstances, a random number with a Poisson distribution          The number of meteorites greater than 1 meter diameter that strike Earth in a year      The number of patients arriving in an emergency room between 10 and 11 pm      The number of laser photons hitting a detector in a particular time interval        The Poisson distribution is an appropriate model if the following assumptions are true          $x$ is the number of times an event occurs in an interval and $x$ can take values 0, 1, 2, ….      The occurrence of one event does not affect the probability that a second event will occur. That is, events occur independently.      The average rate at which events occur is independent of any occurrences. For simplicity, this is usually assumed to be constant, but may in practice vary with time.      Two events cannot occur at exactly the same instant; instead, at each very small sub-interval, either exactly one event occurs, or no event occurs.        If these conditions are true, then k is a Poisson random variable, and the distribution of k is a Poisson distribution.    The Poisson distribution is also the limit of a binomial distribution, for which the probability of success for each trial equals λ divided by the number of trials, as the number of trials approaches infinity    See more - Wikipedia - Poission Distribution"
  },
  
  {
    "title": "A Quick Look of Continuous Distribution",
    "url": "/posts/quick_look_continuous_distributions/",
    "categories": "Probability, Distribution",
    "tags": "Gaussian, Gamma, Beta, Dirichlet",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Uniform distribution      $x\\\\sim \\\\mathcal{U}(x\\\\vert a,b)$, $x\\\\in [a,b]$\\\\[\\\\begin{aligned} p(x) = \\\\mathcal{U}(x\\\\vert a,b) = \\\\frac {1}{b-a} \\\\end{aligned}\\\\]  Univariate Gaussian distribution      $x\\\\sim \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2)$\\\\[\\\\begin{aligned} p(x) = \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2) = \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}}\\\\exp(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}) \\\\end{aligned}\\\\]    Expectation: $\\\\mu$                  Proof\\\\[\\\\begin{aligned} \\\\mathbb{E}[x] &amp;= \\\\int_{-\\\\infty}^{+\\\\infty}xp(x) \\\\\\\\&amp;= \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\int_{-\\\\infty}^{+\\\\infty}x\\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right)dx \\\\\\\\&amp;= \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\int_{-\\\\infty}^{+\\\\infty}(y+\\\\mu)\\\\exp\\\\left(-\\\\frac{y^2}{2\\\\sigma^2}\\\\right)dy \\\\qquad \\\\text{let \\\\space} y=x-\\\\mu \\\\\\\\&amp;= \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\int_{-\\\\infty}^{+\\\\infty}y\\\\exp\\\\left(-\\\\frac{y^2}{2\\\\sigma^2}\\\\right)dy + \\\\frac {\\\\mu}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\int_{-\\\\infty}^{+\\\\infty}\\\\exp\\\\left(-\\\\frac{y^2}{2\\\\sigma^2}\\\\right)dy \\\\end{aligned}\\\\]                              The first term:\\\\[\\\\begin{aligned} \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\int_{-\\\\infty}^{+\\\\infty}y\\\\exp\\\\left(-\\\\frac{y^2}{2\\\\sigma^2}\\\\right)dy &amp;= 0 \\\\qquad \\\\text{odd \\\\space function} \\\\end{aligned}\\\\]                                The second term:\\\\[\\\\begin{aligned} \\\\text{let \\\\space} I &amp;= \\\\int_{-\\\\infty}^{+\\\\infty}\\\\exp\\\\left(-\\\\frac{y^2}{2\\\\sigma^2}\\\\right)dy \\\\\\\\ \\\\text{then \\\\space} I^2 &amp;= \\\\int_{-\\\\infty}^{+\\\\infty}\\\\int_{-\\\\infty}^{+\\\\infty} \\\\exp\\\\left(-\\\\frac{y_1^2}{2\\\\sigma^2}\\\\right)\\\\exp\\\\left(-\\\\frac{y_2^2}{2\\\\sigma^2}\\\\right)dy_1dy_2 \\\\\\\\ &amp;= \\\\int_{-\\\\infty}^{+\\\\infty}\\\\int_{-\\\\infty}^{+\\\\infty} \\\\exp\\\\left(-\\\\frac{y_1^2+y_2^2}{2\\\\sigma^2}\\\\right)dy_1dy_2 \\\\\\\\ \\\\text{let \\\\space} y_1 &amp;= r\\\\cos(\\\\theta) \\\\\\\\ y_2 &amp;= r\\\\sin(\\\\theta) \\\\\\\\ I^2 &amp;= \\\\int_0^{2\\\\pi}\\\\int_0^{+\\\\infty}\\\\exp\\\\left(-\\\\frac{r^2}{2\\\\sigma^2}\\\\right)r dr d\\\\theta \\\\qquad \\\\qquad  \\\\left\\\\vert \\\\frac{\\\\partial(x,y)}{\\\\partial(r,\\\\theta)}\\\\right\\\\vert = r \\\\\\\\&amp;= 2\\\\pi \\\\int_0^{+\\\\infty}\\\\exp\\\\left(-\\\\frac{r^2}{2\\\\sigma^2}\\\\right)r dr \\\\\\\\&amp;= 2\\\\pi\\\\sigma^2\\\\int_0^{+\\\\infty} \\\\exp(-t)dt \\\\qquad \\\\qquad t= \\\\frac {r^2}{2\\\\sigma^2}、dt=\\\\frac{r}{\\\\sigma^2}dr \\\\\\\\ &amp;= 2\\\\pi\\\\sigma^2 \\\\end{aligned}\\\\]                                Then:\\\\[\\\\begin{aligned} \\\\mathbb{E}[x] = \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}}(0 + \\\\mu I) = \\\\mu \\\\end{aligned}\\\\]                                    Variance: $\\\\sigma^2$\\\\[\\\\begin{aligned} \\\\text{var}[x] &amp;= \\\\int_{-\\\\infty}^{+\\\\infty}(x-\\\\mu)^2\\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}}\\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right) \\\\\\\\ &amp;= \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\int_{-\\\\infty}^{+\\\\infty} y^2\\\\exp\\\\left(-\\\\frac {y^2}{2\\\\sigma^2}\\\\right)dy \\\\\\\\ &amp;= \\\\frac {1}{\\\\sqrt{2\\\\pi\\\\sigma^2}}\\\\int_0^{+\\\\infty} 2\\\\sigma^2t \\\\exp(-t)\\\\frac{\\\\sqrt{2\\\\sigma^2}}{\\\\sqrt{t}}dt \\\\\\\\ &amp;= \\\\frac {2\\\\sigma^2}{\\\\sqrt{\\\\pi}}\\\\int_0^{+\\\\infty}\\\\frac{t^{\\\\frac12}}{\\\\exp(t)}dt \\\\\\\\&amp;= \\\\frac {2\\\\sigma^2}{\\\\sqrt{\\\\pi}} \\\\Gamma(\\\\frac 32) \\\\\\\\ &amp;= \\\\sigma^2 \\\\qquad \\\\qquad \\\\because \\\\Gamma(\\\\frac 32)=\\\\frac{\\\\sqrt{\\\\pi}}{2} \\\\end{aligned}\\\\]  Multivariate Gaussian distribution      $\\\\boldsymbol{x}\\\\backsim \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\boldsymbol{\\\\Sigma})$\\\\[\\\\begin{aligned} \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu},\\\\boldsymbol{\\\\Sigma}) = \\\\frac{1}{(2\\\\pi)^{\\\\frac{D}{2}}}\\\\frac{1}{\\\\vert\\\\boldsymbol{\\\\Sigma}\\\\vert^{\\\\frac 12}}e^{-\\\\frac 12 (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})^T\\\\boldsymbol{\\\\Sigma}^{-1}(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})} \\\\end{aligned}\\\\]  Student’s t-distribution      $x\\\\sim \\\\text{St}(x\\\\vert \\\\mu, \\\\lambda, \\\\nu)$\\\\[\\\\begin{aligned} p(x) = \\\\text{St}(x\\\\vert \\\\mu, \\\\lambda, \\\\nu) = \\\\frac {\\\\Gamma(\\\\nu/2+1/2)}{\\\\Gamma(\\\\nu /2)}\\\\left(\\\\frac \\\\lambda {\\\\pi\\\\nu}\\\\right)^{1/2} \\\\left[1+\\\\frac {\\\\lambda(x-\\\\mu)^2}{\\\\nu} \\\\right]^{-\\\\frac {\\\\nu + 1}{2}} \\\\end{aligned}\\\\]    Student’s t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a normally distributed population in situations where the sample size is small and the population’s standard deviation is unknown  The t-distribution plays a role in a number of widely used statistical analyses, including Student’s t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis.      If we take a sample of $n$ observations from a normal distribution, then the t-distribution with $\\\\nu=n-1$ degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term $\\\\sqrt {n}$. In this way, the t-distribution can be used to construct a confidence interval for the true mean.        Multivariate Student’s t-distribution\\\\[\\\\begin{aligned} \\\\mathrm{St}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, \\\\boldsymbol{\\\\Lambda}, \\\\nu) &amp;= \\\\int_0^\\\\infty \\\\mathcal{N}(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}, (\\\\eta \\\\boldsymbol{\\\\Lambda})^{-1}) \\\\mathrm{Gam}(\\\\eta \\\\vert \\\\frac \\\\nu 2, \\\\frac \\\\nu 2)d\\\\eta \\\\\\\\ &amp;= \\\\frac {\\\\Gamma((D+\\\\nu)/2)}{\\\\Gamma(\\\\nu/2)} \\\\frac {\\\\vert \\\\boldsymbol{\\\\Lambda} \\\\vert^{1/2}}{(\\\\pi \\\\nu)^{D/2}} \\\\left[1+ \\\\frac {(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})^T\\\\Lambda(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})}{\\\\nu}\\\\right]^{-(D+\\\\nu)/2} \\\\end{aligned}\\\\]  Chi-squared distribution      If $Z_1, …, Z_k$ are independent, standard normal random variables, then the sum of their squares\\\\[Q = \\\\sum_{i=1}^k Z_i^2\\\\]    is distribution according to the chi-squared distribution with $k$ degrees of freedom. This is usually denoted as:\\\\[Q \\\\sim \\\\mathcal{X}^2(k) \\\\text{\\\\qquad or \\\\qquad} Q \\\\sim \\\\mathcal{X}_k^2\\\\]    The chi-squared distribution is used primarily in hypothesis testing, and to a lesser extent for confidence intervals for population variance when the underlying distribution is normal.      The chi-squared distribution is not as often applied in the direct modeling of natural phenomena        $x\\\\sim \\\\text{Chi}(x\\\\vert k)$: $k$ degrees of freedom\\\\[p(x) = \\\\text{Chi}(x\\\\vert k) = \\\\left\\\\{\\\\begin{aligned} &amp;\\\\frac {1}{2^{k/2}\\\\Gamma(k/2)}x^{\\\\frac k2 -1} e^{-\\\\frac x2} \\\\qquad &amp;x&gt;0 \\\\\\\\ &amp;0 \\\\qquad &amp;\\\\text{otherwise} \\\\end{aligned} \\\\right.\\\\]        Note: if $x&gt;0$, Chi-squared distribution is a Gamma distribution\\\\[\\\\begin{aligned} \\\\frac {1}{2^{k/2}\\\\Gamma(k/2)}x^{\\\\frac k2 -1} e^{-\\\\frac x2} = \\\\text{Gamma}\\\\left(x\\\\mid \\\\frac k2, \\\\frac 12\\\\right) \\\\end{aligned}\\\\]  Beta distribution      $x\\\\sim Beta(x\\\\vert a,b), x \\\\in [0,1], a&gt;0, b&gt;0$\\\\[\\\\begin{aligned} p(x) = Beta(x\\\\vert a,b) &amp;= \\\\frac {x^{a-1}(1-x)^{b-1}}{\\\\int_0^1 u^{a-1}(1-u)^{b-1}du} \\\\\\\\ &amp;= \\\\frac 1{B(a,b)}x^{a-1}(1-x)^{b-1} \\\\qquad &amp; \\\\text{Beta \\\\space function} \\\\\\\\ &amp;= \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)}x^{a-1}(1-x)^{b-1} \\\\qquad &amp;\\\\text{Gamma\\\\space function} \\\\end{aligned}\\\\]        Expectation: $\\\\frac {a}{a+b}$\\\\[\\\\begin{aligned}  \\\\mathbb{E}[x] &amp;= \\\\int_0^1 \\\\frac {x}{B(a,b)}x^{a-1}(1-x)^{b-1} \\\\\\\\&amp;= \\\\frac {B(a+1,b)}{B(a,b)} \\\\\\\\&amp;= \\\\frac {\\\\Gamma(a+1)\\\\Gamma(b)}{\\\\Gamma(a+b+1)}B(a,b)^{-1} \\\\\\\\&amp;= \\\\frac {a\\\\Gamma(a)\\\\Gamma(b)}{(a+b)\\\\Gamma(a+b)}B(a,b)^{-1} \\\\qquad \\\\qquad \\\\Gamma(n+1) = n\\\\Gamma(n) \\\\\\\\&amp;= \\\\frac a{a+b} \\\\end{aligned}\\\\]        Variance: $\\\\frac {ab}{(a+b)^2(a+b+1)}$\\\\[\\\\begin{aligned}  \\\\mathbb{E}[x^2] &amp;= \\\\int_0^1 \\\\frac {x^2}{B(a,b)}x^{a-1}(1-x)^{b-1} \\\\\\\\&amp;= \\\\frac {B(a+2,b)}{B(a,b)} \\\\\\\\&amp;= \\\\frac {\\\\Gamma(a+2)\\\\Gamma(b)}{\\\\Gamma(a+b+2)}B(a,b)^{-1} \\\\\\\\&amp;= \\\\frac {a(a+1)\\\\Gamma(a)\\\\Gamma(b)}{(a+b)(a+b+1)\\\\Gamma(a+b)}B(a,b)^{-1} \\\\qquad \\\\qquad \\\\Gamma(n+1) = n\\\\Gamma(n) \\\\\\\\&amp;= \\\\frac {a(a+1)}{(a+b)(a+b+1)} \\\\\\\\ \\\\\\\\ \\\\text{var}[x] &amp;= \\\\mathbb{E}[x^2] - \\\\mathbb{E}[x]^2 \\\\\\\\ &amp;= \\\\frac {a(a+1)}{(a+b)(a+b+1)} - \\\\frac {a^2}{(a+b)^2} \\\\\\\\ &amp;= \\\\frac {ab}{(a+b)^2(a+b+1)} \\\\end{aligned}\\\\]  Dirichlet distribution      Multivariate Beta distribution        $\\\\boldsymbol{x} \\\\sim Dir(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{a})$                  $\\\\boldsymbol{x}=[x_1,x_2,···,x_K]^T$\\\\[\\\\begin{aligned} &amp;x_k \\\\in [0,1], k=1,2,···,K \\\\\\\\ &amp;\\\\sum_{k=1}^K x_k = 1 \\\\end{aligned}\\\\]                    $\\\\boldsymbol{a}=[a_1, a_2,···,a_K]^K$\\\\[\\\\tilde{a} = \\\\sum_{k=1}^K a_k\\\\]          \\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) = Dir(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{a}) = \\\\frac {\\\\Gamma(\\\\tilde{a})}{\\\\Gamma(a_1)\\\\Gamma(a_2)···\\\\Gamma(a_K)}\\\\prod_{k=1}^Kx_k^{a_k-1} \\\\end{aligned}\\\\]        Expectation: $\\\\frac {a_i}{\\\\tilde{a}}$\\\\[\\\\begin{aligned} \\\\mathbb{E}[x_i] &amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {\\\\Gamma(\\\\tilde{a})}{\\\\prod_{k=1}^{K}\\\\Gamma(a_k)}x_i\\\\prod_{k=1}^Kx_k^{a_k-1}  dx_1dx_2d_3···dx_K \\\\\\\\ &amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {\\\\Gamma(\\\\tilde{a})}{\\\\prod_{k\\\\neq i}^{K}\\\\Gamma(a_k)}x_ix_i^{a_i-1} \\\\prod_{k\\\\neq i}^{K}x_k^{a_k-1} dx_1dx_2d_3···dx_K \\\\\\\\&amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {a_i\\\\Gamma(\\\\tilde{a})}{\\\\Gamma(a_i+1)\\\\prod_{k\\\\neq i}^{K}\\\\Gamma(a_k)}x_i^{a_i+1-1} \\\\prod_{k\\\\neq i}^{K}x_k^{a_k-1} dx_1dx_2d_3···dx_K \\\\\\\\&amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {a_i\\\\Gamma(\\\\tilde{a}+1)}{\\\\tilde{a}\\\\Gamma(a_i+1)\\\\prod_{k\\\\neq i}^{K}\\\\Gamma(a_k)}x_i^{a_i+1-1} \\\\prod_{k\\\\neq i}^{K}x_k^{a_k-1} dx_1dx_2d_3···dx_K \\\\\\\\ &amp;= \\\\frac {a_i}{\\\\tilde{a}} \\\\end{aligned}\\\\]        Variance: $\\\\frac {a_i(\\\\tilde{a}-a_i)}{\\\\tilde{a}^2(\\\\tilde{a}+1)}$\\\\[\\\\begin{aligned} \\\\mathbb{E}[x_i^2] &amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {\\\\Gamma(\\\\tilde{a})}{\\\\prod_{k=1}^{K}\\\\Gamma(a_k)}x_i^2\\\\prod_{k=1}^Kx_k^{a_k-1}  dx_1···dx_K \\\\\\\\&amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {a_i(a_i+1)\\\\Gamma(\\\\tilde{a}+2)}{\\\\tilde{a}(\\\\tilde{a}+1)\\\\Gamma(a_i+2)\\\\prod_{k\\\\neq i}^{K}\\\\Gamma(a_k)}x_i^{a_i+2-1} \\\\prod_{k\\\\neq i}^{K}x_k^{a_k-1} dx_1···dx_K  \\\\\\\\ &amp;= \\\\frac {a_i(a_i+1)}{\\\\tilde{a}(\\\\tilde{a}+1)} \\\\\\\\ \\\\\\\\ \\\\text{var}[x_i] &amp;= \\\\frac {a_i(a_i+1)}{\\\\tilde{a}(\\\\tilde{a}+1)} - \\\\frac {a_i^2}{\\\\tilde{a}^2} = \\\\frac {a_i(\\\\tilde{a}-a_i)}{\\\\tilde{a}^2(\\\\tilde{a}+1)} \\\\end{aligned}\\\\]        Covariance: $\\\\frac {-a_ia_j}{\\\\tilde{a}^2(\\\\tilde{a}+1)}$\\\\[\\\\begin{aligned} \\\\begin{aligned} \\\\mathbb{E}[x_ix_j] &amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {\\\\Gamma(\\\\tilde{a})}{\\\\prod_{k=1}^{K}\\\\Gamma(a_k)}x_ix_j\\\\prod_{k=1}^Kx_k^{a_k-1}  dx_1···dx_K \\\\\\\\&amp;= \\\\int_0^1···\\\\int_0^1 \\\\frac {a_ia_j\\\\Gamma(\\\\tilde{a}+2)}{\\\\tilde{a}(\\\\tilde{a}+1)\\\\Gamma(a_i+1)\\\\Gamma(a_j+1)\\\\prod_{k\\\\neq i,j}^{K}\\\\Gamma(a_k)}x_i^{a_i+1-1}x_j^{a_j+1-1}  \\\\prod_{k\\\\neq i,j}^{K}x_k^{a_k-1} dx_1···dx_K  \\\\\\\\ &amp;= \\\\frac {a_ia_j}{\\\\tilde{a}(\\\\tilde{a}+1)} \\\\\\\\ \\\\\\\\ \\\\text{var}[x_i, x_j] &amp;= \\\\mathbb{E}[x_ix_j] - \\\\mathbb{E}[x_i]\\\\mathbb{E}[x_j] \\\\\\\\&amp;= \\\\frac {a_ia_j}{\\\\tilde{a}(\\\\tilde{a}+1)} - \\\\frac {a_ia_j}{\\\\tilde{a}^2} \\\\\\\\&amp;= \\\\frac {-a_ia_j}{\\\\tilde{a}^2(\\\\tilde{a}+1)} \\\\end{aligned} \\\\end{aligned}\\\\]  Gamma distribution      $x\\\\sim \\\\text{Gamma}(x\\\\vert a,b), x&gt;0, a&gt;0,b&gt;0$\\\\[\\\\begin{aligned} p(x) = \\\\text{Gamma}(x\\\\vert a,b) &amp;= \\\\frac {1}{\\\\Gamma(a)}b^ax^{a-1}e^{-bx} \\\\end{aligned}\\\\]        Expectation: $\\\\frac ab$\\\\[\\\\begin{aligned}  \\\\mathbb{E}[x] &amp;= \\\\int_0^{+\\\\infty}\\\\frac {x}{\\\\Gamma(a)}b^ax^{a-1}e^{-bx}dx \\\\\\\\&amp;= \\\\frac {1}{\\\\Gamma(a)}\\\\int_0^{+\\\\infty}(bx)^ae^{-bx}dx \\\\\\\\&amp;= \\\\frac {1}{\\\\Gamma(a)}\\\\int_0^{+\\\\infty}\\\\frac {t^a}{e^t}\\\\frac 1bdt \\\\\\\\&amp;= \\\\frac {\\\\Gamma(a+1)}{b\\\\Gamma(a)} \\\\\\\\&amp;= \\\\frac ab \\\\end{aligned}\\\\]        Variance: $\\\\frac {a}{b^2}$\\\\[\\\\begin{aligned} \\\\mathbb{E}[x^2] &amp;= \\\\int_0^{+\\\\infty}\\\\frac {x^2}{\\\\Gamma(a)}b^ax^{a-1}e^{-bx}dx \\\\\\\\&amp;= \\\\frac {1}{b\\\\Gamma(a)}\\\\int_0^{+\\\\infty}(bx)^{a+1}e^{-bx}dx \\\\\\\\&amp;= \\\\frac {1}{b\\\\Gamma(a)}\\\\int_0^{+\\\\infty}\\\\frac {t^{a+1}}{e^t}\\\\frac 1bdt \\\\\\\\&amp;= \\\\frac {\\\\Gamma(a+2)}{b^2\\\\Gamma(a)} \\\\\\\\&amp;= \\\\frac {a(a+1)}{b^2} \\\\\\\\ \\\\\\\\ \\\\text{var}[x] &amp;= \\\\mathbb{E}[x^2] - \\\\mathbb{E}[x]^2 \\\\\\\\ &amp;= \\\\frac {a(a+1)}{b^2} - \\\\frac {a^2}{b^2} \\\\\\\\ &amp;= \\\\frac {a}{b^2} \\\\end{aligned}\\\\]  Inverse Gamma distribution      $x\\\\sim \\\\text{Gamma}(x\\\\vert a,b), x&gt;0, a&gt;0,b&gt;0$\\\\[\\\\begin{aligned} p(x) &amp;= \\\\text{Gamma}(x\\\\vert a,b)  \\\\\\\\ &amp;= \\\\frac {1}{\\\\Gamma(a)}b^a\\\\left(\\\\frac 1x\\\\right)^{a+1}e^{-\\\\frac bx} \\\\end{aligned}\\\\]          Derivation from Gamma distribution              Let $Y = g(X) = \\\\frac 1X$, then $g^{-1}(Y) = \\\\frac 1Y$, the pdf of $Y$ is:\\\\[\\\\begin{aligned} f_Y(y) &amp;= f_X(g^{-1}(y))\\\\left\\\\vert \\\\frac {d}{dy}g^{-1}(y) \\\\right\\\\vert \\\\\\\\ &amp;= \\\\frac {1}{\\\\Gamma(a)}b^a(1/y)^{a-1}e^{-b/y} \\\\frac {1}{y^2} \\\\\\\\&amp;= \\\\frac {1}{\\\\Gamma(a)}b^a(1/y)^{a+1}e^{-b/y}   \\\\end{aligned}\\\\]                  Expection: $\\\\mathbb{E}[x] = 1/\\\\mathbb{E}[1/x] = \\\\frac ba$        Variance: $\\\\frac {-b^2}{a^2(a+1)}$\\\\[\\\\begin{aligned} \\\\mathbb{E}[x^2] &amp;=  1/\\\\mathbb{E}[1/x^2] \\\\\\\\ &amp;= \\\\frac {b^2}{a(a+1)} \\\\\\\\ \\\\\\\\ \\\\text{var}[x] &amp;=  \\\\mathbb{E}[x^2] - \\\\mathbb{E}[x]^2 \\\\\\\\ &amp;= \\\\frac {-b^2}{a^2(a+1)} \\\\end{aligned}\\\\]  Wishart distribution      Let $X$ be a $p × p$ symmetric matrix of random variables that is positive definite. Let $V$ be a (fixed) symmetric positive definite matrix of size $p × p$.        Then, if $n ≥ p$, $X$ has a Wishart distribution with $n$ degrees of freedom if it has the probability density function:        $X \\\\sim \\\\mathcal{W}(X\\\\vert V, n)$\\\\[\\\\begin{aligned} f_\\\\mathcal{\\\\boldsymbol{X}}(X) = \\\\frac {1}{2^{np/2}\\\\vert V \\\\vert^{n/2} \\\\Gamma_p(n/2) } \\\\vert X \\\\vert^{(n-p-1)/2}e^{-\\\\frac 12 \\\\text{tr}(V^{-1}X)} \\\\end{aligned}\\\\]          The positive integer $n$ is the number of degrees of freedom. Sometimes this is written $\\\\mathcal{W}(V, p, n)$. For $n ≥ p$ the matrix $S\\\\sim \\\\mathcal{W}(V, p)$ is invertible with probability 1 if $V$ is invertible.              Where $\\\\Gamma_p$ is the multivariate gamma function:\\\\[\\\\Gamma_p\\\\left(\\\\frac n2\\\\right) = \\\\pi^{p(p-1)/4}\\\\prod_{j=1}^p \\\\Gamma\\\\left(\\\\frac n2 - \\\\frac {j-1}{2}) \\\\right)\\\\]                  Note:          The density above is not the joint density of all the $p^{2}$ elements of the random matrix X (such $p^{2}$-dimensional density does not exist because of the symmetry constrains $X_{ij}=X_{ji})$, it is rather the joint density of $p(p+1)/2$ elements $X_{ij}$ for $i&lt;j$.      Also, the density formula above applies only to positive definite matrices $X$ for other matrices the density is equal to zero.      If $p=V=1$, then this distribution is a Chi-squared distribution with $n$ degrees of freedom, also because $x&gt;0$, it is also a Gamma distribution.      Inverse Wishart distribution      $X \\\\sim \\\\mathcal{W}^{-1}(X\\\\vert V, n)$\\\\[\\\\begin{aligned} f_\\\\mathcal{\\\\boldsymbol{X}}(X) = \\\\frac {\\\\vert V \\\\vert^{n/2}}{2^{np/2} \\\\Gamma_p(n/2) } \\\\vert X \\\\vert^{-(n+p+1)/2}e^{-\\\\frac 12 \\\\text{tr}(VX^{-1})} \\\\end{aligned}\\\\]                  Where $\\\\Gamma_p$ is the multivariate gamma function:\\\\[\\\\Gamma_p\\\\left(\\\\frac n2\\\\right) = \\\\pi^{p(p-1)/4}\\\\prod_{j=1}^p \\\\Gamma\\\\left(\\\\frac n2 - \\\\frac {j-1}{2}) \\\\right)\\\\]                  we see if $A = X^{-1}$, then $A \\\\sim \\\\mathcal{W}(V^{-1}, n)$  Gauss-Gamma distribution      For a pair of random variables, $(x,t)$, suppose that the conditional distribution of $X$ given $T$ is given by\\\\[p(x\\\\vert \\\\tau) \\\\sim \\\\mathcal{N}\\\\left(x\\\\vert \\\\mu, (\\\\lambda \\\\tau)^{-1}\\\\right)\\\\]        And suppose also that the marginal distribution of $t$ is given by:\\\\[p(\\\\tau) \\\\sim \\\\text{Gamma}(\\\\tau \\\\vert a,b)\\\\]        Then $(x,\\\\tau)$ has a Gauss-Gamma distribution: $(x,\\\\tau) \\\\sim \\\\text{Gauss-Gamma}(\\\\mu, \\\\lambda, a, b)$\\\\[\\\\begin{aligned}p(x,\\\\tau) &amp;= \\\\text{Gauss-Gamma}(\\\\mu, \\\\lambda, a, b) \\\\\\\\&amp;= \\\\mathcal{N}\\\\left(x\\\\vert \\\\mu, (\\\\lambda \\\\tau)^{-1}\\\\right)\\\\text{Gamma}(a,b) \\\\\\\\&amp;= \\\\sqrt{\\\\frac{\\\\lambda \\\\tau}{2\\\\pi}}\\\\exp\\\\left( -\\\\frac 12 \\\\lambda \\\\tau (x-\\\\mu)^2 \\\\right) \\\\frac {b^a}{\\\\Gamma(a)}\\\\tau^{a-1}\\\\exp(-b\\\\tau) \\\\end{aligned}\\\\]        In the multivariate form:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x},\\\\tau) &amp;= \\\\text{Gauss-Gamma}(\\\\boldsymbol{\\\\mu}, V^{-1}, a, b) \\\\\\\\&amp;= \\\\mathcal{N}\\\\left(x\\\\vert \\\\boldsymbol{\\\\mu}, \\\\frac 1\\\\tau V \\\\right)\\\\text{Gamma}(\\\\tau \\\\vert a,b) \\\\\\\\ &amp;= \\\\vert V\\\\vert^{-\\\\frac 12}\\\\left(\\\\frac{\\\\tau}{2\\\\pi}\\\\right)^{k/2} \\\\exp\\\\left( -\\\\frac \\\\tau 2  (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})^TV^{-1}(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}) \\\\right) \\\\frac {b^a}{\\\\Gamma(a)}\\\\tau^{a-1}\\\\exp(-b\\\\tau) \\\\end{aligned}\\\\]          Where $V$ is the $k\\\\times k$ covariance matrix for the Gaussian prior on $\\\\boldsymbol{\\\\mu}$            Note:          The marginal distribution of $\\\\tau$ is a gamma distribution      The conditional distribution of $x$ given $\\\\tau$ is a gaussian distribution      The marginal distribution of $x$ is a Student’s t-distribution.      Gauss-inverse-Gamma distribution      $(x,\\\\sigma^2) \\\\sim \\\\text{Gauss-Gamma}^{-1}(\\\\mu, \\\\lambda, a, b)$\\\\[\\\\begin{aligned} p(x, \\\\sigma^2) = \\\\text{Gauss-Gamma}^{-1}(\\\\mu, \\\\lambda, a, b) &amp;= \\\\mathcal{N}(x\\\\vert \\\\mu, \\\\sigma^2/\\\\lambda)\\\\text{Gamma}^{-1}(\\\\sigma^2\\\\vert a,b) \\\\\\\\ &amp;= \\\\sqrt{\\\\frac {\\\\lambda}{2\\\\pi\\\\sigma^2}}\\\\exp\\\\left( -\\\\frac {\\\\lambda}{2\\\\sigma^2}  (x-\\\\mu)^2 \\\\right) \\\\frac {b^a}{\\\\Gamma(a)}\\\\left(\\\\frac 1{\\\\sigma^2} \\\\right)^{a+1}\\\\exp\\\\left(-\\\\frac{b}{\\\\sigma^2}\\\\right)  \\\\end{aligned}\\\\]        In the multivariate form:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}, \\\\sigma^2) &amp;= \\\\text{Gauss-Gamma}^{-1}(\\\\boldsymbol{\\\\mu}, V^{-1}, a, b) \\\\\\\\&amp;= \\\\mathcal{N}\\\\left(x\\\\vert \\\\boldsymbol{\\\\mu}, \\\\sigma^2 V \\\\right)\\\\text{Gamma}^{-1}(\\\\sigma^2 \\\\vert a,b)  \\\\\\\\ &amp;= \\\\vert V\\\\vert^{-\\\\frac 12}(2\\\\pi\\\\sigma^2)^{-k/2}\\\\exp\\\\left( -\\\\frac {1}{2\\\\sigma^2}  (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})^TV^{-1}(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}) \\\\right) \\\\frac {b^a}{\\\\Gamma(a)}\\\\left(\\\\frac 1{\\\\sigma^2} \\\\right)^{a+1}\\\\exp\\\\left(-\\\\frac{b}{\\\\sigma^2}\\\\right) \\\\end{aligned}\\\\]          Where $V$ is the $k\\\\times k$ covariance matrix for the Gaussian prior on $\\\\boldsymbol{\\\\mu}$      Gauss-Wishart distribution      Suppose\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu}\\\\vert \\\\boldsymbol{\\\\mu}_0, V^{-1}, \\\\Lambda) \\\\sim \\\\mathcal{N}(\\\\boldsymbol{\\\\mu}\\\\vert \\\\boldsymbol{\\\\mu}_0, V\\\\Lambda^{-1}) \\\\end{aligned}\\\\]    and:\\\\[\\\\begin{aligned} p(\\\\Lambda \\\\vert A, \\\\nu) \\\\sim \\\\mathcal{W}(\\\\Lambda \\\\vert A, \\\\nu) \\\\end{aligned}\\\\]        Then\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu}, \\\\Lambda) &amp;\\\\sim \\\\mathcal{NW}(\\\\boldsymbol{\\\\mu}, \\\\Lambda \\\\vert \\\\boldsymbol{\\\\mu}_0, V^{-1}, W, nu) \\\\\\\\ &amp;= \\\\mathcal{N}(\\\\boldsymbol{\\\\mu}\\\\vert \\\\boldsymbol{\\\\mu}_0, V\\\\Lambda^{-1})\\\\mathcal{W}(\\\\Lambda \\\\vert W, \\\\nu) \\\\end{aligned}\\\\]          Where $V$ is the $k\\\\times k$ covariance matrix for the Gaussian prior on $\\\\boldsymbol{\\\\mu}$      Gauss-inverse-Wishart distribution      Suppose\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu}\\\\vert \\\\boldsymbol{\\\\mu}_0, V^{-1}, \\\\Sigma) \\\\sim \\\\mathcal{N}\\\\left(\\\\boldsymbol{\\\\mu}\\\\vert \\\\boldsymbol{\\\\mu}_0, V\\\\Sigma \\\\right) \\\\end{aligned}\\\\]    and:\\\\[\\\\begin{aligned} p(\\\\Sigma \\\\vert A, \\\\nu) \\\\sim \\\\mathcal{W}^{-1}(\\\\Sigma \\\\vert W, \\\\nu) \\\\end{aligned}\\\\]        Then\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu}, \\\\Sigma) &amp;\\\\sim \\\\mathcal{NW}^{-1}(\\\\boldsymbol{\\\\mu}, \\\\Sigma \\\\vert \\\\boldsymbol{\\\\mu}_0, V^{-1}, W, \\\\nu) \\\\\\\\ &amp;= \\\\mathcal{N}\\\\left(\\\\boldsymbol{\\\\mu}\\\\vert \\\\boldsymbol{\\\\mu}_0, V \\\\Sigma \\\\right)\\\\mathcal{W}^{-1}(\\\\Sigma \\\\vert W, \\\\nu) \\\\end{aligned}\\\\]          Where $V$ is the $k\\\\times k$ covariance matrix for the Gaussian prior on $\\\\boldsymbol{\\\\mu}$      Matrix Gaussian Distribution\\\\[\\\\begin{aligned} p(X\\\\mid M, U, V) = \\\\frac {1}{(2\\\\pi)^{\\\\frac {np}{2}} \\\\vert V\\\\vert^{\\\\frac n2} \\\\vert U\\\\vert^{\\\\frac p2}} \\\\exp\\\\left( -\\\\frac 12 \\\\mathrm{Tr}\\\\left[ V^{-1}(X-M)^TU^{-1}(X-M) \\\\right] \\\\right) \\\\end{aligned}\\\\]  $M$ is $n\\\\times p$  $U$ is $p \\\\times p$  $V$ is $n \\\\times n$"
  },
  
  {
    "title": "Probaility Theory",
    "url": "/posts/probability_theorem/",
    "categories": "Probability",
    "tags": "",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Probaility Theory      Sum Rule  \\\\(p(X) = \\\\sum_Y p(X,Y)\\\\)        Product Rule  \\\\(p(X,Y) = p(Y \\\\vert X)p(X)\\\\)        Bayes’ Theorem  \\\\(\\\\begin{aligned} p(Y\\\\vert X) = \\\\frac {p(X \\\\vert Y)p(Y)}{p(X)} = \\\\frac {p(X \\\\vert Y)p(Y)}{\\\\sum_Y p(X \\\\vert Y)p(Y)} \\\\end{aligned}\\\\)        Prior Probability: For random variables $X$ (state variable) and $Y$ (target variable), it is the probability $P(Y)$ before we ovserve the identity of the state $X$.        Posterior Probability: Once we are told that the identity of state $X$, we can then use Bayes’ theorem to compute the probability $P(Y\\\\vert X)$, which shall call the posterior probability because it is the probability obtained after we observed $X$.    Probability Densities: If the probability of a real-valued variable $x$ falling in the interval $(x, x+\\\\delta x)$ is given by $p(x)\\\\delta x$ for $\\\\delta x \\\\rightarrow 0$, then $p(x)$ is called the probability density over $x$.          the probability that $x$ will lie in an interval $(a,b)$ is then given by  \\\\(p(x\\\\in (a,b)) = \\\\int_a^b p(x)dx\\\\)      the probability density $p(x)$ satisfy two condition  \\\\(\\\\begin{aligned} p(x) &amp;\\\\ge 0 \\\\\\\\ \\\\int_{-\\\\infty}^{\\\\infty} p(x)dx &amp;= 1 \\\\end{aligned}\\\\)        Probability Densities Transformation: Consider a change of variables $x=g(y)$, then a function $f(x)$ becomes $\\\\tilde{f}(y) = f(g(y))$. Then for the probability $p_x(x)$ and $p_y(y)$, observations falling in the range $(x, x+\\\\delta x)$ will, for small values of $\\\\delta x$, be transformaed in to the range $(y, y+\\\\delta y)$ where $p_x(x)\\\\delta x \\\\simeq p_y(y)\\\\delta y$ and hence  \\\\(\\\\begin{aligned} p_y(y) &amp;= p_x(x) \\\\left \\\\vert \\\\frac {dx}{dy} \\\\right\\\\vert \\\\\\\\ &amp;= p_x(g(y)) \\\\left \\\\vert g&#39;(y) \\\\right\\\\vert \\\\end{aligned}\\\\)          One consequence of this property is that the concept of maximum of a probability density is dependent on the choice of variable.        cumulative distribution function: it is the probability of $x$ lies in the interval $(-\\\\infty, a)$  \\\\(P(x\\\\le a) = \\\\int_{-\\\\infty}^a p(x) dx\\\\)          which satisfies $P’(x) = p(x)$            Probability Mass Function: Just like the probability density function, but the variable $x$ is a discrete variable.    Expectation: The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of $f(x)$ and will be denoted by $\\\\mathbb{E}[f]$  \\\\(\\\\begin{aligned} \\\\mathbb{E}[f] &amp;= \\\\sum_x p(x)f(x) \\\\\\\\ \\\\mathbb{E}[f] &amp;= \\\\int p(x)f(x)dx \\\\end{aligned}\\\\)          It can be approxmated as a finite sum over a finite number N of points drawn from the probability distribution or probability density  \\\\(\\\\mathbb{E}[f] \\\\simeq \\\\frac 1N \\\\sum_{n=1}^N f(x_n)\\\\)      Note: $\\\\mathbb{E}_x [f(x,y)]$ is a function of $y$      Conditional Expection:  \\\\(\\\\begin{aligned} \\\\mathbb{E}_x [f\\\\vert y] &amp;= \\\\sum_x p(x\\\\vert y) f(x) \\\\\\\\ \\\\mathbb{E}_x [f\\\\vert y] &amp;= \\\\int p(x\\\\vert y) f(x)dx \\\\end{aligned}\\\\)            Variance and Covariance  \\\\(\\\\begin{aligned} var[f] &amp;= \\\\mathbb{E}\\\\left[(f(x) - \\\\mathbb{E}[f(x)])^2\\\\right] \\\\\\\\ var[x] &amp;= \\\\mathbb{E}\\\\left[(x-\\\\mathbb{E}[x])^2\\\\right] \\\\\\\\ &amp;= \\\\mathbb{E}[x^2] - \\\\mathbb{E}[x]^2 \\\\\\\\ \\\\\\\\ cov[x,y] &amp;= \\\\mathbb{E}_{x,y}\\\\left[ (x-\\\\mathbb{E}[x])(y-\\\\mathbb{E}[y]) \\\\right] \\\\\\\\&amp;= \\\\mathbb{E}_{x,y}[xy] - \\\\mathbb{E}[x]\\\\mathbb{E}[y] \\\\\\\\ \\\\\\\\ cov[\\\\mathbf{x}, \\\\mathbf{y}] &amp;= \\\\mathbb{E}_{\\\\mathbf{x}, \\\\mathbf{y}}\\\\left[(\\\\mathbf{x}-\\\\mathbb{E}[\\\\mathbf{x}])(\\\\mathbf{y}^T-\\\\mathbb{E}[\\\\mathbf{y}^T]) \\\\right] \\\\\\\\ &amp;= \\\\mathbb{E}_{\\\\mathbf{x},\\\\mathbf{y}}[\\\\mathbf{x}\\\\mathbf{y}^T] - \\\\mathbb{E}[\\\\mathbf{x}]\\\\mathbb{E}[\\\\mathbf{y}^T]  \\\\end{aligned}\\\\)    Probabilities in Bayesian and Frequentist view          In frequentist view: probabilities is viewed as in terms of the frequencies of random, repeatable events      In Bayesian view: probabilities provide a quantification of uncertainty.        Likelihood Function: Given the observed data set $D$, $p(D\\\\vert \\\\mathbf{w})$ can be viewed as a function of the parameter vector $\\\\mathbf{w}$, in which case it is called the likelihood function.          It expresses how probable the observed data set is for different settings of the parameter vector $\\\\mathbf{w}$. But likelihood function is not a probability distribution.      Given this definition of likelihood, we can state the Bayes’s theorem in words:  \\\\(\\\\mathrm{posterior} \\\\propto \\\\mathrm{likelihood} \\\\times \\\\mathrm{prior}\\\\)            The different meaning of likelihood in Bayesian and Frequentist paradigms                  In frequentist paradigms: $\\\\mathbf{w}$ is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets $D$ (all possible data sets, not only the given observed data sets)                    In Bayesian paradigms: $\\\\mathbf{w}$ is considered to be a random variable.Furthermore there is only a single dataset $D$, the one that is actually observed, then the uncertainty in the parameters is expressed through a probability distribution on $\\\\mathbf{w}$.                  Maximum Likelihood:  It is a widely used frequentist estimator, $\\\\mathbf{w}{ml} = \\\\arg \\\\max\\\\mathbf{w} p(D\\\\vert \\\\mathbf{w})$. This corresponds to choosing the value of $\\\\mathbf{w}$ for which the probability of the observed data set is maximized.        Bootstrap: This is a approach to determining frequentist error bars. The multiple data sets are created simply by drawing N points at random from $\\\\mathbf{X}$, the given data sets with size N, and repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set $\\\\mathbf{X}$        Noninformative Prior: Reducing teh dependence on the prior in Bayesian approach.    independent and identically distributed or i.i.d: Data points that are drawn independently from the same distribution are said to be independent and identically distributed (i.i.d)."
  },
  
  {
    "title": "Conjugate_priors",
    "url": "/posts/conjugate_priors/",
    "categories": "",
    "tags": "",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Bernoulli Distributions      Bernoulli Distributions is the discrete probability distribution of a random variable which takes the value 1 with probability $\\\\mu$ and the value 0 with probability ${\\\\displaystyle 1-\\\\mu}$        $x\\\\in {0, 1}, \\\\mu \\\\in [0,1]$\\\\[\\\\begin{aligned} \\\\mathrm{Bern}(x\\\\vert \\\\mu) &amp;= \\\\mu^x(1-\\\\mu)^{1-x} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[x] &amp;= \\\\sum_{x} xp(x) = \\\\mu  \\\\\\\\ \\\\mathrm{var}[x] &amp;= \\\\mathbb{E}[(x-\\\\mu)^2] \\\\\\\\ &amp;= \\\\sum_{x} (x-\\\\mu)^2p(x)\\\\\\\\&amp;= \\\\mu^2(1-\\\\mu) + (1-\\\\mu)^2\\\\mu \\\\\\\\&amp;= \\\\mu(1-\\\\mu) \\\\end{aligned}\\\\]        Likelihood Function                  Let $D = {x_1, x_2, \\\\cdots, x_N }$ of observed data sets of $x$, and $m = \\\\sum_{i=1}^N x_i$.\\\\[\\\\begin{aligned} p(D\\\\vert \\\\mu) &amp;= \\\\prod_{i=1}^N \\\\mu^{x_i}(1-\\\\mu)^{1-x_i} \\\\\\\\ &amp;= \\\\mu^m(1-\\\\mu)^{N-m} \\\\end{aligned}\\\\]            Then we can see $m = \\\\sum_{i=1}^N x_i$ is a sufficient statistic for the data under this distribution.              And using maximum likelihood approach, we obtain:\\\\[\\\\mu_{ml} = \\\\frac mN\\\\]            Binomial Distributions      The binomial distribution with parameters $N$ and $\\\\mu$ is the discrete probability distribution of the number of successes $m$ in a sequence of $N$ independent experiments. It is the distribution of the number $m$ of  ovservations of $x=1$ in bernoulli distributions given that the data set has size $N$.\\\\[\\\\begin{aligned} \\\\mathrm{Bin}(m\\\\vert N,\\\\mu) &amp;= \\\\left( \\\\begin{matrix}N\\\\\\\\m\\\\end{matrix} \\\\right) \\\\mu^m(1-\\\\mu)^{N-m} \\\\\\\\ &amp;= \\\\frac {N!}{(N-m)!m!} \\\\mu^m(1-\\\\mu)^{N-m}  \\\\end{aligned}\\\\]                  Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[m] &amp;= \\\\sum_{m=0}^N m\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\&amp;= \\\\sum_{m=1}^N m\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\ &amp;= \\\\sum_{m=1}^N m \\\\frac {N!}{(N-m)!m!} \\\\mu^m(1-\\\\mu)^{N-m} \\\\\\\\ &amp;= N\\\\mu \\\\sum_{m=1}^N \\\\frac {(N-1)!}{((N-1)-(m-1))!(m-1)!} \\\\mu^{m-1}(1-\\\\mu)^{(N-1)-(m-1)} \\\\\\\\ &amp;= N\\\\mu \\\\sum_{k=0}^{N-1} \\\\frac {(N-1)!}{((N-1)-k)!k!} \\\\mu^{k}(1-\\\\mu)^{(N-1)-k} \\\\\\\\ &amp;= N\\\\mu \\\\sum_{k=0}^{N-1} \\\\mathrm{Bin}(k\\\\vert N-1, \\\\mu) \\\\\\\\ &amp;= N\\\\mu  \\\\end{aligned}\\\\]                    Variance\\\\[\\\\begin{aligned} \\\\mathrm{var}[m] &amp;= \\\\mathbb{E}[(m-\\\\mathbb{E}[m])^2] \\\\\\\\ &amp;= \\\\sum_{m=0}^N (m-\\\\mathbb{E}[m])^2 \\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\&amp;= \\\\sum_{m=0}^N (m-N\\\\mu)^2 \\\\mathrm{Bin}(m\\\\vert N,\\\\mu) \\\\\\\\ &amp;= \\\\sum_{m=0}^N m^2\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) -2N\\\\mu \\\\sum_{m=0}^N m\\\\mathrm{Bin}(m\\\\vert N,\\\\mu) + N^2\\\\mu^2 \\\\sum_{m=0}^N  \\\\\\\\ &amp;= \\\\left[\\\\sum_{m=1}^N m^2\\\\frac {N!}{(N-m)!m!} \\\\mu^m(1-\\\\mu)^{N-m} \\\\right]  -2N\\\\mu N\\\\mu + N^2 \\\\mu^2 \\\\\\\\ &amp;= -N^2\\\\mu^2 + N\\\\mu \\\\sum_{m=1}^N m \\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\\\\\&amp;= -N^2\\\\mu^2 + N\\\\mu \\\\sum_{m=1}^N (1+m-1) \\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\\\\\ &amp;=  -N^2\\\\mu^2 + N\\\\mu\\\\left( \\\\sum_{m=1}^N\\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} +  \\\\sum_{m=1}^N (m-1) \\\\frac {(N-1)!}{(N-m)!(m-1)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\right) \\\\\\\\ &amp;= -N^2\\\\mu^2 + N\\\\mu\\\\left(1+\\\\sum_{m=2}^N \\\\frac {(N-1)!}{(N-m)!(m-2)!}\\\\mu^{m-1}(1-\\\\mu)^{N-m} \\\\right)  \\\\\\\\ &amp;= -N^2\\\\mu^2 + N\\\\mu \\\\left(1+(N-1)\\\\mu \\\\sum_{m=2}^N \\\\frac {(N-2)!}{((N-2)-(m-2))!(m-2)!}\\\\mu^{m-2}(1-\\\\mu)^{(N-2)-(m-2)} \\\\right) \\\\\\\\&amp;= -N^2\\\\mu^2 + N\\\\mu\\\\left(1+ (N-1)\\\\mu\\\\sum_{k=0}^{N-2} \\\\mathrm{Bin}(k\\\\vert N-2, \\\\mu) \\\\right) \\\\\\\\&amp;= -N^2\\\\mu^2 + N\\\\mu(1+N\\\\mu-\\\\mu) \\\\\\\\&amp;= N\\\\mu(1-\\\\mu)\\\\end{aligned}\\\\]            Beta Distribution      New we introduce a prior distribution of $p(\\\\mu)$ over the parameter $\\\\mu$ in the Bernouli and Binomial distribution        The beta distribution as the prior distribution will have a simple interpretation as well as some useful analytical properties.        conjugacy: In this case, if we let the posterior distribution is priportional to the product of the prior and the likelihood function, then it will have the same functional form as the prior. Then we will see the posterior distribution is also a beta distribution. This property is called conjugacy.\\\\[\\\\begin{aligned} \\\\mathrm{Beta}(\\\\mu \\\\vert a, b) = \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1} \\\\end{aligned}\\\\]                  where $\\\\Gamma(x)$ is the Gamma function defined by:\\\\[\\\\begin{aligned}\\\\Gamma(x) = \\\\int_0^{\\\\infty} u^{x-1}e^{-u}du \\\\end{aligned}\\\\]                              $\\\\Gamma(x+1) = x\\\\Gamma(x)$, and then $\\\\Gamma(x+1) = x!$ when $x$ is an integer.\\\\[\\\\begin{aligned} \\\\Gamma(x+1) &amp;= \\\\int_0^{\\\\infty} u^{x}e^{-u}du \\\\\\\\&amp;= \\\\int_0^{\\\\infty} -u^{x}d(e^{-u}) \\\\\\\\&amp;=-u^{x}e^{-u} - \\\\int_0^\\\\infty x u^{x-1} e^{-u} du \\\\qquad\\\\qquad (using: \\\\space \\\\int u(x)d[v(x)] = u(x)v(x) - \\\\int v(x)d[u(x)]) \\\\\\\\&amp;= -u^{x}e^{-u} + x\\\\Gamma(x)  \\\\end{aligned}\\\\]                                          according $u \\\\in [0,+\\\\infty)$, if $u=0$, $-u^x e^{-u}=0$; and if $u\\\\rightarrow +\\\\infty$:\\\\[\\\\lim_{u\\\\rightarrow +\\\\infty} -\\\\frac {u^x}{e^u} = \\\\lim_{u\\\\rightarrow +\\\\infty} -\\\\frac {x!}{e^u} = 0 \\\\qquad\\\\qquad (using:\\\\space \\\\mathrm{L&#39;Hôpital&#39;s \\\\space rule})\\\\]                                            then we have $\\\\Gamma(x+1) = x\\\\Gamma(x)$.                                                                The coefficient ensures that the beta distribution is normalized.\\\\[\\\\begin{aligned} \\\\int_0^1 \\\\mathrm{Beta}(\\\\mu \\\\vert a, b) du = 1 \\\\end{aligned}\\\\]                  Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\mu] &amp;= \\\\int_0^1 \\\\mu \\\\mathrm{Beta}(\\\\mu \\\\vert a, b) d\\\\mu \\\\\\\\&amp;= \\\\int_0^1 \\\\mu \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1}d\\\\mu \\\\\\\\&amp;= \\\\frac {a\\\\Gamma(a+1+b)}{(a+b)\\\\Gamma(a+1)\\\\Gamma(b)} \\\\int_0^1  \\\\mu^{a}(1-\\\\mu)^{b-1}d\\\\mu \\\\\\\\ &amp;= \\\\frac {a}{a+b} \\\\int_0^1 \\\\frac {\\\\Gamma(c+b)}{\\\\Gamma(c)\\\\Gamma(b)} \\\\mu^{c-1}(1-\\\\mu)^{b-1}d\\\\mu \\\\\\\\ &amp;= \\\\frac {a}{a+b}\\\\end{aligned}\\\\]        Variance\\\\[\\\\begin{aligned} \\\\mathrm{var}[\\\\mu]  &amp;= \\\\mathbb{E}[(\\\\mu - \\\\frac {a}{a+b})^2] \\\\\\\\&amp;= \\\\int_0^1 (\\\\mu - \\\\frac {a}{a+b})^2 \\\\frac {\\\\Gamma(a+b)}{\\\\Gamma(a)\\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1}  d\\\\mu \\\\\\\\&amp;= \\\\frac {a}{a+b} \\\\left(\\\\int_0^1 \\\\mu \\\\frac {\\\\Gamma(a+b+1)}{\\\\Gamma(a+1)\\\\Gamma(b)} \\\\mu^{a}(1-\\\\mu)^{b-1}\\\\right) - 2\\\\frac {a}{a+b} * \\\\frac {a}{a+b} + \\\\left(\\\\frac {a}{a+b}\\\\right)^2 \\\\\\\\&amp;= \\\\frac {a}{a+b} \\\\frac {a+1}{a+b+1} - \\\\left(\\\\frac {a}{a+b}\\\\right)^2 \\\\\\\\ &amp;= \\\\frac {ab}{(a+b)^2(a+b+1)} \\\\end{aligned}\\\\]        The Posterior Distribution\\\\[\\\\begin{aligned} p(\\\\mu \\\\vert D) &amp;\\\\propto p(D\\\\vert \\\\mu)p(\\\\mu) \\\\\\\\ p(\\\\mu \\\\vert m, N, a, b) &amp;\\\\propto \\\\mu^{m+a-1}(1-\\\\mu)^{N-m+b-1} \\\\end{aligned}\\\\]                  Then we have:\\\\[\\\\begin{aligned} p(\\\\mu \\\\vert m, N, a, b) = \\\\frac {\\\\Gamma(N+a+b)}{\\\\Gamma(m+a)\\\\Gamma(N-m+b)} \\\\mu^{m+a-1}(1-\\\\mu)^{N-m+b-1} \\\\end{aligned}\\\\]                  The Predictive Distribution\\\\[\\\\begin{aligned} p(x\\\\vert D) &amp;= \\\\int_0^1 p(x\\\\vert \\\\mu) p(\\\\mu\\\\vert D)d\\\\mu \\\\\\\\ &amp;=\\\\left\\\\{\\\\begin{aligned}&amp;\\\\int_0^1 \\\\mu p(\\\\mu\\\\vert D)d\\\\mu \\\\qquad \\\\qquad &amp;x=1 \\\\\\\\ &amp; \\\\int_0^1 (1-\\\\mu) p(\\\\mu\\\\vert D)d\\\\mu \\\\qquad \\\\qquad &amp;x=0 \\\\end{aligned} \\\\right. \\\\\\\\ &amp;= \\\\left\\\\{\\\\begin{aligned}&amp;\\\\mathbb{E}_{\\\\mu}[\\\\mu \\\\vert D] \\\\qquad \\\\qquad &amp;x=1 \\\\\\\\ &amp; 1-\\\\mathbb{E}_{\\\\mu}[\\\\mu \\\\vert D] \\\\qquad \\\\qquad &amp;x=0 \\\\end{aligned} \\\\right.  \\\\end{aligned}\\\\]                  Using the posterior distribution result, we obtain:\\\\[\\\\begin{aligned} p(x=1 \\\\vert D) &amp;= \\\\int_0^1 \\\\mu \\\\frac {\\\\Gamma(N+a+b)}{\\\\Gamma(m+a)\\\\Gamma(N-m+b)} \\\\mu^{m+a-1}(1-\\\\mu)^{N-m+b-1} d\\\\mu &amp;= \\\\frac {m+a}{N+a+b} \\\\\\\\ p(x=0\\\\vert D) &amp;= 1 - p(x=1 \\\\vert D) &amp;= \\\\frac {N-m+b}{N+a+b}  \\\\end{aligned}\\\\]                    Then we can see, in the limit of an infinitely large data set $m,N\\\\rightarrow \\\\infty$, this result reduces to the maximum likelihood result $\\\\frac mN$.                    It is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set.                    For a finite data set, the posterior distribution mean for $\\\\mu$ always lie between prior mean and the maximum likelihood estimate for $\\\\mu$ corresponding to the relative frequencies of events.            # Beta Distributionsimport numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import betaparameters = [(0.1,0.3), (1,1), (8,4), (200,150)]fig, axs = plt.subplots(1,4, figsize=(24,6))for i in range(4):    a, b = parameters[i]    ax = axs[i]        # mu = np.linspace(beta.ppf(0, a, b), beta.ppf(1, a, b), 100)    mu = np.linspace(0,1, 100)    ax.plot(mu, beta.pdf(mu, a, b),           &#39;r&#39;, lw=3, alpha=0.8, label=&#39;beta pdf&#39;)    ax.set_xlim((0,1))    ax.set_title(&quot;a={}, b={}&quot;.format(a,b))    ax.set_xlabel(&quot;$\\\\mu$&quot;)plt.show()      We can see that as $a\\\\rightarrow \\\\infty$, $b\\\\rightarrow \\\\infty$, the beta distribution becomes more sharply peaked, because the variance goes to zero for $a\\\\rightarrow \\\\infty$ or $b\\\\rightarrow \\\\infty$.        Also in the posterior distribution of $\\\\mu$, as the number of observations increases, the posterior distribution becomes more sharply peaked. And the variance of posterior distribution decreases, that means the uncertainty represented by the posterior distribution will also decrease.        But is that a general property of Bayesian learning, we can see in the following:                  To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold.            Consider a gengeral Bayesian inference problem for a parameter $\\\\theta$ for which we have observed a data set $D$, described by the joint distribution $p(\\\\theta, D)$.              Firstly: we can see that the posterior mean of $\\\\theta$, averaged over the distribution generation the data, is equal to the prior mean of $\\\\theta$.\\\\[\\\\begin{aligned} \\\\mathbb{E}_\\\\theta[\\\\theta] &amp;= \\\\mathbb{E}_D[\\\\mathbb{E}_\\\\theta[\\\\theta\\\\vert D]] \\\\\\\\ Proof: \\\\qquad&amp;  \\\\\\\\ \\\\mathbb{E}_D[\\\\mathbb{E}_\\\\theta[\\\\theta\\\\vert D]] &amp;= \\\\int \\\\left[ \\\\int \\\\theta p(\\\\theta\\\\vert D)d\\\\theta \\\\right]p(D) dD \\\\\\\\ &amp;= \\\\int \\\\theta p(\\\\theta) d\\\\theta \\\\\\\\ &amp;= \\\\mathbb{E}_\\\\theta[\\\\theta] \\\\end{aligned}\\\\]                    And then, we can see the prior variance of $\\\\theta$ is equal to the average posterior variance of $\\\\theta$ plus to the variance of posterior mean of $\\\\theta$.\\\\[\\\\begin{aligned} \\\\mathrm{var}_{\\\\theta}[\\\\theta] &amp;= \\\\mathbb{E}_{D}[\\\\mathrm{var}_\\\\theta[\\\\theta\\\\vert D]] + \\\\mathrm{var}_{D}[\\\\mathbb{E}_\\\\theta[\\\\theta\\\\vert D]] \\\\\\\\ &amp;\\\\ge \\\\mathbb{E}_{D}[\\\\mathrm{var}_\\\\theta[\\\\theta\\\\vert D]] \\\\end{aligned}\\\\]                    It means that, on average, the variance of posterior distribution is smaller than the prior variance. And the reduction is greater if the variance in the posterior mean is greater.            Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance.      The generalization of the Bernouli distribution to multinomial Variables      For the discrete variables that can take on one of $K$ possible mutually exclusive state.        Using 1-of-K scheme to represent the $\\\\boldsymbol{x}$ by a K-dimensional vector.        Then we can write the distribution of $\\\\boldsymbol{x}$ by:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}\\\\vert \\\\boldsymbol{\\\\mu}) &amp;= \\\\prod_{k=1}^K \\\\mu_i^{x_k} \\\\\\\\ \\\\sum_k^K x_k &amp;= 1 \\\\\\\\ \\\\sum_k^K \\\\mu_i &amp;= 1 \\\\end{aligned}\\\\]        The likelihood function of the distribution of $\\\\boldsymbol{x}$ with a data set $D$ of $N$ independent observations $D = {\\\\mathbf{x}_1, \\\\mathbf{x}_2, \\\\cdots, \\\\mathbf{x}_N}$:\\\\[\\\\begin{aligned} p(D\\\\vert \\\\boldsymbol{\\\\mu} ) = \\\\prod_i^N\\\\prod_k^K \\\\mu_k^{x_{ik}} = \\\\prod_k^K \\\\mu_k^{\\\\sum_i x_{ik}} =  \\\\prod_k^K \\\\mu_k^{m_k} \\\\end{aligned}\\\\]        The Maximum likelihood solution of the $\\\\boldsymbol{\\\\mu}$ can be found by using the Lagrange multiplier $\\\\lambda$ and maximizing\\\\[\\\\begin{aligned} L = \\\\sum_{k=1}^K m_k \\\\ln \\\\mu_k + \\\\lambda(\\\\sum_{k=1}^K \\\\mu_k - 1) \\\\end{aligned}\\\\]                  then\\\\[\\\\begin{aligned} \\\\frac {\\\\partial L}{\\\\partial \\\\mu_k} &amp;= \\\\frac {m_k}{\\\\mu_k} + \\\\lambda = 0 \\\\\\\\ \\\\mu_k &amp;= -\\\\frac {m_k}{\\\\lambda} \\\\\\\\ \\\\\\\\ \\\\Rightarrow \\\\space \\\\sum_{k=1}^K -\\\\frac {m_k}{\\\\lambda} &amp;= 1 \\\\\\\\ \\\\Rightarrow \\\\space \\\\lambda &amp;= -N \\\\\\\\ \\\\\\\\ \\\\Rightarrow \\\\space \\\\mu_k &amp;= \\\\frac {m_k}{N} \\\\end{aligned}\\\\]            Multinomial Distribution      The joint distribution of $m_1m_2\\\\cdots m_K$ is given by:    \\\\(\\\\begin{aligned} \\\\mathrm{Mult}(m_1m_2\\\\cdots m_K \\\\vert N, \\\\boldsymbol{\\\\mu}) = \\\\binom{N}{m_1m_2\\\\cdots m_K} \\\\prod_{k=1}^K \\\\mu_k^{m_k} \\\\end{aligned}\\\\)          This is called multinomial distribution.      Dirichlet Distribution      we now introduce a prior distribution  for the parameters $\\\\boldsymbol{\\\\mu} = {\\\\mu_1, \\\\mu_2, \\\\cdots, \\\\mu_K}$ of the multinomial distribution.        By inspection of the form of the multinomial distribution, we see that the conjugate prior is given by\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) \\\\propto \\\\prod_{k=1}^K \\\\mu_{k}^{\\\\alpha_k-1} \\\\end{aligned}\\\\]          Because of the summation constraint $\\\\sum_{k}\\\\mu_k = 1$, the distribution over the space of the ${\\\\mu_1, \\\\mu_2, \\\\cdots, \\\\mu_K}$ is confined to a simplex of dimensionality $K-1$.            The normalized form fot this distribution is by\\\\[\\\\begin{aligned} \\\\mathrm{Dir}(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) = \\\\frac {\\\\Gamma(\\\\sum_k \\\\alpha_k)}{\\\\prod_k \\\\Gamma(\\\\alpha_k)} \\\\prod_{k=1}^K \\\\mu_{k}^{\\\\alpha_k-1} \\\\end{aligned}\\\\]                  so that:\\\\[\\\\begin{aligned} \\\\int_0^1 \\\\mathrm{Dir}(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) d\\\\boldsymbol{\\\\mu} = 1 \\\\end{aligned}\\\\]                  Mean\\\\[\\\\begin{aligned} \\\\mathbb{E}[\\\\mu_i] = \\\\frac {\\\\alpha_i}{\\\\sum_k \\\\alpha_k}  \\\\end{aligned}\\\\]        Variance    \\\\(\\\\begin{aligned} \\\\mathrm{var}[\\\\mu_i] &amp;= \\\\frac {\\\\alpha_i((\\\\sum_k \\\\alpha^k) - \\\\alpha_i)}{((\\\\sum_k \\\\alpha^k)+1)\\\\left(\\\\sum_k \\\\alpha^k\\\\right)^2} \\\\\\\\ &amp;= \\\\frac {\\\\alpha_i(\\\\alpha_0 - \\\\alpha_i)}{\\\\alpha_0^2(\\\\alpha_0 + 1)} \\\\end{aligned}\\\\)          Here we have let $\\\\alpha_0 = \\\\sum_k \\\\alpha_k$            Covariance          $\\\\forall \\\\space i\\\\neq j$    \\\\[\\\\begin{aligned} \\\\mathrm{Cov}[\\\\mu_i, \\\\mu_j] &amp;= \\\\mathbb{E}[(\\\\mu_i - \\\\frac {\\\\alpha_i}{\\\\alpha_0})(\\\\mu_j - \\\\frac {\\\\alpha_j}{\\\\alpha_0})] \\\\\\\\ &amp;= \\\\frac {\\\\alpha_i\\\\alpha_j}{\\\\alpha_0(\\\\alpha_0+1) - \\\\frac {\\\\alpha_i\\\\alpha_j}{\\\\alpha_0^2}} \\\\\\\\ &amp;=  \\\\frac {-\\\\alpha_i\\\\alpha_j}{\\\\alpha_0^2(\\\\alpha_0+1)} \\\\end{aligned}\\\\]        The Posterior Distribution\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu} \\\\vert D, \\\\boldsymbol{\\\\alpha}) \\\\propto p(D\\\\vert \\\\boldsymbol{\\\\mu}) p(\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha}) \\\\propto \\\\prod_{k=1}^K \\\\mu_k^{m_k+\\\\alpha_k-1}  \\\\end{aligned}\\\\]                  Then we see the posterior distribution again takes the form of dirichlet distribution, so it is indeed a conjugate prior for the multinomial.\\\\[\\\\begin{aligned} p(\\\\boldsymbol{\\\\mu} \\\\vert D, \\\\boldsymbol{\\\\alpha}) &amp;= \\\\mathrm{Dir} (\\\\boldsymbol{\\\\mu} \\\\vert \\\\boldsymbol{\\\\alpha} + \\\\boldsymbol{m}) \\\\\\\\ &amp;= \\\\frac {\\\\Gamma(\\\\sum_k (\\\\alpha_k +m_k))}{\\\\prod_k \\\\Gamma(\\\\alpha_k+m_k)} \\\\prod_{k=1}^K \\\\mu_k^{m_k+\\\\alpha_k-1} \\\\\\\\&amp;= \\\\frac {\\\\Gamma(N + \\\\sum_k \\\\alpha_k )}{\\\\prod_k \\\\Gamma(\\\\alpha_k+m_k)} \\\\prod_{k=1}^K \\\\mu_k^{m_k+\\\\alpha_k-1} \\\\end{aligned}\\\\]            "
  },
  
  {
    "title": "Bayes&#39; Theorem for Gaussian Variables",
    "url": "/posts/bayes_theorem_for_gaussian_variables/",
    "categories": "Probability",
    "tags": "Gaussian Distribution",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Bayes’ Theorem for Gaussian Variables      Given the distribution of $p(\\\\boldsymbol{x})$ and $p(\\\\boldsymbol{y} \\\\vert \\\\boldsymbol{x})$, they both are gaussian distribution, like this:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{x}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{\\\\mu}, \\\\Lambda^{-1}) \\\\\\\\p(\\\\boldsymbol{y} \\\\vert \\\\boldsymbol{x}) &amp;= \\\\mathcal{N}(\\\\boldsymbol{y} \\\\vert A\\\\boldsymbol{x} + b, L^{-1}) \\\\end{aligned}\\\\]          This is a example of a linear gaussian model.      Problem 1: Find the joint distribution $p(\\\\boldsymbol{x}, \\\\boldsymbol{y})$      We have know the distribution of $\\\\boldsymbol{x,y}$ must be gaussian distribution, so let:\\\\[\\\\begin{aligned} \\\\boldsymbol{z} = \\\\binom{\\\\boldsymbol{x}}{\\\\boldsymbol{y}} = \\\\mathcal{N}(\\\\boldsymbol{z} \\\\vert \\\\boldsymbol{\\\\mu}_z, \\\\Sigma_z) \\\\end{aligned}\\\\]        Just need consider the quadratic form of those distribution:\\\\[\\\\begin{aligned} p(\\\\boldsymbol{z}) \\\\propto p(\\\\boldsymbol{y} \\\\vert \\\\boldsymbol{x})p(\\\\boldsymbol{x}) \\\\end{aligned}\\\\]                  Then:\\\\[\\\\begin{aligned} -\\\\frac 12 (\\\\boldsymbol{z}-\\\\boldsymbol{\\\\mu}_z)^T\\\\Sigma_z^{-1}(\\\\boldsymbol{z}-\\\\boldsymbol{\\\\mu}_z) &amp;= -\\\\frac 12 \\\\boldsymbol{z}^T\\\\Sigma_z^{-1}\\\\boldsymbol{z} +\\\\boldsymbol{z}^T\\\\Sigma_z^{-1}\\\\boldsymbol{\\\\mu}_z + \\\\mathrm{const}_0   \\\\\\\\ &amp;= -\\\\frac 12 (\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu})^T\\\\Lambda(\\\\boldsymbol{x}-\\\\boldsymbol{\\\\mu}) -\\\\frac 12 (\\\\boldsymbol{y}-A\\\\boldsymbol{x}-b)^T L((\\\\boldsymbol{y}-A\\\\boldsymbol{x}-b)) + \\\\mathrm{const}_1 \\\\\\\\ &amp;= -\\\\frac 12 \\\\boldsymbol{x}^T(\\\\Lambda+A^T L A)\\\\boldsymbol{x} -\\\\frac 12 \\\\boldsymbol{y}^TL\\\\boldsymbol{y} +\\\\frac 12\\\\boldsymbol{x}^T A^TL \\\\boldsymbol{y}^T + \\\\frac 12\\\\boldsymbol{y}^TLA\\\\boldsymbol{x} \\\\\\\\ &amp; + \\\\boldsymbol{x}^T(\\\\Lambda\\\\boldsymbol{\\\\mu}-A^TLb) + \\\\boldsymbol{y}^TLb \\\\\\\\&amp;+ \\\\mathrm{const}_2 \\\\\\\\ &amp;= -\\\\frac 12 \\\\binom{\\\\boldsymbol{x}}{\\\\boldsymbol{y}}^T \\\\binom{\\\\Lambda+A^T L A \\\\qquad -A^TL}{-LA \\\\qquad L} \\\\binom{\\\\boldsymbol{x}}{\\\\boldsymbol{y}} + \\\\binom{\\\\boldsymbol{x}}{\\\\boldsymbol{y}}^T \\\\binom{\\\\Lambda\\\\boldsymbol{\\\\mu}-A^TLb}{Lb} + \\\\mathrm{const}_2 \\\\end{aligned}\\\\]                    Thus:\\\\[\\\\begin{aligned} \\\\mathrm{cov}[\\\\boldsymbol{z}] = \\\\Sigma_z &amp;= \\\\binom{\\\\Lambda+A^T L A \\\\qquad -A^TL}{-LA \\\\qquad L}^{-1} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[\\\\boldsymbol{z}] = \\\\boldsymbol{\\\\mu}_z &amp;= \\\\Sigma_z\\\\binom{\\\\Lambda\\\\boldsymbol{\\\\mu}-A^TLb}{Lb} = \\\\binom{\\\\Lambda+A^T L A \\\\qquad -A^TL}{-LA \\\\qquad L}^{-1} \\\\binom{\\\\Lambda\\\\boldsymbol{\\\\mu}-A^TLb}{Lb} \\\\\\\\\\\\\\\\ \\\\end{aligned}\\\\]                    Using the inverse of the partitioned matrix equation, we have:\\\\[\\\\begin{aligned} \\\\mathrm{cov}[\\\\boldsymbol{z}] =  \\\\Sigma_z &amp;= \\\\left(\\\\begin{matrix}\\\\Lambda^{-1}&amp; \\\\space \\\\Lambda^{-1}A^T \\\\\\\\ A\\\\Lambda^{-1}&amp; \\\\space L^{-1}+A\\\\Lambda^{-1}A^T \\\\end{matrix} \\\\right) \\\\\\\\ \\\\\\\\  \\\\mathbb{E}[\\\\boldsymbol{z}] = \\\\boldsymbol{\\\\mu}_z &amp;= \\\\left(\\\\begin{matrix} \\\\boldsymbol{\\\\mu}\\\\\\\\ A\\\\boldsymbol{\\\\mu}+b \\\\end{matrix} \\\\right) \\\\\\\\\\\\\\\\  \\\\end{aligned}\\\\]            Problem 2: Find the marginal distribution $p(\\\\boldsymbol{y})$\\\\[\\\\begin{aligned} p(\\\\boldsymbol{y}) = \\\\int p(\\\\boldsymbol{x}, \\\\boldsymbol{y}) d\\\\boldsymbol{x} \\\\end{aligned}\\\\]      According to the marginal gaussian distribution results ,we have:\\\\[\\\\begin{aligned} \\\\mathrm{cov}[\\\\boldsymbol{y}] &amp;= L^{-1}+A\\\\Lambda^{-1}A^T \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[\\\\boldsymbol{y}] &amp;= A\\\\boldsymbol{\\\\mu} +b \\\\\\\\\\\\\\\\ \\\\end{aligned}\\\\]  Problem 3: Find the conditional distribution $p(\\\\boldsymbol{x} \\\\vert \\\\boldsymbol{y})$      According to the conditional gaussian distribution results ,we have:\\\\[\\\\begin{aligned} \\\\mathrm{cov}[\\\\boldsymbol{x}] &amp;= (\\\\Lambda+A^T L )^{-1} \\\\\\\\ \\\\\\\\ \\\\mathbb{E}[\\\\boldsymbol{x}] &amp;= \\\\boldsymbol{\\\\mu} + (\\\\Lambda+A^T L )^{-1}A^TL(y-A\\\\boldsymbol{\\\\mu} -b)\\\\\\\\&amp;= \\\\boldsymbol{\\\\mu} - (\\\\Lambda+A^T L )^{-1}A^TLA\\\\boldsymbol{\\\\mu} + (\\\\Lambda+A^T L )^{-1}A^TL(y-b) \\\\\\\\ &amp;= (I - (\\\\Lambda+A^T L )^{-1}A^TLA)\\\\boldsymbol{\\\\mu} + (\\\\Lambda+A^T L )^{-1}A^TL(y-b) \\\\\\\\ &amp;= (\\\\Lambda+A^T L )^{-1}\\\\Lambda\\\\boldsymbol{\\\\mu} + (\\\\Lambda+A^T L )^{-1}A^TL(y-b) \\\\\\\\ &amp;= (\\\\Lambda+A^T L )^{-1}\\\\left(A^TL(y-b)+\\\\Lambda \\\\boldsymbol{\\\\mu}\\\\right) \\\\\\\\\\\\\\\\  \\\\end{aligned}\\\\]  "
  },
  
  {
    "title": "Information Theorem",
    "url": "/posts/Information_Theory/",
    "categories": "Information Theory, Basic Concept",
    "tags": "Entropy",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Information Theory      Information Content or Self-Information  \\\\(\\\\begin{aligned} h(x) &amp;= -\\\\log_2 p(x) \\\\\\\\ h(x) &amp;= -\\\\ln p(x) \\\\end{aligned}\\\\)        information Entropy: The average amount of information needed to specify the  state of a random variable. (The average amount of information that they transmit in the process)\\\\[\\\\begin{aligned} H[X] &amp;=  \\\\mathbb{E}_{X}[h(x)] = -\\\\sum_{x\\\\in X} p(x)\\\\log_2 p(x) \\\\qquad bits \\\\\\\\ H[X]&amp;=  \\\\mathbb{E}_{X}[h(x)] = -\\\\sum_{x\\\\in X} p(x)\\\\ln p(x) \\\\qquad nats \\\\end{aligned}\\\\]          Distributions $p(x)$ that are sharply peaked around a few values will have a relatively low entropy, where those that are speread more evenly across many values will have higher entropy.            Differential Entropy\\\\[H(X) = -\\\\int p(x)\\\\ln p(x)\\\\]          To specify a continuous variable very precisely requires a large number of bits        Maximum Entropy                  Discrete variables: if $X$ is discrete variable, then $H(X)$ will have a maximum value when $p(x)$ is a uniform distribution. Proof with following:\\\\[\\\\begin{aligned} \\\\max &amp; \\\\qquad -\\\\sum_{x\\\\in X} p(x) \\\\ln p(x) \\\\\\\\ s.t. &amp; \\\\qquad \\\\sum_{x} p(x) = 1, x\\\\in X  \\\\end{aligned}\\\\]                  To simplify notion, we assume $X = {x_1, x_2, \\\\cdots, x_K}$, and let $p_i = p(x_i)$, $\\\\mathbf{p} = {p_1,p_2,\\\\cdots,p_K}$          Then introduce a Lagrange multiplier $\\\\lambda \\\\neq 0$, we obtained a Lagrange function:  \\\\(\\\\begin{aligned} L(\\\\mathbf{p}, \\\\lambda) &amp;= -\\\\sum_{i=1}^K p_i \\\\ln p_i + \\\\lambda(\\\\sum_{i=1}^K p_i - 1)\\\\end{aligned}\\\\)                      and then: \\\\(\\\\begin{aligned} &amp; \\\\frac {\\\\partial L}{\\\\partial p_i} = -\\\\ln p_i - 1 + \\\\lambda = 0 \\\\\\\\ \\\\Leftrightarrow &amp; p_i = \\\\exp(\\\\lambda - 1) \\\\end{aligned}\\\\)                                back substitution of this result into constraint equations, we have:\\\\[\\\\begin{aligned}\\\\lambda = 1 + \\\\ln \\\\frac 1N \\\\end{aligned}\\\\]                                finally, we have:\\\\[\\\\begin{aligned} p_i = \\\\frac 1N, \\\\qquad i=1,2,\\\\cdots, K \\\\end{aligned}\\\\]                                      Continuous Variable: if $X$ is continuous variable, then $H(X)$ will have a maximum value when $p(x)$ is a gaussian distribution. Proof with following:\\\\[\\\\begin{aligned} \\\\max &amp;\\\\qquad -\\\\int p(x) \\\\ln p(x) dx \\\\\\\\ s.t. &amp;\\\\qquad \\\\int p(x)dx = 1 \\\\\\\\ &amp;\\\\qquad \\\\int xp(x) dx = \\\\mu \\\\\\\\ &amp;\\\\qquad \\\\int (x-\\\\mu)^2xp(x) = \\\\sigma^2 \\\\end{aligned}\\\\]                              Then introduce Lagrange multipliers $\\\\lambda_1,\\\\lambda_2,\\\\lambda_3$, at least one of them not equal zero, we obtained a Lagrange function:\\\\[\\\\begin{aligned} L(p(x), \\\\lambda_1,\\\\lambda_2,\\\\lambda_3) &amp;= -\\\\int p(x) \\\\ln p(x) dx + \\\\lambda_1(\\\\int p(x)dx - 1) + \\\\lambda_2 (\\\\int xp(x) dx - \\\\mu) + \\\\lambda_3(\\\\int (x-\\\\mu)^2p(x) - \\\\sigma^2) \\\\\\\\ &amp;= -\\\\int p(x) \\\\left[ \\\\ln p(x)-\\\\lambda_1-\\\\lambda_2 x - \\\\lambda_3 (x-\\\\mu)^2 \\\\right] \\\\end{aligned}\\\\]                                Using the calculus of variations::\\\\[\\\\begin{aligned} \\\\frac {\\\\delta L}{\\\\delta p} = \\\\ln p(x) - \\\\lambda_1 -\\\\lambda_2 x - \\\\lambda_3 (x-\\\\mu)^2 + 1 =0 \\\\end{aligned}\\\\]                                Then we have:\\\\[\\\\begin{aligned} p(x) = \\\\exp(\\\\lambda_1 + \\\\lambda_2 x + \\\\lambda_3 (x-\\\\mu)^2 - 1) \\\\end{aligned}\\\\]                                So we can see $p(x)$ shall be a gaussian distribution, and\\\\[\\\\begin{aligned} p(x) &amp;= \\\\exp(\\\\lambda_1 + \\\\lambda_2 x + \\\\lambda_3 (x-\\\\mu)^2 - 1)  = \\\\frac {1}{\\\\sqrt{2\\\\pi \\\\sigma^2}} \\\\exp\\\\left( \\\\frac 1{2\\\\sigma^2} (x-\\\\mu)^2 \\\\right) \\\\end{aligned}\\\\]                                    Joint Entropy  \\\\(\\\\begin{aligned} H(X,Y) = \\\\mathbb{E}_{X,Y} [-\\\\ln p(x,y)] = \\\\left\\\\{\\\\begin{aligned} &amp;-\\\\sum_{x}\\\\sum_{y} p(x,y)\\\\ln p(x,y) \\\\\\\\ &amp;-\\\\int\\\\int p(x,y)\\\\ln p(x,y)dxdy \\\\end{aligned}\\\\right.  \\\\end{aligned}\\\\)        Conditional Entropy  \\\\(\\\\begin{aligned} H(Y\\\\vert X) = \\\\mathbb{E}_{X,Y} [-\\\\ln p(y\\\\vert x)] = \\\\left\\\\{\\\\begin{aligned} &amp;-\\\\sum_{x}\\\\sum_{y} p(x,y)\\\\ln p(y\\\\vert x) \\\\\\\\ &amp;-\\\\int\\\\int p(x,y)\\\\ln p(y\\\\vert x)dxdy \\\\end{aligned}\\\\right.  \\\\end{aligned}\\\\)          $H(Y\\\\vert X) = H(X, Y) - H(X)$  \\\\(\\\\begin{aligned} H(X,Y) &amp;= -\\\\int\\\\int p(x,y)\\\\ln p(x,y)dxdy \\\\\\\\ &amp;= -\\\\int\\\\int p(x,y)\\\\ln p(y\\\\vert x)dxdy  - \\\\int\\\\int p(x,y)\\\\ln p(x)dxdy \\\\\\\\&amp;= H(Y\\\\vert X) + H(X)  \\\\end{aligned}\\\\)            Mutual Information: Mutual information measures the amount of information that can be obtained about one random variable by observing another.\\\\[\\\\begin{aligned} I(X,Y) &amp;= I(Y,X) \\\\\\\\&amp;= H(X) + H(Y) - H(X,Y) \\\\\\\\&amp;= H(Y) - H(Y\\\\vert X) \\\\\\\\&amp;= H(X) - H(X\\\\vert Y) \\\\\\\\&amp;= \\\\left\\\\{\\\\begin{aligned} &amp;-\\\\sum_{x}\\\\sum_{y} p(x,y)\\\\ln \\\\left(\\\\frac {p(x)p(y)}{p(x,y)}\\\\right) \\\\\\\\ &amp;-\\\\int\\\\int p(x,y)\\\\ln \\\\left(\\\\frac {p(x)p(y)}{p(x,y)}\\\\right)dxdy \\\\end{aligned}\\\\right. \\\\end{aligned}\\\\]        Cross Entropy: It describes the difficulty of expressing probability distribution p through probability distribution q.          Consider two distribution $p(X), q(X)$: i.e. consider some unknown distribution $p(X)$, and suppose that we have modelled this using an approximating distribution $q(X)$.    \\\\[\\\\begin{aligned} H(p,q) = -\\\\mathbb{E}_{p}[\\\\log q] = \\\\left\\\\{\\\\begin{aligned} &amp;-\\\\sum_{x}p(x)\\\\ln q(x) \\\\\\\\ &amp;-\\\\int p(x)\\\\ln q(x)dx \\\\end{aligned}\\\\right.  \\\\end{aligned}\\\\]        Relative Entropy or KL divergence (Kullback-Leibler divergence) or Information Gain: KL divergence is a way of comparing two distributions    \\\\(\\\\begin{aligned} KL(p\\\\vert\\\\vert q) = H(p,q) - H(p) = \\\\left\\\\{\\\\begin{aligned} &amp; -\\\\sum_{x}p(x)\\\\ln \\\\left(\\\\frac {q(x)}{p(x)}\\\\right) \\\\\\\\ &amp;-\\\\int p(x)\\\\ln \\\\left(\\\\frac {q(x)}{p(x)}\\\\right)dx \\\\end{aligned}\\\\right. \\\\end{aligned}\\\\)          realtion with mutual information:  \\\\(\\\\begin{aligned} I(X,Y) = KL\\\\left( p(x,y) \\\\vert\\\\vert p(x)p(y) \\\\right) \\\\end{aligned}\\\\)      Attention:  \\\\(H(p,q) - H(p) = KL(p\\\\vert\\\\vert q) \\\\neq KL(q\\\\vert\\\\vert p) = H(q,p) - H(q)\\\\)      "
  },
  
  {
    "title": "Bitwise_operator",
    "url": "/posts/bitwise_operator/",
    "categories": "",
    "tags": "",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "Bitwise OperatorNot ~0和1互换\\\\[~(~x) = x\\\\]Note: 计算中，所有正数最高位都是1，因此计算中正数的非运算都是And &amp;有0则0Or |有1则1XOR ^同0异1"
  },
  
  {
    "title": "什么是P问题、NP问题和NPC问题",
    "url": "/posts/Problem-P-NP-NPC-NPhard/",
    "categories": "Blogs, 复杂度",
    "tags": "NP-problem",
    "date": "2021-02-20 00:00:00 +0000",
    





    "snippet": "什么是P问题、NP问题和NPC问题(转载注：全文转载自matrix67， 这是一个很NB的关于NP等概念的阐述)这或许是众多OIer最大的误区之一。你会经常看到网上出现“这怎么做，这不是NP问题吗”、“这个只有搜了，这已经被证明是NP问题了”之类的话。你要知道，大多数人此时所说的NP问题其实都是指的NPC问题。他们没有搞清楚NP问题和NPC问题的概念。NP问题并不是那种“只有搜才行”的问题，NPC问题才是。好，行了，基本上这个误解已经被澄清了。下面的内容都是在讲什么是P问题，什么是NP问题，什么是NPC问题，你如果不是很感兴趣就可以不看了。接下来你可以看到，把NP问题当成是 NPC问题是一个多大的错误。还是先用几句话简单说明一下时间复杂度。时间复杂度并不是表示一个程序解决问题需要花多少时间，而是当问题规模扩大后，程序需要的时间长度增长得有多快。也就是说，对于高速处理数据的计算机来说，处理某一个特定数据的效率不能衡量一个程序的好坏，而应该看当这个数据的规模变大到数百倍后，程序运行时间是否还是一样，或者也跟着慢了数百倍，或者变慢了数万倍。不管数据有多大，程序处理花的时间始终是那么多的，我们就说这个程序很好，具有O(1)的时间复杂度，也称常数级复杂度；数据规模变得有多大，花的时间也跟着变得有多长，这个程序的时间复杂度就是O(n)，比如找n个数中的最大值；而像冒泡排序、插入排序等，数据扩大2倍，时间变慢4倍的，属于O(n^2)的复杂度。还有一些穷举类的算法，所需时间长度成几何阶数上涨，这就是O(a^n)的指数级复杂度，甚至O(n!)的阶乘级复杂度。不会存在O(2n^2)的复杂度，因为前面的那个“2”是系数，根本不会影响到整个程序的时间增长。同样地，O (n^3+n^2)的复杂度也就是O(n^3)的复杂度。因此，我们会说，一个O(0.01n^3)的程序的效率比O(100*n^2)的效率低，尽管在n很小的时候，前者优于后者，但后者时间随数据规模增长得慢，最终O(n^3)的复杂度将远远超过O(n^2)。我们也说，O(n^100)的复杂度小于O(1.01^n)的复杂度。容易看出，前面的几类复杂度被分为两种级别，其中后者的复杂度无论如何都远远大于前者：一种是O(1),O(log(n)),O(n^a)等，我们把它叫做多项式级的复杂度，因为它的规模n出现在底数的位置；另一种是O(a^n)和O(n!)型复杂度，它是非多项式级的，其复杂度计算机往往不能承受。当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。自然地，人们会想到一个问题：会不会所有的问题都可以找到复杂度为多项式级的算法呢？很遗憾，答案是否定的。有些问题甚至根本不可能找到一个正确的算法来，这称之为“不可解问题”(Undecidable Decision Problem)。The Halting Problem就是一个著名的不可解问题，在我的Blog上有过专门的介绍和证明。再比如，输出从1到n这n个数的全排列。不管你用什么方法，你的复杂度都是阶乘级，因为你总得用阶乘级的时间打印出结果来。有人说，这样的“问题”不是一个“正规”的问题，正规的问题是让程序解决一个问题，输出一个“YES”或“NO”（这被称为判定性问题），或者一个什么什么的最优值（这被称为最优化问题）。那么，根据这个定义，我也能举出一个不大可能会有多项式级算法的问题来：Hamilton回路。问题是这样的：给你一个图，问你能否找到一条经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路（满足这个条件的路径叫做Hamilton回路）。这个问题现在还没有找到多项式级的算法。事实上，这个问题就是我们后面要说的NPC问题。下面引入P类问题的概念：如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于P问题。P是英文单词多项式的第一个字母。哪些问题是P类问题呢？通常NOI和NOIP不会出不属于P类问题的题目。我们常见到的一些信息奥赛的题目都是P问题。道理很简单，一个用穷举换来的非多项式级时间的超时程序不会涵盖任何有价值的算法。接下来引入NP问题的概念。这个就有点难理解了，或者说容易理解错误。在这里强调（回到我竭力想澄清的误区上），NP问题不是非P类问题。NP问题是指可以在多项式的时间里验证一个解的问题。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。比方说，我RP很好，在程序中需要枚举时，我可以一猜一个准。现在某人拿到了一个求最短路径的问题，问从起点到终点是否有一条小于100个单位长度的路线。它根据数据画好了图，但怎么也算不出来，于是来问我：你看怎么选条路走得最少？我说，我RP很好，肯定能随便给你指条很短的路出来。然后我就胡乱画了几条线，说就这条吧。那人按我指的这条把权值加起来一看，嘿，神了，路径长度98，比100小。于是答案出来了，存在比100小的路径。别人会问他这题怎么做出来的，他就可以说，因为我找到了一个比100 小的解。在这个题中，找一个解很困难，但验证一个解很容易。验证一个解只需要O(n)的时间复杂度，也就是说我可以花O(n)的时间把我猜的路径的长度加出来。那么，只要我RP好，猜得准，我一定能在多项式的时间里解决这个问题。我猜到的方案总是最优的，不满足题意的方案也不会来骗我去选它。这就是NP问题。当然有不是NP问题的问题，即你猜到了解但是没用，因为你不能在多项式的时间里去验证它。下面我要举的例子是一个经典的例子，它指出了一个目前还没有办法在多项式的时间里验证一个解的问题。很显然，前面所说的Hamilton回路是NP问题，因为验证一条路是否恰好经过了每一个顶点非常容易。但我要把问题换成这样：试问一个图中是否不存在Hamilton回路。这样问题就没法在多项式的时间里进行验证了，因为除非你试过所有的路，否则你不敢断定它“没有Hamilton回路”。之所以要定义NP问题，是因为通常只有NP问题才可能找到多项式的算法。我们不会指望一个连多项式地验证一个解都不行的问题存在一个解决它的多项式级的算法。相信读者很快明白，信息学中的号称最困难的问题——“NP问题”，实际上是在探讨NP问题与P类问题的关系。很显然，所有的P类问题都是NP问题。也就是说，能多项式地解决一个问题，必然能多项式地验证一个问题的解——既然正解都出来了，验证任意给定的解也只需要比较一下就可以了。关键是，人们想知道，是否所有的NP问题都是P类问题。我们可以再用集合的观点来说明。如果把所有P类问题归为一个集合P中，把所有 NP问题划进另一个集合NP中，那么，显然有P属于NP。现在，所有对NP问题的研究都集中在一个问题上，即究竟是否有P=NP？通常所谓的“NP问题”，其实就一句话：证明或推翻P=NP。NP问题一直都是信息学的巅峰。巅峰，意即很引人注目但难以解决。在信息学研究中，这是一个耗费了很多时间和精力也没有解决的终极问题，好比物理学中的大统一和数学中的歌德巴赫猜想等。目前为止这个问题还“啃不动”。但是，一个总的趋势、一个大方向是有的。人们普遍认为，P=NP不成立，也就是说，多数人相信，存在至少一个不可能有多项式级复杂度的算法的NP问题。人们如此坚信P≠NP是有原因的，就是在研究NP问题的过程中找出了一类非常特殊的NP问题叫做NP-完全问题，也即所谓的 NPC问题。C是英文单词“完全”的第一个字母。正是NPC问题的存在，使人们相信P≠NP。下文将花大量篇幅介绍NPC问题，你从中可以体会到NPC问题使P=NP变得多么不可思议。为了说明NPC问题，我们先引入一个概念——约化(Reducibility，有的资料上叫“归约”)。简单地说，一个问题A可以约化为问题B的含义即是，可以用问题B的解法解决问题A，或者说，问题A可以“变成”问题B。《算法导论》上举了这么一个例子。比如说，现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以约化为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为0。按照这个规则把前一个问题转换成后一个问题，两个问题就等价了。同样地，我们可以说，Hamilton回路可以约化为TSP问题(Travelling Salesman Problem，旅行商问题)：在Hamilton回路问题中，两点相连即这两点距离为0，两点不直接相连则令其距离为1，于是问题转化为在TSP问题中，是否存在一条长为0的路径。Hamilton回路存在当且仅当TSP问题中存在长为0的回路。“问题A可约化为问题B”有一个重要的直观意义：B的时间复杂度高于或者等于A的时间复杂度。也就是说，问题A不比问题B难。这很容易理解。既然问题A能用问题B来解决，倘若B的时间复杂度比A的时间复杂度还低了，那A的算法就可以改进为B的算法，两者的时间复杂度还是相同。正如解一元二次方程比解一元一次方程难，因为解决前者的方法可以用来解决后者。很显然，约化具有一项重要的性质：约化具有传递性。如果问题A可约化为问题B，问题B可约化为问题C，则问题A一定可约化为问题C。这个道理非常简单，就不必阐述了。现在再来说一下约化的标准概念就不难理解了：如果能找到这样一个变化法则，对任意一个程序A的输入，都能按这个法则变换成程序B的输入，使两程序的输出相同，那么我们说，问题A可约化为问题B。当然，我们所说的“可约化”是指的可“多项式地”约化(Polynomial-time Reducible)，即变换输入的方法是能在多项式的时间里完成的。约化的过程只有用多项式的时间完成才有意义。好了，从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。再回想前面讲的P和NP问题，联想起约化的传递性，自然地，我们会想问，如果不断地约化上去，不断找到能“通吃”若干小NP问题的一个稍复杂的大NP问题，那么最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP问题的这样一个超级NP问题？答案居然是肯定的。也就是说，存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。这一类问题就是传说中的NPC 问题，也就是NP-完全问题。NPC问题的出现使整个NP问题的研究得到了飞跃式的发展。我们有理由相信，NPC问题是最复杂的问题。再次回到全文开头，我们可以看到，人们想表达一个问题不存在多项式的高效算法时应该说它“属于NPC问题”。此时，我的目的终于达到了，我已经把NP问题和NPC问题区别开了。到此为止，本文已经写了近5000字了，我佩服你还能看到这里来，同时也佩服一下自己能写到这里来。NPC问题的定义非常简单。同时满足下面两个条件的问题就是NPC问题。首先，它得是一个NP问题；然后，所有的NP问题都可以约化到它。证明一个问题是 NPC问题也很简单。先证明它至少是一个NP问题，再证明其中一个已知的NPC问题能约化到它（由约化的传递性，则NPC问题定义的第二条也得以满足；至于第一个NPC问题是怎么来的，下文将介绍），这样就可以说它是NPC问题了。既然所有的NP问题都能约化成NPC问题，那么只要任意一个NPC问题找到了一个多项式的算法，那么所有的NP问题都能用这个算法解决了，NP也就等于P 了。因此，给NPC找一个多项式算法太不可思议了。因此，前文才说，“正是NPC问题的存在，使人们相信P≠NP”。我们可以就此直观地理解，NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。顺便讲一下NP-Hard问题。NP-Hard问题是这样一种问题，它满足NPC问题定义的第二条但不一定要满足第一条（就是说，NP-Hard问题要比 NPC问题的范围广）。NP-Hard问题同样难以找到多项式的算法，但它不列入我们的研究范围，因为它不一定是NP问题。即使NPC问题发现了多项式级的算法，NP-Hard问题有可能仍然无法得到多项式级的算法。事实上，由于NP-Hard放宽了限定条件，它将有可能比所有的NPC问题的时间复杂度更高从而更难以解决。不要以为NPC问题是一纸空谈。NPC问题是存在的。确实有这么一个非常具体的问题属于NPC问题。下文即将介绍它。下文即将介绍逻辑电路问题。这是第一个NPC问题。其它的NPC问题都是由这个问题约化而来的。因此，逻辑电路问题是NPC类问题的“鼻祖”。逻辑电路问题是指的这样一个问题：给定一个逻辑电路，问是否存在一种输入使输出为True。什么叫做逻辑电路呢？一个逻辑电路由若干个输入，一个输出，若干“逻辑门”和密密麻麻的线组成。看下面一例，不需要解释你马上就明白了。┌───┐│ 输入1├─→┐    ┌──┐└───┘    └─→┤    │                    │ or ├→─┐┌───┐    ┌─→┤    │    │    ┌──┐│ 输入2├─→┤    └──┘    └─→┤    │&amp;nbsp;└───┘    │                ┌─→┤AND ├──→输出            └────────┘┌→┤    │┌───┐    ┌──┐            │  └──┘│ 输入3├─→┤ NOT├─→────┘└───┘    └──┘这是个较简单的逻辑电路，当输入1、输入2、输入3分别为True、True、False或False、True、False时，输出为True。有输出无论如何都不可能为True的逻辑电路吗？有。下面就是一个简单的例子。┌───┐│输入1 ├→─┐    ┌──┐└───┘    └─→┤    │                    │AND ├─→┐            ┌─→┤    │    │            │    └──┘    │  ┌──┐            │                └→┤    │┌───┐    │                    │AND ├─→输出│输入2 ├→─┤  ┌──┐      ┌→┤    │└───┘    └→┤NOT ├→──┘  └──┘                └──┘上面这个逻辑电路中，无论输入是什么，输出都是False。我们就说，这个逻辑电路不存在使输出为True的一组输入。回到上文，给定一个逻辑电路，问是否存在一种输入使输出为True，这即逻辑电路问题。逻辑电路问题属于NPC问题。这是有严格证明的。它显然属于NP问题，并且可以直接证明所有的NP问题都可以约化到它（不要以为NP问题有无穷多个将给证明造成不可逾越的困难）。证明过程相当复杂，其大概意思是说任意一个NP问题的输入和输出都可以转换成逻辑电路的输入和输出（想想计算机内部也不过是一些 0和1的运算），因此对于一个NP问题来说，问题转化为了求出满足结果为True的一个输入（即一个可行解）。有了第一个NPC问题后，一大堆NPC问题就出现了，因为再证明一个新的NPC问题只需要将一个已知的NPC问题约化到它就行了。后来，Hamilton 回路成了NPC问题，TSP问题也成了NPC问题。现在被证明是NPC问题的有很多，任何一个找到了多项式算法的话所有的NP问题都可以完美解决了。因此说，正是因为NPC问题的存在，P=NP变得难以置信。P=NP问题还有许多有趣的东西，有待大家自己进一步的挖掘。攀登这个信息学的巅峰是我们这一代的终极目标。现在我们需要做的，至少是不要把概念弄混淆了。"
  }
  
]

