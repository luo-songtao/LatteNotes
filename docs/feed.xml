

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://0.0.0.0:4000/</id>
  <title>Candy Note</title>
  <subtitle></subtitle>
  <updated>2022-03-27T18:06:19+00:00</updated>
  <author>
    <name>luo-songtao</name>
    <uri>http://0.0.0.0:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://0.0.0.0:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://0.0.0.0:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator>
  <rights> Â© 2022 luo-songtao </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>LeetCode - Dynamic Programming</title>
    <link href="http://0.0.0.0:4000/posts/dynamic_programming/" rel="alternate" type="text/html" title="LeetCode - Dynamic Programming" />
    <published>2022-03-01T00:00:00+00:00</published>
  
    <updated>2022-03-01T00:00:00+00:00</updated>
  
    <id>http://0.0.0.0:4000/posts/dynamic_programming/</id>
    <content src="http://0.0.0.0:4000/posts/dynamic_programming/" />
    <author>
      <name>{"name"=>"luo-songtao", "link"=>"https://github.com/luo-songtao"}</name>
    </author>

  
    
    <category term="LeetCode" />
    
    <category term="Dynamic Programming" />
    
  

  
    <summary>
      





      Dynamic Programming

Level Easy

LeetCode0053 maximum-subarray
int LeetCode0053::maxContiguousSubArraySum(vector&amp;lt;int&amp;gt;&amp;amp; nums) {
    /**
     * Find largest sum subarray, but just return its sum.
     * Note: A subarray is a `contiguous` part of an array
     * 
     * https://leetcode-cn.com/problems/maximum-subarray/
     * 
     * Examples:
     * Input: nums = [-2,1,-3,4,-1,2,1,-5,4...
    </summary>
  

  </entry>

  
  <entry>
    <title>Variational Inference</title>
    <link href="http://0.0.0.0:4000/posts/variational_inference/" rel="alternate" type="text/html" title="Variational Inference" />
    <published>2022-02-21T23:00:00+00:00</published>
  
    <updated>2022-02-21T23:00:00+00:00</updated>
  
    <id>http://0.0.0.0:4000/posts/variational_inference/</id>
    <content src="http://0.0.0.0:4000/posts/variational_inference/" />
    <author>
      <name>{"name"=>"luo-songtao", "link"=>"https://github.com/luo-songtao"}</name>
    </author>

  
    
    <category term="Machine Learning" />
    
    <category term="Variational Inference" />
    
  

  
    <summary>
      





      Variational Inference

In Bayesian Learning, when the involved integrations  are no longer computationally tractable. Then Variational Approximation can be used.

Although there is nothing intrinsically approximate about variational methods, they do naturally lend themselves to finding approximate solutions.

  This is done by restricting the range of functions over which the optimization is pe...
    </summary>
  

  </entry>

  
  <entry>
    <title>Expectation Maximization</title>
    <link href="http://0.0.0.0:4000/posts/expectation_maximization/" rel="alternate" type="text/html" title="Expectation Maximization" />
    <published>2022-02-21T23:00:00+00:00</published>
  
    <updated>2022-02-21T23:00:00+00:00</updated>
  
    <id>http://0.0.0.0:4000/posts/expectation_maximization/</id>
    <content src="http://0.0.0.0:4000/posts/expectation_maximization/" />
    <author>
      <name>{"name"=>"luo-songtao", "link"=>"https://github.com/luo-songtao"}</name>
    </author>

  
    
    <category term="Machine Learning" />
    
    <category term="Expectation Maximization" />
    
  

  
    <summary>
      





      Expectation Maximization

EM Lower Bound analysis


  
    The expectation Maximization algorithm, or EM aoglrithm, is a general technique for finding maximum likelihood solutoins for probabilistic models having latent variables.
  
  Consider a probabilistic model in which we collectively denote:
    
      all of the observed variables by $\mathbf{X}$
      all of the hidden variables by $\ma...
    </summary>
  

  </entry>

  
  <entry>
    <title>Bayesian Gaussian Mixture Model - Variational Inference</title>
    <link href="http://0.0.0.0:4000/posts/Bayesian_gmm_varinfer/" rel="alternate" type="text/html" title="Bayesian Gaussian Mixture Model - Variational Inference" />
    <published>2022-02-21T23:00:00+00:00</published>
  
    <updated>2022-02-21T23:00:00+00:00</updated>
  
    <id>http://0.0.0.0:4000/posts/Bayesian_gmm_varinfer/</id>
    <content src="http://0.0.0.0:4000/posts/Bayesian_gmm_varinfer/" />
    <author>
      <name>{"name"=>"luo-songtao", "link"=>"https://github.com/luo-songtao"}</name>
    </author>

  
    
    <category term="Machine Learning" />
    
    <category term="Variational Inference" />
    
  

  
    <summary>
      





      Bayesian Gaussian Mixture Model - Variational Inference

Likelihood Funcitons

\[\begin{aligned} p(\mathbf{Z}\vert \boldsymbol{\pi}) &amp;amp;= \prod_{n=1}^N\prod_{k=1}^K \pi_k^{z_{nk}} \\ \\ p(\mathbf{X}\vert \mathbf{Z}, \boldsymbol{\mu}, \Lambda) &amp;amp;= \prod_{n=1}^N\prod_{k=1}^K \mathcal{N}(\boldsymbol{x}_n \vert \boldsymbol{\mu}_k, \Lambda_k^{-1})^{z_{nk}} \end{aligned}\]

Conjugate Priors

\[\...
    </summary>
  

  </entry>

  
  <entry>
    <title>Gaussian Processes</title>
    <link href="http://0.0.0.0:4000/posts/gaussian_processes/" rel="alternate" type="text/html" title="Gaussian Processes" />
    <published>2022-02-21T01:00:00+00:00</published>
  
    <updated>2022-02-21T01:00:00+00:00</updated>
  
    <id>http://0.0.0.0:4000/posts/gaussian_processes/</id>
    <content src="http://0.0.0.0:4000/posts/gaussian_processes/" />
    <author>
      <name>{"name"=>"luo-songtao", "link"=>"https://github.com/luo-songtao"}</name>
    </author>

  
    
    <category term="Machine Learning" />
    
    <category term="Stochastic Processes" />
    
  

  
    <summary>
      





      Gaussian Processes

In the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly.


  At first sight, it might seem difficult to work with a distribution over the uncountably infinite space of functions.
  However, as we shall see, for a finite training set we only need to consider the values of the function...
    </summary>
  

  </entry>

</feed>


