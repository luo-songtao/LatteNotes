<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta name="generator" content="Jekyll">

  <title>机器学习(四)-最大熵模型</title>
  <link rel="stylesheet" href="/css/main.css">

  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" /> <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>机器学习(四)-最大熵模型 | MicroNotes</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习(四)-最大熵模型" />
<meta name="author" content="LuoSongtao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="最大熵原理" />
<meta property="og:description" content="最大熵原理" />
<link rel="canonical" href="http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(4)-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B" />
<meta property="og:url" content="http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(4)-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B" />
<meta property="og:site_name" content="MicroNotes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-30T19:00:04-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"LuoSongtao"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(4)-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B"},"description":"最大熵原理","@type":"BlogPosting","headline":"机器学习(四)-最大熵模型","dateModified":"2020-03-30T19:00:04-05:00","url":"http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(4)-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B","datePublished":"2020-03-30T19:00:04-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>

</head>

<body>
  <div id="wrapper">
    <header>
  
  
  <div class="container">
      

      <div style="padding-left: 42px;">
        <a id="username" href="/">
        
        <span style="font-size: x-large; color: #6a9fb5;"># </span>
        <span style="font-size: x-large; color: #aa759f;">luosongtao</span>
        <span style="font-size: x-large; color: #6a9fb5;">@</span>
        <span style="font-size: x-large; color: #83a;">home:</span>
        <span style="font-size: x-large; color: #cc0000;">~$</span>
        </a>
        
<div id="search">
	<input type="text" id="search-input" placeholder=" find / -name " style="vertical-align:middle;">
	<ul id="results-container"></ul>
</div>

<!-- script pointing to jekyll-search.js -->
<script src="/assets/js/simple-jekyll-search.min.js"></script>
<script async src="/searchdata.js"></script>


      </div>

      <ul id="results-container"></ul>

      <nav class="nav">
    
    
        
        <h3><a href="/">最新</a></h3>
        
    
        
        <h3><a href="/algorithms">Algorithms</a></h3>
        
    
        
        <h3><a href="/optimization">Optimization</a></h3>
        
    
        
        <h3><a href="/algebra">代数论</a></h3>
        
    
        
        <h3><a href="/docker">Docker</a></h3>
        
    
        
        <h3><a href="/bigdata">大数据</a></h3>
        
    
        
        <h3><a href="/mathematical-analysis">数学分析</a></h3>
        
    
        
        <h3><a href="/probability-theory">概率论</a></h3>
        
    
        
        <h3><a href="/others">其他</a></h3>
        
    
</nav>
      <div class="header-links">
        
        <a href="/archive"><h2 class="header-link">时间轴</h2></a>
<a href="/about"><h2 class="header-link">关于</h2></a>
<!--<a href="/atom.xml"><h2 class="header-link">RSS</h2></a>-->
      </div>
  <div>

</header>
    
    <div class="container">
      <article>
  <h1>机器学习(四)-最大熵模型</h1>
  <time datetime="2020-03-30T19:00:04-05:00" class="by-line">30 Mar 2020</time>
  <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#最大熵原理">最大熵原理</a>
<ul>
<li class="toc-entry toc-h4"><a href="#定义">定义</a></li>
<li class="toc-entry toc-h4"><a href="#熵公式">熵公式</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#最大熵模型定义">最大熵模型定义</a>
<ul>
<li class="toc-entry toc-h4"><a href="#最大熵模型的特征与特征函数">最大熵模型的特征与特征函数</a></li>
<li class="toc-entry toc-h4"><a href="#最大熵模型的约束条件">最大熵模型的约束条件</a>
<ul>
<li class="toc-entry toc-h5"><a href="#期望">期望</a></li>
<li class="toc-entry toc-h5"><a href="#约束条件">约束条件</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#条件熵">条件熵</a></li>
<li class="toc-entry toc-h4"><a href="#最大熵模型的定义">最大熵模型的定义</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#最大熵模型的求解">最大熵模型的求解</a>
<ul>
<li class="toc-entry toc-h4"><a href="#拉格朗日乘子法">拉格朗日乘子法</a>
<ul>
<li class="toc-entry toc-h5"><a href="#对pyvert-boldsymbolx求偏导">对$P(y\vert \boldsymbol{x})$求偏导</a></li>
<li class="toc-entry toc-h5"><a href="#最终得到的最大熵模型">最终得到的最大熵模型</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#拉格朗日对偶法求解">拉格朗日对偶法求解</a>
<ul>
<li class="toc-entry toc-h5"><a href="#原问题与对偶问题最优解的等价性">原问题与对偶问题最优解的等价性</a>
<ul>
<li class="toc-entry toc-h6"><a href="#原问题">原问题</a></li>
<li class="toc-entry toc-h6"><a href="#对偶问题">对偶问题</a></li>
</ul>
</li>
<li class="toc-entry toc-h5"><a href="#求解对偶问题">求解对偶问题</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#极大似然估计法求解">极大似然估计法求解</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#最大熵模型的学习">最大熵模型的学习</a></li>
</ul><h2 id="最大熵原理">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵原理</h2>

<h4 id="定义">
<a class="anchor" href="#%E5%AE%9A%E4%B9%89" aria-hidden="true"><span class="octicon octicon-link"></span></a>定义</h4>

<ul>
  <li>
    <p>最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型。</p>
  </li>
  <li>
    <p>若概率模型需要满足一些约束条件(满足约束条件的概率模型/分布可能有无穷多个)，那么最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p>
  </li>
  <li>
    <p>最大熵原理指出，对一个随机事件的概率分布进行预测时，预测应当满足全部已知的约束，而对未知的情况不要做任何主观假设，应认为对于未知的情况都是”等可能的”。
而在这种情况下，概率分布最均匀，预测的风险最小，因此得到的概率分布的熵是最大。</p>
  </li>
  <li>
    <p>也就是说，最大熵原理通过熵的最大化来表示等可能性</p>
  </li>
</ul>

<h4 id="熵公式">
<a class="anchor" href="#%E7%86%B5%E5%85%AC%E5%BC%8F" aria-hidden="true"><span class="octicon octicon-link"></span></a>熵公式</h4>

<ul>
  <li>
    <p>假设离散随机变量$X$是的概率分布是$P(X)$，则其熵是：</p>

    <script type="math/tex; mode=display">H(X) = -\sum_{\boldsymbol{x}} P(X=\boldsymbol{x})\log P(X=\boldsymbol{x})</script>

    <ul>
      <li>$H(X)$依赖于X的分布,而与X的具体值无关。H(X)越大,表示X的不确定性越大</li>
    </ul>
  </li>
  <li>
    <p>可证，熵$H(X)$满足：</p>

    <script type="math/tex; mode=display">0 \le H(X) \le \log \vert X \vert</script>

    <ul>
      <li>$\vert X \vert$表示离散随机变量$X$的可能取值个数</li>
      <li>最大熵原理指出概率分布越均匀，熵越大，即当且仅当X的分布服从均匀分布时上式右边的等号成立</li>
    </ul>
  </li>
  <li>
    <p>最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型</p>
  </li>
</ul>

<h2 id="最大熵模型定义">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵模型定义</h2>

<ul>
  <li>
    <p>定义$\mathcal{X}$，$\mathcal{Y}$分别表示$R_n$中的输入空间和输出空间，$X$和$Y$分别是$\mathcal{X}$，$\mathcal{Y}$的随机变量。</p>
  </li>
  <li>
    <p>假设分类模型是一个条件概率分布$P(Y\vert X)$(判别式的)，即对于给定的输入$X$，根据条件概率$P(Y\vert X)$输出$Y$。</p>
  </li>
</ul>

<h4 id="最大熵模型的特征与特征函数">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E4%B8%8E%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵模型的特征与特征函数</h4>

<ul>
  <li>
    <p>通常”特征”往往仅指对输入抽取特征，用函数表示就是 $f(\boldsymbol{x})$；而在最大熵模型中，是对输入和输出同时抽取特征，用函数表示就是 $f(\boldsymbol{x},y)$</p>
  </li>
  <li>
    <p>特征函数的作用可理解为相当于对特征进行特征转换、提取或向量化处理表达</p>
  </li>
  <li>
    <p>如</p>

    <script type="math/tex; mode=display">% <![CDATA[
f(\boldsymbol{x},y) = \left \{ \begin{aligned} &1 \qquad &&当\boldsymbol{x},y满足某一事实，如 \boldsymbol{x}=\boldsymbol{x}_0,y=y_0 \\ &0 \qquad &&不满足该事实 \end{aligned} \right. %]]></script>
  </li>
  <li>
    <p>注意特征函数可以任意实值函数，且一个模型中可以定义多个特征函数</p>
  </li>
</ul>

<h4 id="最大熵模型的约束条件">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵模型的约束条件</h4>

<h5 id="期望">
<a class="anchor" href="#%E6%9C%9F%E6%9C%9B" aria-hidden="true"><span class="octicon octicon-link"></span></a>期望</h5>
<ul>
  <li>对于给定的训练样本数据集(假设样本数为 $m$)：
    <ul>
      <li>
        <p>联合分布 $P(X=\boldsymbol{x},Y=y)$ 的经验分布 $\tilde{P}(X=\boldsymbol{x},Y=y)$:</p>

        <script type="math/tex; mode=display">\tilde{P}(X=\boldsymbol{x},Y=y) = \frac {count(X=\boldsymbol{x},Y=y)}{m}</script>
      </li>
      <li>
        <p>边缘分布 $P(X=\boldsymbol{x})$ 的经验分布 $\tilde{P}(X=\boldsymbol{x})$:</p>

        <script type="math/tex; mode=display">\tilde{P}(X=\boldsymbol{x}) = \frac {count(X=\boldsymbol{x})}{m}</script>
      </li>
    </ul>
  </li>
  <li>则可得相应的期望：
    <ul>
      <li>
        <p>特征函数 $f(\boldsymbol{x},y)$ 关于经验分布 $\tilde{P}(X=\boldsymbol{x},Y=y)$ 的期望是：</p>

        <script type="math/tex; mode=display">E_{\tilde{P}}(f) = \sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x},Y=y) \space f(\boldsymbol{x}, y)</script>
      </li>
      <li>
        <p>模型 $P(Y\vert X)$ 关于特征函数 $f(\boldsymbol{x},y)$ 的期望是：</p>

        <script type="math/tex; mode=display">E_{P}(f) = \sum_{\boldsymbol{x},y} P(X=\boldsymbol{x},Y=y) \space f(\boldsymbol{x}, y) \approx \sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x}) \space P(Y=y\vert X=\boldsymbol{x}) \space f(\boldsymbol{x}, y)</script>
      </li>
    </ul>
  </li>
</ul>

<h5 id="约束条件">
<a class="anchor" href="#%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6" aria-hidden="true"><span class="octicon octicon-link"></span></a>约束条件</h5>

<ul>
  <li>
    <p>最大熵模型的约束条件就是，对于所有的特征函数 $f_i, i=1,2,…n$(假定有$n$个特征函数，也就意味着有$n$个约束条件)：</p>

    <script type="math/tex; mode=display">E_{\tilde{P}}(f_i) = E_{P}(f_i)</script>
  </li>
</ul>

<h4 id="条件熵">
<a class="anchor" href="#%E6%9D%A1%E4%BB%B6%E7%86%B5" aria-hidden="true"><span class="octicon octicon-link"></span></a>条件熵</h4>

<ul>
  <li>
    <p>关于条件分布 $P(Y\vert X)$ 的条件熵为：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} H(P) &= H(Y \vert X) \\ &=  \sum_{i=1}^m P(X=\boldsymbol{x}_i) H(Y \vert X=\boldsymbol{x}_i) \\ &= -\sum_{i=1}^m P(X=\boldsymbol{x}_i) \cdot \sum_{j=1}^n P(Y=y_j \vert X=\boldsymbol{x}_i) \log P(Y=y_j \vert X=\boldsymbol{x}_i) \\ &= -\sum_{\boldsymbol{x},y} P(Y=y, X=\boldsymbol{x}) \log P(Y=y \vert X=\boldsymbol{x}) \\ &\approx  -\sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x}) P(Y=y \vert X=\boldsymbol{x})\log P(Y=y \vert X=\boldsymbol{x}) \end{aligned} %]]></script>
  </li>
</ul>

<h4 id="最大熵模型的定义">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9A%E4%B9%89" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵模型的定义</h4>

<ul>
  <li>设 $P^*$ 是最大熵模型的解，则最大熵模型定义为：
    <ul>
      <li>第一，优先保证模型满足已知的所有约束，约束集合记为：<script type="math/tex">C = \{ P \vert E_{\tilde{P}}(f_i) = E_{P}(f_i), \qquad i=1,2,...n \}</script>
</li>
      <li>第二，使条件熵 $H(P)$ 取最大</li>
    </ul>

    <script type="math/tex; mode=display">P^* = arg \max_{P\in C} H(P) \space 或 \space P^*= arg \min_{P\in C} -H(P)</script>
  </li>
</ul>

<h2 id="最大熵模型的求解">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B1%82%E8%A7%A3" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵模型的求解</h2>

<ul>
  <li>
    <p>由上，可知最大熵模型的求解问题其实就是一个<strong>等式约束优化问题</strong>：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} &\min_{P\in C} &&-H(P) = \sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x}) P(Y=y \vert X=\boldsymbol{x})\log P(Y=y \vert X=\boldsymbol{x}) \\ &subject.to &&\sum_y P(Y=y\vert X=\boldsymbol{x}) = 1 \\ & &&E_{\tilde{P}}(f_i) - E_{P}(f_i) = 0, \space i=1,2,...n \end{aligned} %]]></script>
  </li>
  <li>
    <p>为表达方便，简写为：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} &\min_{P\in C} &&-H(P) = \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) P(y \vert \boldsymbol{x})\log P(y \vert \boldsymbol{x}) \\ &subject.to &&\sum_y P(y\vert \boldsymbol{x}) = 1 \\ & &&E_{\tilde{P}}(f_i) - E_{P}(f_i) = 0, \space i=1,2,...n  \end{aligned} %]]></script>
  </li>
</ul>

<h4 id="拉格朗日乘子法">
<a class="anchor" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95" aria-hidden="true"><span class="octicon octicon-link"></span></a>拉格朗日乘子法</h4>

<ul>
  <li>
    <p>对于<strong>等式约束优化问题</strong>可引入拉格朗日乘子，构建拉格朗日函数：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} L(P, \boldsymbol{\omega}) &= -H(P) + \omega_0[1-\sum_y P(y\vert \boldsymbol{x})] + \sum_{i=1}^n \omega_i [E_{\tilde{P}}(f_i) - E_{P}(f_i)] \\ &= \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) P(y \vert \boldsymbol{x})\log P(y \vert \boldsymbol{x}) + \omega_0[1-\sum_y P(y\vert \boldsymbol{x})] + \sum_{i=1}^n \omega_i[\sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x},y) \space f_i(\boldsymbol{x}, y) - \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) \space P(y\vert \boldsymbol{x}) \space f_i(\boldsymbol{x}, y)] \end{aligned} %]]></script>
  </li>
</ul>

<h5 id="对pyvert-boldsymbolx求偏导">
<a class="anchor" href="#%E5%AF%B9pyvert-boldsymbolx%E6%B1%82%E5%81%8F%E5%AF%BC" aria-hidden="true"><span class="octicon octicon-link"></span></a>对$P(y\vert \boldsymbol{x})$求偏导</h5>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \frac {\partial L(P, \boldsymbol{\omega})}{\partial P(y\vert \boldsymbol{x})} &= \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) [\log P(y \vert \boldsymbol{x})+1] - \sum_y \omega_0 - \sum_{i=1}^n \omega_i \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) \space f_i(\boldsymbol{x}, y) \\ &= \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x})[1+\log P(y \vert \boldsymbol{x})-\omega_0-\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)]\end{aligned} %]]></script>

<ul>
  <li>
    <p>令偏导数为0，将对数视为自然对数，可得：</p>

    <script type="math/tex; mode=display">P(y \vert \boldsymbol{x}) = e^{-1+\omega_0+\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} =\frac {1}{e^{1-\omega_0}}e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}</script>
  </li>
  <li>
    <p>由已知约束条件$\sum_y P(y \vert \boldsymbol{x}) = 1$，可得：</p>

    <script type="math/tex; mode=display">\sum_y P(y \vert \boldsymbol{x}) = \sum_y \frac {1}{e^{1-\omega_0}}e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} = 1</script>

    <script type="math/tex; mode=display">\Longrightarrow e^{1-\omega_0} = \sum_y e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}</script>
  </li>
</ul>

<h5 id="最终得到的最大熵模型">
<a class="anchor" href="#%E6%9C%80%E7%BB%88%E5%BE%97%E5%88%B0%E7%9A%84%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B" aria-hidden="true"><span class="octicon octicon-link"></span></a>最终得到的最大熵模型</h5>

<ul>
  <li>
    <p>令</p>

    <script type="math/tex; mode=display">Z_\omega (\boldsymbol{x}) = e^{1-\omega_0} = \sum_y e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}</script>
  </li>
  <li>
    <p>这里<script type="math/tex">Z_\omega (\boldsymbol{x})</script>称为规范化因子，起到了归一化的作用</p>
  </li>
  <li>
    <p>最大熵模型的后验概率：</p>

    <script type="math/tex; mode=display">P(y\vert \boldsymbol{x}) = \frac {1}{Z_\omega (\boldsymbol{x})} e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}</script>
  </li>
</ul>

<h4 id="拉格朗日对偶法求解">
<a class="anchor" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%B3%95%E6%B1%82%E8%A7%A3" aria-hidden="true"><span class="octicon octicon-link"></span></a>拉格朗日对偶法求解</h4>

<h5 id="原问题与对偶问题最优解的等价性">
<a class="anchor" href="#%E5%8E%9F%E9%97%AE%E9%A2%98%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E6%9C%80%E4%BC%98%E8%A7%A3%E7%9A%84%E7%AD%89%E4%BB%B7%E6%80%A7" aria-hidden="true"><span class="octicon octicon-link"></span></a>原问题与对偶问题最优解的等价性</h5>

<ul>
  <li>因满足以下三条性质，根据强对偶理论，可知最大熵模型的拉格朗日函数的拉格朗日对偶性中：原问题的最优解和对偶问题的最优解之间无对偶间隙。即对偶问题的最优解等价于原问题的最优解。
    <ul>
      <li>根据集合$C$的定义表述，可知$C$是一个多面体；</li>
      <li>由熵的凹性，可知 $L(P, \boldsymbol{\omega})$ 是关于 $P$的凸函数；</li>
      <li>同时加之，约束条件是线性的；</li>
    </ul>
  </li>
</ul>

<h6 id="原问题">
<a class="anchor" href="#%E5%8E%9F%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>原问题</h6>

<ul>
  <li>
    <p>转换为等价的极小极大问题(原问题)</p>

    <script type="math/tex; mode=display">\min_{P\in C} \max_{\boldsymbol{\omega}} L(P, \boldsymbol{\omega})</script>
  </li>
</ul>

<h6 id="对偶问题">
<a class="anchor" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>对偶问题</h6>

<ul>
  <li>
    <p>对偶函数：</p>

    <script type="math/tex; mode=display">q(\boldsymbol{\omega}) = \min_{P\in C} L(P, \boldsymbol{\omega})</script>
  </li>
  <li>
    <p>转换为对偶的极大极小问题(对偶问题)</p>

    <script type="math/tex; mode=display">\max_{\boldsymbol{\omega}} \min_{P\in C} L(P, \boldsymbol{\omega})</script>
  </li>
</ul>

<h5 id="求解对偶问题">
<a class="anchor" href="#%E6%B1%82%E8%A7%A3%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>求解对偶问题</h5>

<ul>
  <li>
    <p>求解对偶问题内部的极小化问题得 $P^*$：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} P^* &= arg \min_{P\in C} L(P, \boldsymbol{\omega}) \\ &= \frac {1}{Z_\omega (\boldsymbol{x})} e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} \end{aligned} %]]></script>
  </li>
  <li>
    <p>求解对偶问题外部的极大化问题得 $\boldsymbol{\omega}^*$：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \boldsymbol{\omega}^* &= arg \max_{\boldsymbol{\omega}} q(\boldsymbol{\omega}) \\ &= arg \max_{\boldsymbol{\omega}} \min_{P\in C} L(P, \boldsymbol{\omega}) \\ &= arg \max_{\boldsymbol{\omega}} L(P^*, \boldsymbol{\omega}) \end{aligned} %]]></script>

    <ul>
      <li>代入$P^*$，化简后得：</li>
    </ul>

    <script type="math/tex; mode=display">\boldsymbol{\omega}^* = arg \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x}, y} \tilde{P}(\boldsymbol{x}, y) \sum_{i=1}^n \omega_i f_i(\boldsymbol{x}, y)-\sum_{\boldsymbol{x}} \tilde{P}(\boldsymbol{x}) \log Z_\omega (\boldsymbol{x})</script>
  </li>
</ul>

<h4 id="极大似然估计法求解">
<a class="anchor" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%B3%95%E6%B1%82%E8%A7%A3" aria-hidden="true"><span class="octicon octicon-link"></span></a>极大似然估计法求解</h4>

<ul>
  <li>
    <p>似然函数</p>

    <script type="math/tex; mode=display">L(\boldsymbol{\omega}) = \prod_{i=1}^m P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega}), \qquad m表示训练样本数</script>
  </li>
  <li>
    <p>上面的似然函数，稍微调整下可以写为：</p>

    <script type="math/tex; mode=display">L(\boldsymbol{\omega}) = \prod_{i=1}^k P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega})^{count(X=\boldsymbol{x}_i)}, \qquad k表示X的取值个数</script>

    <ul>
      <li>左右同时取m次方根</li>
    </ul>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} L(\boldsymbol{\omega})^{\frac 1m} &= \prod_{i=1}^k P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega})^{\frac {count(X=\boldsymbol{x}_i)}{m}}, \qquad k表示X的取值个数 \\ &= \prod_{i=1}^k P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega})^{\tilde{P}(X=\boldsymbol{x}_i)} \\ &= \prod_{\boldsymbol{x},y} P(y \vert \boldsymbol{x}; \boldsymbol{\omega})^{\tilde{P}(\boldsymbol{x})} \end{aligned} %]]></script>

    <ul>
      <li>显然对 $L(\boldsymbol{\omega})$ 极大化等价于对其 $m$次方根的极大化</li>
    </ul>
  </li>
  <li>
    <p>则可得条件分布$P(y\vert \boldsymbol{x})$ 极大化对数似然估计函数为：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \max_{\boldsymbol{\omega}} \log L(\boldsymbol{\omega}) &\equiv \max_{\boldsymbol{\omega}} \log L(\boldsymbol{\omega})^{\frac 1m} \\ &= \max_{\boldsymbol{\omega}} \log \prod_{\boldsymbol{x},y} P(y \vert \boldsymbol{x}; \boldsymbol{\omega})^{\tilde{P}(\boldsymbol{x})} \\ &= \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}, y) \log P(y\vert \boldsymbol{x}; \boldsymbol{\omega}) \end{aligned} %]]></script>
  </li>
  <li>
    <p>则极大化对数似然函数，可得：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \boldsymbol{\omega}^* &= arg \max_{\boldsymbol{\omega}} \log L(\boldsymbol{\omega}) \\ &= arg \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}, y) \sum_{i=1}^n \omega_i f_i(\boldsymbol{x}, y) - \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x},y) \log Z_\omega (\boldsymbol{x}) \\ &= arg \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}, y) \sum_{i=1}^n \omega_i f_i(\boldsymbol{x}, y) - \sum_{\boldsymbol{x}} \tilde{P}(\boldsymbol{x}) \log Z_\omega (\boldsymbol{x}) \qquad 根据\sum_y P(\boldsymbol{x},y)=1得出 \end{aligned} %]]></script>
  </li>
</ul>

<h2 id="最大熵模型的学习">
<a class="anchor" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%A6%E4%B9%A0" aria-hidden="true"><span class="octicon octicon-link"></span></a>最大熵模型的学习</h2>

<ul>
  <li>根据前面的推导已经得到参数的表达式，因此接下来可以使用如梯度下降、牛顿法、拟牛顿法等方法进行求解</li>
  <li>对于最大熵模型的学习，也有专门的最优化算法-改进的迭代尺度法(IIS)</li>
</ul>

</article>
      <section id="main_content">
        <ul>
            
        </ul>
      </section>
    </div>
  </div>
   <footer>
  <a href="/">
    <span>
        <b>LuoSongtao</b>
    </span>
    
    <span>© 2020</span>
  </a>
</footer>

  
</body>

</html>