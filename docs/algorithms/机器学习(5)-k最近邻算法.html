<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta name="generator" content="Jekyll">

  <title>机器学习(五)-k最近邻算法</title>
  <link rel="stylesheet" href="/css/main.css">

  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" /> <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>机器学习(五)-k最近邻算法 | MicroNotes</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习(五)-k最近邻算法" />
<meta name="author" content="LuoSongtao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="KNN" />
<meta property="og:description" content="KNN" />
<link rel="canonical" href="http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(5)-k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" />
<meta property="og:url" content="http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(5)-k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" />
<meta property="og:site_name" content="MicroNotes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-30T19:00:05-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"LuoSongtao"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(5)-k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"},"description":"KNN","@type":"BlogPosting","headline":"机器学习(五)-k最近邻算法","dateModified":"2020-03-30T19:00:05-05:00","url":"http://0.0.0.0:4000/algorithms/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(5)-k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95","datePublished":"2020-03-30T19:00:05-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>

</head>

<body>
  <div id="wrapper">
    <header>
  
  
  <div class="container">
      

      <div style="padding-left: 42px;">
        <a id="username" href="/">
        
        <span style="font-size: x-large; color: #6a9fb5;"># </span>
        <span style="font-size: x-large; color: #aa759f;">luosongtao</span>
        <span style="font-size: x-large; color: #6a9fb5;">@</span>
        <span style="font-size: x-large; color: #83a;">home:</span>
        <span style="font-size: x-large; color: #cc0000;">~$</span>
        </a>
        
<div id="search">
	<input type="text" id="search-input" placeholder=" find / -name " style="vertical-align:middle;">
	<ul id="results-container"></ul>
</div>

<!-- script pointing to jekyll-search.js -->
<script src="/assets/js/simple-jekyll-search.min.js"></script>
<script async src="/searchdata.js"></script>


      </div>

      <ul id="results-container"></ul>

      <nav class="nav">
    
    
        
        <h3><a href="/">最新</a></h3>
        
    
        
        <h3><a href="/algorithms">Algorithms</a></h3>
        
    
        
        <h3><a href="/optimization">Optimization</a></h3>
        
    
        
        <h3><a href="/algebra">代数论</a></h3>
        
    
        
        <h3><a href="/docker">Docker</a></h3>
        
    
        
        <h3><a href="/bigdata">大数据</a></h3>
        
    
        
        <h3><a href="/mathematical-analysis">数学分析</a></h3>
        
    
        
        <h3><a href="/probability-theory">概率论</a></h3>
        
    
        
        <h3><a href="/others">其他</a></h3>
        
    
</nav>
      <div class="header-links">
        
        <a href="/archive"><h2 class="header-link">时间轴</h2></a>
<a href="/about"><h2 class="header-link">关于</h2></a>
<!--<a href="/atom.xml"><h2 class="header-link">RSS</h2></a>-->
      </div>
  <div>

</header>
    
    <div class="container">
      <article>
  <h1>机器学习(五)-k最近邻算法</h1>
  <time datetime="2020-03-30T19:00:05-05:00" class="by-line">30 Mar 2020</time>
  <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#knn">KNN</a>
<ul>
<li class="toc-entry toc-h4"><a href="#knn算法的工作机制">KNN算法的工作机制</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#knn算法的基本要素">KNN算法的基本要素</a>
<ul>
<li class="toc-entry toc-h4"><a href="#k值的选择">K值的选择</a>
<ul>
<li class="toc-entry toc-h5"><a href="#距离度量">距离度量</a></li>
<li class="toc-entry toc-h5"><a href="#欧式距离">欧式距离</a></li>
<li class="toc-entry toc-h5"><a href="#曼哈顿距离">曼哈顿距离</a></li>
<li class="toc-entry toc-h5"><a href="#mahalanobis距离">Mahalanobis距离</a></li>
<li class="toc-entry toc-h5"><a href="#bhattacharyya距离">Bhattacharyya距离</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#分类回归决策规则">分类/回归决策规则</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#kd树">kd树</a>
<ul>
<li class="toc-entry toc-h4"><a href="#kd树构造">kd树构造</a></li>
<li class="toc-entry toc-h4"><a href="#kd树搜索">kd树搜索</a></li>
</ul>
</li>
</ul><h2 id="knn">
<a class="anchor" href="#knn" aria-hidden="true"><span class="octicon octicon-link"></span></a>KNN</h2>

<p>k近邻算法(k-nearest neighbor, KNN)是一种基本分类与回归方法。</p>

<h4 id="knn算法的工作机制">
<a class="anchor" href="#knn%E7%AE%97%E6%B3%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6" aria-hidden="true"><span class="octicon octicon-link"></span></a>KNN算法的工作机制</h4>

<ul>
  <li>给定测试样本，基于某种距离度量(如欧氏距离)找出测试集中与其最靠近的k个训练样本，然后基于这k个近邻样本的信息进行预测。
    <ul>
      <li>给定的测试样本，其中的实例类别已定(分类任务)，或实例的真是标记值已定(回归任务)</li>
      <li>在分类任务中，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测
  对于分类问题，给定$m$个训练样本$(\boldsymbol{x}_i, y_i)$，设定参数$k$，假设类型数为$c$，待分类样本的特征向量是$\boldsymbol{x}$，那么预测的流程：
        <ul>
          <li>在训练样本中找出离$\boldsymbol{x}$最近的$k$个样本，假设这些样本的集合为$N$</li>
          <li>统计集合$N$中每一类的个数$C_i, i=1,2,···,c$</li>
          <li>最终分类结果为$\max_i C_i$</li>
        </ul>
      </li>
      <li>在回归任务中，可使用”平均法”，即将这k个样本的实值输出，标记的平均值作为预测结果</li>
    </ul>
  </li>
  <li>可见，KNN算法不具有显示的学习过程。
    <ul>
      <li>KNN实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”</li>
      <li>KNN是“lazy learning”的著名代表，此类学习技术在训练阶段仅仅是吧样本保存起来，训练时间开销为0，等需要进行预测时再进行处理。与之相对的是“eager learning”</li>
    </ul>
  </li>
  <li>KNN算法实现简单，但缺点是训练样本数大、特征向量维数很高时，计算复杂度高。
    <ul>
      <li>每次预测时要计算待预测样本和每一个训练样本的距离。而且要对距离进行排序找到最近的k个样本。
        <ul>
          <li>可以使用高效的部分排序算法，只找出最小的k个数</li>
          <li>或是利用kd数实现快速的近邻样本查找</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>变种：
    <ul>
      <li>带权重的KNN</li>
      <li>模糊KNN</li>
    </ul>
  </li>
</ul>

<h2 id="knn算法的基本要素">
<a class="anchor" href="#knn%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0" aria-hidden="true"><span class="octicon octicon-link"></span></a>KNN算法的基本要素</h2>

<ul>
  <li>k值的选择</li>
  <li>距离度量</li>
  <li>分类/回归决策规则</li>
</ul>

<h4 id="k值的选择">
<a class="anchor" href="#k%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9" aria-hidden="true"><span class="octicon octicon-link"></span></a>K值的选择</h4>
<ul>
  <li>k值的选择会对KNN算法的结果产生重大影响。它需要根据问题和数据的特点来确定，并注意：
    <ul>
      <li>如果选择较小k值，由于只有与输入实例较近的训练实例才会对预测结果作用，估计误差会增大，因此将对这些训练实例非常敏感，从而容易造成过拟合</li>
      <li>如果选择较大的k值，估计误差虽然可以减少，但近似误差，即与输入实例较远的训练实例也会对预测起作用，会使预测发生错误。</li>
      <li>k值的减小意味着整体模型变得复杂， k值的增大意味着整体模型变得简单</li>
    </ul>

    <p>实际中，k值取值一般比较小，并通常采用交叉验证法来选取最优的k值</p>
  </li>
</ul>

<h5 id="距离度量">
<a class="anchor" href="#%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F" aria-hidden="true"><span class="octicon octicon-link"></span></a>距离度量</h5>

<ul>
  <li>KNN算法模型的特征空间一般是n维实数向量空间$R^n$</li>
</ul>

<h5 id="欧式距离">
<a class="anchor" href="#%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB" aria-hidden="true"><span class="octicon octicon-link"></span></a>欧式距离</h5>

<script type="math/tex; mode=display">d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{\sum_{i=1}^n(u_i-v_i)^2}</script>

<h5 id="曼哈顿距离">
<a class="anchor" href="#%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB" aria-hidden="true"><span class="octicon octicon-link"></span></a>曼哈顿距离</h5>

<script type="math/tex; mode=display">d(\boldsymbol{u}, \boldsymbol{v}) = \sum_{i=1}^n \vert u_i-v_i \vert</script>

<h5 id="mahalanobis距离">
<a class="anchor" href="#mahalanobis%E8%B7%9D%E7%A6%BB" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mahalanobis距离</h5>

<ul>
  <li>
    <p>Mahalanobis距离是一种概率意义上的距离。它度量两个随机向量的相似度。</p>

    <script type="math/tex; mode=display">d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{(\boldsymbol{u}-\boldsymbol{v})^TS(\boldsymbol{u}-\boldsymbol{v})}</script>

    <ul>
      <li>矩阵$S$是正定的(否则没有意义)</li>
      <li>当$S$是单位矩阵时，Mahalanobis距离退化为欧氏距离</li>
    </ul>

    <p>$S$矩阵可以通过计算训练样本集的协方差矩阵得到，也可以通过训练样本学习得到</p>
  </li>
  <li>
    <p>KNN算法的精度在很大程度上依赖于所使用的距离度量标准，“距离度量学习”是一种从带标签的样本集中学习，得到距离度量矩阵的方法</p>
  </li>
</ul>

<h5 id="bhattacharyya距离">
<a class="anchor" href="#bhattacharyya%E8%B7%9D%E7%A6%BB" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bhattacharyya距离</h5>

<ul>
  <li>
    <p>Bhattacharyya距离定义了离散型或连续型概率分布的相似性</p>

    <ul>
      <li>
        <p>对于离散型随机变量的分布，定义为：</p>

        <script type="math/tex; mode=display">d(\boldsymbol{u}, \boldsymbol{v}) =  -\ln (\sum_{i=1}^n\sqrt{u_i\cdot y_i})</script>

        <p>其中随机向量的分量值必须非负</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="分类回归决策规则">
<a class="anchor" href="#%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99" aria-hidden="true"><span class="octicon octicon-link"></span></a>分类/回归决策规则</h4>

<ul>
  <li>KNN中分类决策规则往往是多数表决</li>
  <li>KNN中回归决策规则往往是“平均值”</li>
</ul>

<h2 id="kd树">
<a class="anchor" href="#kd%E6%A0%91" aria-hidden="true"><span class="octicon octicon-link"></span></a>kd树</h2>

<ul>
  <li>
    <p>KNN的实现，主要考虑的问题是如何对训练数据进行快速的k近邻搜索，尤其是在特征空间的维数大及训练数据容量大时。KNN最简单的实现方法是线性扫描，但显而易见，性能很低。</p>
  </li>
  <li>
    <p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构</p>
    <ul>
      <li>kd树是一颗二叉树，表示对k维空间的一个划分</li>
      <li>构造kd树相当于不断对垂直于坐标轴超平面将k维空间划分，构成一系列的k维超矩形区域，kd树的每个节点对应于一个k维超矩形区域</li>
    </ul>
  </li>
</ul>

<h4 id="kd树构造">
<a class="anchor" href="#kd%E6%A0%91%E6%9E%84%E9%80%A0" aria-hidden="true"><span class="octicon octicon-link"></span></a>kd树构造</h4>

<ul>
  <li>通常依次选择坐标轴对空间划分
    <ul>
      <li>选择训练实例点在所选定坐标轴上的<strong>中位数</strong>坐标作为切分点，进行二分操作，并将切分点所属的实例点保存在当前的子树的根节点，然后递归处理左右两部分子区域，
  直到两个子区域没有实例存在时停止，最终构建出kd树</li>
    </ul>
  </li>
</ul>

<p>通过中位数构建的kd树是平衡的</p>

<h4 id="kd树搜索">
<a class="anchor" href="#kd%E6%A0%91%E6%90%9C%E7%B4%A2" aria-hidden="true"><span class="octicon octicon-link"></span></a>kd树搜索</h4>

<ul>
  <li>首先从上往下，根据目标点依次在坐标轴上进行判断，直到叶节点</li>
  <li>然后将此叶节点作为“当前最近点”，随即递归向上回退，并坐如下处理
    <ul>
      <li>先判断当前所在节点比“当前最近点”距离更近，则更新当前所在节点为“当前最近点”</li>
      <li>搜索当前节点的另一子节点，在其中查找是否有更近的点，若有，则更新其为新的“当前最近点”</li>
      <li>直到回退到根节点，搜索结束</li>
    </ul>
  </li>
  <li>
    <p>如果实例点是随机分布的，kd树搜索的平均计算复杂度是$O(\log N)$，$N$为训练样本数。</p>
  </li>
  <li>kd树更适合于训练样本数远大于空间维数的k近邻搜索。当空间维数接近训练样本数时，它的效率会迅速下降，几乎接近线性扫描</li>
</ul>


</article>
      <section id="main_content">
        <ul>
            
        </ul>
      </section>
    </div>
  </div>
   <footer>
  <a href="/">
    <span>
        <b>LuoSongtao</b>
    </span>
    
    <span>© 2020</span>
  </a>
</footer>

  
</body>

</html>