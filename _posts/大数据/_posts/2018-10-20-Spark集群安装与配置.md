---
layout: post
tags: [spark,大数据]
category: ['大数据']
---

## Spark安装与配置

- 下载编译版本，以2.2.2版本为例：wget http://apache.cs.utah.edu/spark/spark-2.2.2/spark-2.2.2-bin-hadoop2.7.tgz

- 同安装hadoop一样，解压缩到指定目录下，如这里是`/root/bigdata`

  设置软链接

  ```shell
  ln -s /root/bigdata/spark-2.2.2-bin-hadoop2.7 /root/bigdata/spark
  ```

  设置环境变量：`~/.bash_profile`

  ```shell
  export $SPARK_HOME=/root/bigdata/spark
  ```

  `source ~/.bash_profile`使配置生效

- 修改`spark/config/spark-env.sh`文件，添加：

  ```shell
  export SPARK_HOME=/root/bigdata/spark
  export JAVA_HOME=/root/bigdata/jdk
  export HADOOP_HOME=/root/bigdata/hadoop
  export YARN_HOME=/root/bigdata/hadoop
  export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
  export YARN_CONF_DIR=$YARN_HOME/etc/hadoop
  export SPARK_MASTER_IP=192.168.19.137
  export SPARK_LIBRARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native
  
  SPARK_LOCAL_DIRS=/root/bigdata/spark/tmp
  ```

- 修改`spark/config/slaves`，设置spark的从端机器的域名或IP，这里我们将三台机器都作为从服务器，该配置对于standalone模式资源管理有效

  ```
  hadoop-master
  hadoop-slave1
  hadoop-slave2
  ```

- 在hadoop的配置文件路径下对`yarn-site.xml`添加以下配置，防止yarn关闭spark的APP

  ```xml
  <property>
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
  </property>
  ```

- 同理，如果是搭建集群，那么此时还需要将整个目录拷贝到其他机器行，并配置相应的环境变量

  ```shell
  scp ......
  ```

#### Spark启动

- 关闭防火墙

```shell
systemctl stop firewalld
```

- 启动Master和Worker

```shell
$SPARK_HOME/sbin/start-all.sh
```

- 通过SPARK WEB UI查看Spark集群及Spark
  - http://192.168.19.137:8080/  监控Spark集群
  - http://192.168.19.137:4040/  监控Spark Job

## Pyspark安装与使用

#### 什么是pyspark？

pyspark是python实现spark的API，我们可以利用pyspark编写Spark程序并提交给Spark去执行

#### 安装pyspark

pyspark的版本必须和spark版本一致，但通常我们下载编译版本中，自带了相应版本的pyspark安装包，在`$SPARK_HOME/python`目录下

我们将其安装到虚拟环境中，这里虚拟化我们使用的是miniconda的python，

查看虚拟环境

`conda env list`

```
base                  *  /miniconda2
py365                    /miniconda2/envs/py365
```

进入一个提前配置好的虚拟环境`py365`：`source activate py365`

进入`$SPARK_HOME/python`目录下执行：`python setup.py install`

分别在所有从服务器上重复以上步骤，都安装上pyspark（注意：主从机器的python环境和目录一定要保持一致，否则可能出现问题）



#### 其他

若要在spark中使用hive，那么还需要将hive-sive.xml软链接到spark的conf目录下，并配置HIVE_HOME\HIVE_CONF_DIR环境变量在spark-env.sh中
