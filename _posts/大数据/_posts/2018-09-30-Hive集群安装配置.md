---
layout: post
tags: [hive,大数据,安装配置]
category: ['大数据']
---

## 基于HADOOP分布式集群安装HIVE

#### 环境准备

#### Hive安装与配置


注意：hive的配置相对比较复杂,缺少任何一步都会带来一系列问题，需要耐心，另外以下配置均已CentOS7为例

这里使用当前稳定版：2.3.4

下载：[https://mirrors.tuna.tsinghua.edu.cn/apache/hive/](https://mirrors.tuna.tsinghua.edu.cn/apache/hive/)

解压放到`/root/bigdata/`，并建立软链接`ln -s /root/bigdata/apache-hive-2.3.4-bin /root/bigdata/hive`

环境变量：
```
# .bash_profile
export HIVE_HOME=$HOME/bigdata/bin
export PATH=$PATH:$HIVE_HOME/bin
```

使用source命令使其生效

#### 安装启动

`cd /root/bigdata/hive`

1. 拷贝默认配置文件：`cp hive/conf/hive-default.xml.template hive/conf/hive-site.xml`

2. 修改默认配置文件：`vi hive/conf/hive-site.xml`

    - hive相关路径配置

        - 找到
        ```
          <name>hive.metastore.warehouse.dir</name>
          <value>/user/hive/warehouse</value>
          <name>hive.exec.scratchdir</name>
          <value>/tmp/hive</value>
        ```

        - 这里的两个路径指的是hdfs里的路径，因此需要在hdfs中手动创建这两个目录，并修改权限，如果更改为其他路径，那么相应的创建也需要变更
        ```
        hadoop fs -mkdir -p  /user/hive/warehouse
        hadoop fs -chmod -R 777 /user/hive/warehouse
        hadoop fs -mkdir -p /tmp/hive/
        hadoop fs -chmod -R 777 /tmp/hive
        ```
        - 匹配出所有的`${system:java.io.tmpdir}`替换为指定目录：如`/root/bigdata/hive/tmp`,该目录如果不存在则要自己手工创建，并且赋予读写权限

        - 匹配出所有的`${system:user.name}`替换为root

    - hive元数据\*存储配置：利用mysql数据库存储hive元数据

        - 修改：javax.jdo.option.ConnectionDriverName
            ```
            <property>
              <name>javax.jdo.option.ConnectionDriverName</name>
              <value>com.mysql.jdbc.Driver</value>
            </property>
            ```

        - 修改： javax.jdo.option.ConnectionURL, 对应mysql的地址（后面我们的mysql安装在了hadoop-master机器上，因此这里使用hadoop-master地址）
            ```
            <name>javax.jdo.option.ConnectionURL</name>
             <value>jdbc:mysql://hadoop-master:3306/hive?createDatabaseIfNotExist=true</value>
            ```

        - 修改：javax.jdo.option.ConnectionUserName, 对应mysql用户名
            ```
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>hive</value>
            ```

        - 修改：javax.jdo.option.ConnectionPassword, 对应mysql密码
            ```
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>hive</value>
            ```

3. 拷贝hive变量文件：`cp hive/conf/hive-env.sh.template hive/conf/hive-env.sh`

4. 编辑hive变量文件`vi hive/conf/hive-env.sh`, 设置相应环境变量，如下
    
    ```
    # Set HADOOP_HOME to point to a specific hadoop install directory
    export HADOOP_HOME=/root/bigdata/hadoop

    # Hive Configuration Directory can be controlled by:
    export HIVE_CONF_DIR=/root/bigdata/hive/conf

    # Folder containing extra libraries required for hive compilation/execution can be controlled by:
    export HIVE_AUX_JARS_PATH=/root/bigdata/hive/lib
    ```

5. 将MySQL连接的驱动包[`mysql-connector-java-5.1.47.jar`](http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/)拷贝到`/root/bigdata/hive/lib`下

6. 安装并启动MySQL数据库（在hadoop-master机器上安装并启动）：

    这里使用docker安装MySQL数据库

    安装docker：https://www.alibabacloud.com/help/zh/doc-detail/60742.htm
    ```
    # step 1: 安装必要的一些系统工具
    sudo yum install -y yum-utils device-mapper-persistent-data lvm2
    # Step 2: 添加软件源信息
    sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
    # Step 3: 更新并安装Docker-CE
    sudo yum makecache fast
    sudo yum -y install docker-ce
    # Step 4: 开启Docker服务
    sudo service docker start
    ```

    下载docker镜像：`docker pull mysql:5.7`

    创建MySQL配置文件：'vi /root/scripts/my.cnf'
    ```
    [client]
    default-character-set=utf8mb4

    [mysqld]
    character-set-server=utf8mb4

    [mysql]
    default-character-set=utf8mb4
    ```

    启动MySQL：
    ```
    docker run -dti --name mysql \
    --network=host \
    -v /root/scripts/my.cnf:/etc/mysql/my.cnf  \
    -v mysql-data:/var/lib/mysql \
    -e MYSQL_ROOT_PASSWORD=password \
    -e MYSQL_USER=hive \
    -e MYSQL_PASSWORD=hive \
    -e MYSQL_DATABASE=hive \
    mysql:5.7 \
    --character-set-server=utf8mb4 \
    --collation-server=utf8mb4_unicode_ci
    ```

7. 初始化hive数据库（只用执行一次，除非建立新的hive元数据）
    ```
    schematool -initSchema -dbType mysql
    ```

    执行成功后，在mysql的hive数据库里就已生成相应的metadata数据表
    
    ```
    docker exec -ti mysql mysql -u hive -p
    # 输入密码：hive
    # use hive
    # show tables;
    # 查看是否有表自动创建出来
    ```

8. 再次修改`vi hive/conf/hive-site.xml`,由于使用mysql来进行元数据存储，因此需要
    - 新增（如果有则修改）：把元数据本地存储关闭，否则使用hive进入hive客户端时，会使用本地的元数据
        ```
        <property>
          <name>hive.metastore.local</name>
          <value>false</value>
        </property>
        ```
    - 修改以下配置：开启元数据访问服务，否则启动hive客户端时将无法读取hive的元数据，注意地址，这里我们是在hadoop-master启动该服务，且为分布式集群模式，因此不要把它设置为localhost
        ```
        <property>
            <name>hive.metastore.uris</name>
            <value>thrift://hadoop-master:9083</value>
            <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
        </property>
        ```
    - 启动hive的metastore访问服务：
        ```
        hive --service metastore &
        ```

 9. 连接测试：

    hadoop-master上使用：hive命令进入hive客户端，测试如show databases、show tables等命令是否能使用

    将hadoop-master的hive目录全部拷贝到从服务器上，并配置环境变量，随后连接测试

\* hive元数据：记录着hive中存储着哪些数据库，哪些表等信息，通常是使用mysql数据库来存储hive的元数据

\* 可能出现问题：hadoop的namenode可能进入safe mode导致无法修改hdfs文件，可用命令：`hadoop dfsadmin -safemode leave`解除