---
layout: post
title: 机器学习(一)-线性回归
tags: [线性模型, 线性回归]
category: ['algorithms']
date: 2020-03-31 00:00:01
toc: true
---

## 一元线性回归模型

- 预测函数

    $$f(x) = \omega x + b$$

- 损失函数(使用均方误差表示):
    - 单个样本的误差($y$表示真实值)：
    
        $$e(\omega, b) = (f(x) - y)^2 = (\omega x + b - y)^2$$

    - 全体样本的误差($m$表示样本数)：

        $$E(\omega, b) = \sum_{i=1}^m(f(x_i) - y_i)^2 = \sum_{i=1}^m(\omega x_i + b - y_i)^2$$

- 线性回归的目标就是最小化损失函数，而均方误差最小化，通常可以使用最小二乘法来求解参数

    $$\min_{\omega, b} \space \sum_{i=1}^m(\omega x_i + b - y_i)^2$$

- 分别对系数和偏置项求偏导：

    $$\begin{aligned} \frac {\partial E(\omega, b)}{\partial \omega} &= 2x_i\sum_{i=1}^m(\omega x_i + b - y_i) \\ \frac {\partial E(\omega, b)}{\partial b} &= 2\sum_{i=1}^m(\omega x_i + b - y_i) \end{aligned}$$

- 令上面的偏导为0，可以得到闭式解(损失函数是一个凸函数，令偏导为0时，可以得到最优解):

    $$\begin{aligned} \omega &= \frac {\sum_{i=1}^m y_i(x_i - \sum_{i=1}^m x_i)}{\sum_{i=1}^m x_i^2 - \frac 1m (\sum_{i=1}^m x_i)^2} \\ b &= \frac 1m \sum_{i=1}^m(y_i - \omega x_i)\end{aligned}$$

## 多元线性回归

- 预测函数
    - 一般形式
        
        $$f(\boldsymbol{x}) = \omega_1 x_1 + \omega_2 x_2 + ··· + \omega_n x_n + b$$

    - 向量形式(设$\boldsymbol{\omega}和\boldsymbol{x}均是n维向量$)
    
        $$f(\boldsymbol{x}) = \boldsymbol{\omega}^T\boldsymbol{x} + b$$

- 损失函数

    为了便于分析讨论，计$\boldsymbol{\hat{\omega}} = (\boldsymbol{\omega}; b)$，等价于将$b$看作一个单独的系数项。
    同时令$X$表示$m \times (n+1)$的数据集矩阵,前n列分别是对应n维的样本特征向量，最后一列置为1，对应偏置项,；
    令$\boldsymbol{y}$表示每个样本的真实值构成的$m$维向量

    - 单个样本：

        $$e(\boldsymbol{\hat{\omega}}) = (\boldsymbol{\hat{\omega}}^T\boldsymbol{x} - y)^2$$

    - 全体样本的误差:
        - 一般形式

            $$E(\boldsymbol{\omega}, b) = \sum_{i=1}^m(y_i - \boldsymbol{\omega}^T\boldsymbol{x}_i)^2$$

        - 向量形式

            $$E(\boldsymbol{\hat{\omega}}) = (\boldsymbol{X}\boldsymbol{\hat{\omega}} - \boldsymbol{y})^T(\boldsymbol{X}\boldsymbol{\hat{\omega}} - \boldsymbol{y})$$

- 我们的目标同样是最小化损失函数

    $$\min_{\boldsymbol{\hat{\omega}}} \space (\boldsymbol{X}\boldsymbol{\hat{\omega}} - \boldsymbol{y})^T(\boldsymbol{X}\boldsymbol{\hat{\omega}} - \boldsymbol{y})$$

- 分别对系数和偏置求偏导：

    $$\frac {\partial E(\boldsymbol{\hat{\omega}})}{\partial \boldsymbol{\hat{\omega}}} = 2\boldsymbol{X}^T (\boldsymbol{X}\boldsymbol{\hat{\omega}} - \boldsymbol{y})$$

- 令上面的偏导为0，同样可以得到闭式解

    $$\boldsymbol{\hat{\omega}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$$

## 正则化

当数据集的样本特征很多，而样本数相对较少时，前面的损失函数最小化过程容易陷入过拟合，因此可以考虑对其添加惩罚项，以缓解过拟合的问题

#### L2-正则化： Ridge Regreesion

- L2-正则化即使用L2范数正则化

    $$\min_{\boldsymbol{\hat{\omega}}} \space \sum_{i=1}^m(y_i - \boldsymbol{\omega}^T\boldsymbol{x}_i)^2 + \lambda \Vert \boldsymbol{\hat{\omega}} \Vert_2^2$$

$\lambda>0$称为正则化参数，使用L2正则化的线性回归被称为岭回归(Ridge Regreesion)

#### L1-正则化： LASSO

- L1-正则化使用L1范数正则化

    $$\min_{\boldsymbol{\hat{\omega}}} \space \sum_{i=1}^m(y_i - \boldsymbol{\omega}^T\boldsymbol{x}_i)^2 + \lambda \Vert \boldsymbol{\hat{\omega}} \Vert_1$$

使用L1正则化的线性回归被称为LASSO(Least Absolue Shrinkage and Selection Operator)

###### L0-正则化

如果需要对$\boldsymbol{\hat{\omega}}$施加稀疏约束条件，最自然的是使用L0范数，但L0范数不连续，难以优化求解，因此常用L1范数来近似

###### L1与L2对比

L1与L2都有助于降低过拟合风险，但前者还会带来一个额外的好处，也就是它比后者更易于获得“稀疏”解，即它求得的$\boldsymbol{\hat{\omega}}$会有更少的非零分量
