---
layout: post
title: 机器学习(六)-贝叶斯分类器
tags: [贝叶斯分类器]
category: ['algorithms']
date: 2020-03-31 00:00:06
toc: true
---

贝叶斯分类器是一种概率模型，它用贝叶斯公式解决分类问题。

先设输入空间$$\mathcal{X} \subset R^n$$为n维向量的集合，输出空间为$\mathcal{Y} = \{ c_1, c_2, ···, c_k \}$的集合。$X$是定义在输入空间$\mathcal{X}$上的随机向量，$Y$是定义在输出空间$\mathcal{Y}$上的随机变量。

## 贝叶斯公式

#### 条件概率公式

$$P(X \vert Y) = \frac {P(XY)}{P(Y)}$$

$$P(Y \vert X) = \frac {P(XY)}{P(X)}$$

- $P(XY)$: $X$和$Y$的联合概率分布

#### 全概率公式
根据条件概率公式，可得：

$$ P(X=x) = \sum_{k=1}^K P(X=x;Y=c_k) = \sum_{k=1}^K P(X=x \vert Y=c_k)P(Y=c_k)$$

#### 贝叶斯公式
根据全概率公式，可得：

$$P(Y=c_i \vert X=x) = \frac {P(X=x;Y=c_i)}{P(X=x)} = \frac {P(X=x \vert Y=c_i)P(Y=c_i)}{\sum_{k=1}^K P(X=x \vert Y=c_k)P(Y=c_k)}$$


## 贝叶斯决策

先对上面的贝叶斯公式中相关概率分布进行说明：

- $P(Y=c_k)$ ： 类先验概率。表达了样本空间中各类样本所占的比例，因为根据大数定律，当训练集包含充足的独立同分布样本时，$P(Y=c_k)$可通过各类样本的出现频率来进行估计
- $P(X=x \vert Y=c_k)$ ： 类条件概率。
- $P(Y=c_k \vert X=x)$ ： 类后验概率。

贝叶斯决策也就是通过贝叶斯定理，使用这些概率来选择最优的类别标记。
判断时，我们通常是选择估计出的后验概率最大的类作为最终结果然后输出。而最大化后验概率其实等价于最小化期望损失(后面再介绍)。

估计后验概率，通常有两种策略：
- 判别式：直接对后验概率建模进行预测。
- 生成式：先对联合概率建模，然后再获得后验概率。
- 两种策略简单对比(来自知乎):
    - 要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率
    - 利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个

而根据贝叶斯公式，如果我们要得到类后验概率，我们其实是需要先预测出联合概率的(可见属于生成式模型)。而为了获得联合概率，那么我们需要基于训练集样本估计出类先验概率和类条件概率。对于类先验概率可以直接估计(假定给定的样本量充足)

但对于类条件概率的估计，需要注意：
- 由于样本的特征向量$x$涉及到了其各个分量之间的联合概率问题，因此直接计算它们出现的频率来进行估计，将会带来严重的困难。
- 而如果假设特征向量的各个分量之间相互独立，那么这时条件概率可表示为$P(X=x \vert Y=c_k) = \Pi_{i=1}^n P(X_i = x_i \vert Y=c_k)$，这样的分类器被称为朴素贝叶斯分类器。
- 而如果样本的特征向量服从某种概率分布，如正态分布，则可以通过训练样本对概率分布的参数进行估计。(假定给定的样本是独立同分布的)

## 极大似然估计

前面提到我们需要估计后验概率，使其最大化，根据公式，那么本质上我们是要对估计的类条件概率最大化。

而估计类条件概率一种常用的策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。令$\theta_c$表示$P(X=x \vert Y=c_k)$的参数向量。

事实上，概率模型的训练过成功就是参数估计过程。

而类条件概率也被称为似然，因此最大化类条件概率也就是进行极大似然估计。且最大化类条件概率时，我们不是针对单个单个样本进行最大化，而是针对整个给定类别$c$的所有样本集合($D_c$)进行最大化，则$\theta_c$对于样本集$D_c$的似然：

$$P(D_c \vert Y=c_k) = \prod_{x\in D_c} P(X=x \vert \theta_c) $$

那么对于$\theta_c$的极大似然估计，就是需寻找使$P(D_c \vert Y=c_k)$最大化的参数$\hat{\theta}_c$。

#### 对数似然

由于数值运算时，连乘容易造成下溢，因此通常使用对数似然转换为累加

$$\log P(D_c \vert Y=c_k) =\log \prod_{x\in D_c} P(X=x \vert \theta_c) = \sum_{x\in D_c} \log P(X=x \vert \theta_c)$$

#### Attention

这种参数化方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否复合潜在的真是数据分布。在现实应用中，欲做出能较好地接近潜在真是分布的假设，往往需要在一定程度上利用应用任务本身的经验知识，否则仅凭“猜测”来假设概率分布形式，很可能产生误导性的结果。

## 朴素贝叶斯分类器

前面已经提到，根据原始的贝叶斯公式估计后验概率，在对类条件概率估计时，使用的是样本的特征向量上分量的联合概率，如果分量过多，则可能出现组合爆炸、样本稀疏等问题，因此很难从有限的训练样本中直接估计得到。

因此朴素贝叶斯分类器使用“特征属性条件独立性假设”，也就是假设特征向量的各个分量之间相互独立，那么则对于类条件概率，可表示为：

$$P(X=x \vert Y=c_k) = \prod_{j=1}^n P(X_j = x_j \vert Y=c_k)$$

因此朴素贝叶斯的表达式为：

$$P(Y=c_i \vert X=x) = \frac {P(X=x \vert Y=c_i)P(Y=c_i)}{\sum_{k=1}^K P(X=x \vert Y=c_k)P(Y=c_k)} = \frac {P(Y=c_i)}{\sum_{k=1}^K P(X=x \vert Y=c_k)P(Y=c_k)} \prod_{j=1}^n P(X_j = x_j \vert Y=c_i)$$

由于上式中的分母对于所有类都是相等的，因此将之看作一个归一化因子$Z$，简化后得：

$$P(Y=c_k \vert X=x) = \frac {1}{Z} P(Y=c_k) \prod_{j=1}^n P(X_j = x_j \vert Y=c_k)$$

先验概率(类似地使用$\vert D \vert$表示样本集合的数量)：

$$P(Y=c_k) = \frac {\vert D_{c_k} \vert}{\vert D \vert}$$

类条件概率($\vert D_{c,x_i} \vert$表示类别为$c_k$且第i个分量为x_i的样本集合)：
- 离散值

    $$P(X_i=x_i \vert Y=c_k)= \frac {\vert D_{c_k,x_i} \vert}{\vert D_c \vert}$$

- 连续值
    如果特征向量的分量是连续型随机变量，可以考虑概率密度函数。如可以假设它们服从一维正态分布。然后根据训练样本计算出每个类别样本的每个分量的均值$\mu_{c,i}$和方差$\sigma_{c,i}$,
    则可以得到对应的正态分布，那么就可以用它表示概率密度函数来表示条件概率


#### 平滑处理

在朴素贝叶斯中，如果在某个类别的下，所有样本的特征向量的某一分量的取值都为0，由于计算类条件概率使用连乘，那么将导致整个类条件概率的结果为0。

因此为了避免这样的情况发生，需要对概率值的估计进行平滑处理，常用的处理方式是"拉普拉斯平滑"：令N表示训练集中可能的类别数，$N_i$表示第i个分量的取值数(频率)，则：

$$\begin{aligned} P(Y=c_k) = \frac {\vert D_{c_k} \vert + 1}{\vert D \vert + N}  \\ P(X_i=x_i \vert Y=c_k)= \frac {\vert D_{c_k,x_i}\vert + 1 }{\vert D_c \vert + N_i}  \end{aligned}$$
