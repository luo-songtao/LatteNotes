---
layout: post
title: 机器学习(四)-最大熵模型
tags: [最大熵]
category: ['algorithms']
date: 2020-03-31 00:00:04
toc: true
---

## 最大熵原理

#### 定义

- 最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型。

- 若概率模型需要满足一些约束条件(满足约束条件的概率模型/分布可能有无穷多个)，那么最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。

- 最大熵原理指出，对一个随机事件的概率分布进行预测时，预测应当满足全部已知的约束，而对未知的情况不要做任何主观假设，应认为对于未知的情况都是"等可能的"。
而在这种情况下，概率分布最均匀，预测的风险最小，因此得到的概率分布的熵是最大。

- 也就是说，最大熵原理通过熵的最大化来表示等可能性

#### 熵公式

- 假设离散随机变量$X$是的概率分布是$P(X)$，则其熵是：

    $$H(X) = -\sum_{\boldsymbol{x}} P(X=\boldsymbol{x})\log P(X=\boldsymbol{x})$$

    - $H(X)$依赖于X的分布,而与X的具体值无关。H(X)越大,表示X的不确定性越大

- 可证，熵$H(X)$满足：

    $$0 \le H(X) \le \log \vert X \vert$$

    - $\vert X \vert$表示离散随机变量$X$的可能取值个数
    - 最大熵原理指出概率分布越均匀，熵越大，即当且仅当X的分布服从均匀分布时上式右边的等号成立

- 最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型

## 最大熵模型定义

- 定义$\mathcal{X}$，$\mathcal{Y}$分别表示$R_n$中的输入空间和输出空间，$X$和$Y$分别是$\mathcal{X}$，$\mathcal{Y}$的随机变量。

- 假设分类模型是一个条件概率分布$P(Y\vert X)$(判别式的)，即对于给定的输入$X$，根据条件概率$P(Y\vert X)$输出$Y$。

#### 最大熵模型的特征与特征函数

- 通常"特征"往往仅指对输入抽取特征，用函数表示就是 $f(\boldsymbol{x})$；而在最大熵模型中，是对输入和输出同时抽取特征，用函数表示就是 $f(\boldsymbol{x},y)$

- 特征函数的作用可理解为相当于对特征进行特征转换、提取或向量化处理表达

- 如

    $$f(\boldsymbol{x},y) = \left \{ \begin{aligned} &1 \qquad &&当\boldsymbol{x},y满足某一事实，如 \boldsymbol{x}=\boldsymbol{x}_0,y=y_0 \\ &0 \qquad &&不满足该事实 \end{aligned} \right.$$

- 注意特征函数可以任意实值函数，且一个模型中可以定义多个特征函数

#### 最大熵模型的约束条件

##### 期望
- 对于给定的训练样本数据集(假设样本数为 $m$)：
    - 联合分布 $P(X=\boldsymbol{x},Y=y)$ 的经验分布 $\tilde{P}(X=\boldsymbol{x},Y=y)$:

        $$\tilde{P}(X=\boldsymbol{x},Y=y) = \frac {count(X=\boldsymbol{x},Y=y)}{m}$$

    - 边缘分布 $P(X=\boldsymbol{x})$ 的经验分布 $\tilde{P}(X=\boldsymbol{x})$:

        $$\tilde{P}(X=\boldsymbol{x}) = \frac {count(X=\boldsymbol{x})}{m}$$

- 则可得相应的期望：
    - 特征函数 $f(\boldsymbol{x},y)$ 关于经验分布 $\tilde{P}(X=\boldsymbol{x},Y=y)$ 的期望是：

        $$E_{\tilde{P}}(f) = \sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x},Y=y) \space f(\boldsymbol{x}, y)$$
    
    - 模型 $P(Y\vert X)$ 关于特征函数 $f(\boldsymbol{x},y)$ 的期望是：

        $$E_{P}(f) = \sum_{\boldsymbol{x},y} P(X=\boldsymbol{x},Y=y) \space f(\boldsymbol{x}, y) \approx \sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x}) \space P(Y=y\vert X=\boldsymbol{x}) \space f(\boldsymbol{x}, y)$$

##### 约束条件

- 最大熵模型的约束条件就是，对于所有的特征函数 $f_i, i=1,2,...n$(假定有$n$个特征函数，也就意味着有$n$个约束条件)：

    $$E_{\tilde{P}}(f_i) = E_{P}(f_i)$$

#### 条件熵

- 关于条件分布 $P(Y\vert X)$ 的条件熵为：

    $$\begin{aligned} H(P) &= H(Y \vert X) \\ &=  \sum_{i=1}^m P(X=\boldsymbol{x}_i) H(Y \vert X=\boldsymbol{x}_i) \\ &= -\sum_{i=1}^m P(X=\boldsymbol{x}_i) \cdot \sum_{j=1}^n P(Y=y_j \vert X=\boldsymbol{x}_i) \log P(Y=y_j \vert X=\boldsymbol{x}_i) \\ &= -\sum_{\boldsymbol{x},y} P(Y=y, X=\boldsymbol{x}) \log P(Y=y \vert X=\boldsymbol{x}) \\ &\approx  -\sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x}) P(Y=y \vert X=\boldsymbol{x})\log P(Y=y \vert X=\boldsymbol{x}) \end{aligned}$$

#### 最大熵模型的定义

- 设 $P^*$ 是最大熵模型的解，则最大熵模型定义为：
    - 第一，优先保证模型满足已知的所有约束，约束集合记为：$$C = \{ P \vert E_{\tilde{P}}(f_i) = E_{P}(f_i), \qquad i=1,2,...n \}$$
    - 第二，使条件熵 $H(P)$ 取最大

    $$P^* = arg \max_{P\in C} H(P) \space 或 \space P^*= arg \min_{P\in C} -H(P)$$

## 最大熵模型的求解

- 由上，可知最大熵模型的求解问题其实就是一个**等式约束优化问题**：

    $$\begin{aligned} &\min_{P\in C} &&-H(P) = \sum_{\boldsymbol{x},y} \tilde{P}(X=\boldsymbol{x}) P(Y=y \vert X=\boldsymbol{x})\log P(Y=y \vert X=\boldsymbol{x}) \\ &subject.to &&\sum_y P(Y=y\vert X=\boldsymbol{x}) = 1 \\ & &&E_{\tilde{P}}(f_i) - E_{P}(f_i) = 0, \space i=1,2,...n \end{aligned}$$

- 为表达方便，简写为：

    $$\begin{aligned} &\min_{P\in C} &&-H(P) = \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) P(y \vert \boldsymbol{x})\log P(y \vert \boldsymbol{x}) \\ &subject.to &&\sum_y P(y\vert \boldsymbol{x}) = 1 \\ & &&E_{\tilde{P}}(f_i) - E_{P}(f_i) = 0, \space i=1,2,...n  \end{aligned}$$

#### 拉格朗日乘子法

- 对于**等式约束优化问题**可引入拉格朗日乘子，构建拉格朗日函数：

    $$\begin{aligned} L(P, \boldsymbol{\omega}) &= -H(P) + \omega_0[1-\sum_y P(y\vert \boldsymbol{x})] + \sum_{i=1}^n \omega_i [E_{\tilde{P}}(f_i) - E_{P}(f_i)] \\ &= \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) P(y \vert \boldsymbol{x})\log P(y \vert \boldsymbol{x}) + \omega_0[1-\sum_y P(y\vert \boldsymbol{x})] + \sum_{i=1}^n \omega_i[\sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x},y) \space f_i(\boldsymbol{x}, y) - \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) \space P(y\vert \boldsymbol{x}) \space f_i(\boldsymbol{x}, y)] \end{aligned}$$

##### 对$P(y\vert \boldsymbol{x})$求偏导

$$\begin{aligned} \frac {\partial L(P, \boldsymbol{\omega})}{\partial P(y\vert \boldsymbol{x})} &= \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) [\log P(y \vert \boldsymbol{x})+1] - \sum_y \omega_0 - \sum_{i=1}^n \omega_i \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}) \space f_i(\boldsymbol{x}, y) \\ &= \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x})[1+\log P(y \vert \boldsymbol{x})-\omega_0-\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)]\end{aligned}$$

- 令偏导数为0，将对数视为自然对数，可得：

    $$P(y \vert \boldsymbol{x}) = e^{-1+\omega_0+\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} =\frac {1}{e^{1-\omega_0}}e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} $$

- 由已知约束条件$\sum_y P(y \vert \boldsymbol{x}) = 1$，可得：

    $$\sum_y P(y \vert \boldsymbol{x}) = \sum_y \frac {1}{e^{1-\omega_0}}e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} = 1$$

    $$\Longrightarrow e^{1-\omega_0} = \sum_y e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}$$

##### 最终得到的最大熵模型

- 令

    $$Z_\omega (\boldsymbol{x}) = e^{1-\omega_0} = \sum_y e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}$$

- 这里$$Z_\omega (\boldsymbol{x})$$称为规范化因子，起到了归一化的作用

- 最大熵模型的后验概率：

    $$P(y\vert \boldsymbol{x}) = \frac {1}{Z_\omega (\boldsymbol{x})} e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)}$$

#### 拉格朗日对偶法求解

##### 原问题与对偶问题最优解的等价性

- 因满足以下三条性质，根据强对偶理论，可知最大熵模型的拉格朗日函数的拉格朗日对偶性中：原问题的最优解和对偶问题的最优解之间无对偶间隙。即对偶问题的最优解等价于原问题的最优解。
    - 根据集合$C$的定义表述，可知$C$是一个多面体；
    - 由熵的凹性，可知 $L(P, \boldsymbol{\omega})$ 是关于 $P$的凸函数；
    - 同时加之，约束条件是线性的；

###### 原问题

- 转换为等价的极小极大问题(原问题)

    $$\min_{P\in C} \max_{\boldsymbol{\omega}} L(P, \boldsymbol{\omega})$$

###### 对偶问题

- 对偶函数：

    $$q(\boldsymbol{\omega}) = \min_{P\in C} L(P, \boldsymbol{\omega})$$

- 转换为对偶的极大极小问题(对偶问题)

    $$\max_{\boldsymbol{\omega}} \min_{P\in C} L(P, \boldsymbol{\omega})$$

##### 求解对偶问题

- 求解对偶问题内部的极小化问题得 $P^*$：

    $$\begin{aligned} P^* &= arg \min_{P\in C} L(P, \boldsymbol{\omega}) \\ &= \frac {1}{Z_\omega (\boldsymbol{x})} e^{\sum_{i=1}^n \omega_i \space f_i(\boldsymbol{x}, y)} \end{aligned}$$

- 求解对偶问题外部的极大化问题得 $\boldsymbol{\omega}^*$：

    $$\begin{aligned} \boldsymbol{\omega}^* &= arg \max_{\boldsymbol{\omega}} q(\boldsymbol{\omega}) \\ &= arg \max_{\boldsymbol{\omega}} \min_{P\in C} L(P, \boldsymbol{\omega}) \\ &= arg \max_{\boldsymbol{\omega}} L(P^*, \boldsymbol{\omega}) \end{aligned}$$

    - 代入$P^*$，化简后得：

    $$\boldsymbol{\omega}^* = arg \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x}, y} \tilde{P}(\boldsymbol{x}, y) \sum_{i=1}^n \omega_i f_i(\boldsymbol{x}, y)-\sum_{\boldsymbol{x}} \tilde{P}(\boldsymbol{x}) \log Z_\omega (\boldsymbol{x})$$

#### 极大似然估计法求解

- 似然函数

    $$L(\boldsymbol{\omega}) = \prod_{i=1}^m P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega}), \qquad m表示训练样本数$$

- 上面的似然函数，稍微调整下可以写为：

    $$L(\boldsymbol{\omega}) = \prod_{i=1}^k P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega})^{count(X=\boldsymbol{x}_i)}, \qquad k表示X的取值个数$$
    
    - 左右同时取m次方根

    $$\begin{aligned} L(\boldsymbol{\omega})^{\frac 1m} &= \prod_{i=1}^k P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega})^{\frac {count(X=\boldsymbol{x}_i)}{m}}, \qquad k表示X的取值个数 \\ &= \prod_{i=1}^k P(Y=y_i \vert X=\boldsymbol{x}_i; \boldsymbol{\omega})^{\tilde{P}(X=\boldsymbol{x}_i)} \\ &= \prod_{\boldsymbol{x},y} P(y \vert \boldsymbol{x}; \boldsymbol{\omega})^{\tilde{P}(\boldsymbol{x})} \end{aligned}$$
    
    - 显然对 $L(\boldsymbol{\omega})$ 极大化等价于对其 $m$次方根的极大化

- 则可得条件分布$P(y\vert \boldsymbol{x})$ 极大化对数似然估计函数为：

    $$\begin{aligned} \max_{\boldsymbol{\omega}} \log L(\boldsymbol{\omega}) &\equiv \max_{\boldsymbol{\omega}} \log L(\boldsymbol{\omega})^{\frac 1m} \\ &= \max_{\boldsymbol{\omega}} \log \prod_{\boldsymbol{x},y} P(y \vert \boldsymbol{x}; \boldsymbol{\omega})^{\tilde{P}(\boldsymbol{x})} \\ &= \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}, y) \log P(y\vert \boldsymbol{x}; \boldsymbol{\omega}) \end{aligned} $$

- 则极大化对数似然函数，可得：

    $$\begin{aligned} \boldsymbol{\omega}^* &= arg \max_{\boldsymbol{\omega}} \log L(\boldsymbol{\omega}) \\ &= arg \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}, y) \sum_{i=1}^n \omega_i f_i(\boldsymbol{x}, y) - \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x},y) \log Z_\omega (\boldsymbol{x}) \\ &= arg \max_{\boldsymbol{\omega}} \sum_{\boldsymbol{x},y} \tilde{P}(\boldsymbol{x}, y) \sum_{i=1}^n \omega_i f_i(\boldsymbol{x}, y) - \sum_{\boldsymbol{x}} \tilde{P}(\boldsymbol{x}) \log Z_\omega (\boldsymbol{x}) \qquad 根据\sum_y P(\boldsymbol{x},y)=1得出 \end{aligned}$$

## 最大熵模型的学习

- 根据前面的推导已经得到参数的表达式，因此接下来可以使用如梯度下降、牛顿法、拟牛顿法等方法进行求解
- 对于最大熵模型的学习，也有专门的最优化算法-改进的迭代尺度法(IIS)
