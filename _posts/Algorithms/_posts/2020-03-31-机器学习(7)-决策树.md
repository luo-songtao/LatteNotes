---
layout: post
title: 机器学习(七)-决策树
tags: [决策树]
category: ['algorithms']
date: 2020-03-31 00:00:07
toc: true
---

## 介绍

- 决策树是一种基本的分类与回归方法。决策树呈树形结构。决策树是一种判别模型，天然支持多分类问题
    - 分类树的映射函数是多维空间的分段线性划分，即用平行于各坐标轴的超平面对空间进行切分
    - 回归树的映射函数是分段常数函数

- 决策树是分段线性函数而不是线性函数，它具有非线性建模能力。理论上，只要划分得足够细，分段常熟函数可以逼近闭区间熵任意函数到任意指定精度。

#### 决策树与if-then规则

- 决策树到if-then规则：
    - 由决策树的根节点到叶节点的每一条路径构建一条规则
    - 路径上的特征对应着规则的条件，而叶节点的类对应着规则的结论

- 决策树的路径(if-then规则集合)性质：互斥并且完备
    - 每一个实例都能且只能被一条路径或一条规则所覆盖

#### 决策树与条件概率分布

- 决策树表示给定条件下类的条件概率分布。其定义在特征空间的一个划分上
    - 将特征空间划分为互不相交的单元或区域，在每个单元定义一个类的概率分布就构成了一个条件概率分布
    - 决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成
    - 各叶节点(单元)上条件概率往往偏向某一个类

#### 决策树的学习

- 决策树学习本质上是从训练数据集中归纳出一组分类规则。
    - 能训练出的与数据集不相矛盾的决策树可能有多个。因此最终需要的是一个与训练数据矛盾最小的决策树，同时具有很好的泛化能力
- 或者说决策树学习是由训练数据集估计条件概率模型
    - 基于特征空间划分的类的条件概率模型有无穷多个，最终得到的模型不仅对训练数据有很好的拟合，还需要对未知数据有很好的预测

- 决策树学习过程：
    - 特征选择
        - 决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程
    - 决策树生成
    - 决策树修剪(剪枝)

## 特征选择

- 特征选择：决定用哪个特征来划分特征空间
    - 选取对训练数据具有分类能力的特征
    - 如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称该特征没有分类能力
    - 划分时，希望决策树的分支节点所包含的样本尽可能属于同一类别，即结点的"纯度"越来越高

#### 信息增益(Information Gain)

- 设$X$是一个去有限个值的离散随机变量，其概率分布为：

    $$P(X=x_i) = p_i, \qquad i=1,2,...,n$$

- 熵$H(X)$(表示随机变量不确定性的度量)：

    $$H(X) = - \sum_{i=1}^n p_i\log p_i \qquad n表示X可能的取值个数$$

    - 熵的单位：
        - bit: 当对数以2为底
        - nat: 当对数以自然对数为底

- 信息熵(熵本是物理中的概念，但由香农引入到信息论中后，熵也被称为信息熵)
    - 对于一个训练样本集合$D$而言，信息熵是度量样本集合纯度的一种常用指标
    - 假定当前样本集合$D$中第$i$类样本(共有$k$个类别)所占的比例为$p_i$，则$D$的信息熵为：

        $$H(D) = - \sum_{i=1}^k p_i\log p_i \qquad k表示样本集合中类别总数$$
    
    - 信息熵越小，表示$D$的纯度越高

- 条件熵$H(Y\vert X)$(表示已知随机变量$X$的条件下随机变量$Y$的不确定性)：

    $$\begin{aligned} H(P) &= H(Y \vert X) \\ &=  \sum_{i=1}^n P(X=x_i) H(Y \vert X=x_i) \\ &= - \sum_{i=1}^n P(X=x_i) \cdot \sum_{j=1}^k P(Y=y_j \vert X=x_i) \log P(Y=y_j \vert X=x_i) \qquad n、k分别表示X、Y的取值个数 \\ &= -\sum_{x,y} P(Y=y, X=x) \log P(Y=y \vert X=x) \end{aligned}$$

- 信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度
    - 假设特征$X$有$n$个可能的取值$(x_1, x_2,...,x_n)$，训练数据集$D$中共有$k$类样本$(y_1,y_2,...,y_k)$，则特征$X$对数据集$D$进行划分所获得的信息增益$gain(D,X)$为：

        $$\begin{aligned} gain(D,X) &= H(D) - H(D\vert X) \\ &= - \sum_{j=1}^k \tilde{P}(Y=y_j)\log \tilde{P}(Y=y_j) + \sum_{i=1}^n \tilde{P}(X=x_i) \cdot \sum_{j=1}^k \tilde{P}(Y=y_j \vert X=x_i) \log \tilde{P}(Y=y_j \vert X=x_i) \end{aligned}$$

    - 式中$\tilde{P}$表示根据频数计算而得的概率(经验分布)

- 利用信息增益准则的特征选择方法是：对训练数据集$D$，计算其每个特征的信息增益，选择信息增益最大的特征

#### 信息增益率

- 以最大信息增益进行选取的特征，倾向于选择特征可取值数较多的特征，如果需要消除这样的影响，那么可以使用最大信息增益率准则来进行选取

- 信息增益率$gain_{ratio}(D,X)$:

    $$gain_{ratio}(D,X) = \frac {gain(D,X)}{H_X(D)}$$
    
    - $H_X(D)$表示数据集$D$关于特征$X$的取值的熵(经验熵)

        $$H_X(D) = -\sum_{i=1}^n \tilde{P}(X=x_i) \log \tilde{P}(X=x_i) \qquad n表示特征X的可能取值个数$$

- **但要注意，信息增益率准则虽然减少了对特征可取值数较多的特征的影响，但同时它对特征可取值数较少的属性会有所偏好**
    - 一种比较折中的方案是：**先从候选划分特征中找出信息增益高于平均水平的特征，再从中选择信息增益率最高的特征**

#### 基尼指数(Gini Index)

- 数据集$D$的纯度也可以用**基尼值**来度量：

    $$\begin{aligned} Gini(D) &= \sum_{i=1}^k \sum_{i' \neq i} P(Y=y_i)P(Y=y_{i'}) \\ &= 1- \sum_{i=1}^k P(Y=y_i)^2 \end{aligned}$$

    - 基尼值反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率
    - 基尼值越小，数据集的纯度越高

- 特征$X$的**基尼指数**：

    $$Gini\_Index(D,X) = \sum_{i=1}^n \tilde{P}(X=x_i) \cdot Gini(D_X)$$

    - 其中$$D_X = \{(x_i, y_i) \in D \vert X = x_i \}$$
    - 基尼指数指经$X=x_i$分割后集合$D$的纯度

## 决策树的生成

#### ID3算法

- ID3算法是在决策树各节点上应用信息增益准则选择特征，递归地构建决策树
    - 从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点
    - 在对子节点递归地用以上方法，构建决策树
    - 直到所有特征的信息增益均很小或没有特征可以选择为止

- ID3算法相当于用极大似然法进行概率选择
- 注意：ID3算法只有树的生成，所以该算法生成的树容易产生过拟合

#### C4.5算法

- C4.5算法与ID3算法相似，但主要是使用信息增益率来选择特征

#### CART算法

- 分类与回归树(CART)，分为分类树和回归树，分别用于分类问题和回归问题

- 分类与回归树是一颗二叉决策树

- 分类与回归树生成需要，第一，考虑使用特征向量的哪个分量进行判定；第二，确定分裂分量后，如何划分训练样本，让一部分进入左子树，一部分进入右子树
    - 连续数值型变量：寻找一个分裂阈值
        - 均方误差
    - 离散型类别变量：确定一个子集划分
        - 基尼指数

##### 分类树

- 分类树使用基尼指数，其目标是把数据分成两部分后，两个子集都尽可能的纯，也就是左右子树集合的基尼指数和最小的，即：

    $$\min \frac {count(D_l)}{count(D)}Gini(D_l) + \frac {count(D_r)}{count(D)}Gini(D_r)$$

##### 回归树

- 回归树使用样本均方误差作为评判标准，其目标是把数据分成两部分后，两个子集的均方误差和最小，即

    $$\min \frac {count(D_l)}{count(D)}E(D_l) + \frac {count(D_r)}{count(D)}E(D_r)$$

    $$E(D) = \frac {1}{count(D)} \sum_{i=1}^{count(D)} (x_i - \overline{x})$$

## 决策树的剪枝

- 为了调整决策树的过拟合问题，需要对决策树进行剪枝处理
    - 预剪枝
    - 后剪枝

#### 预剪枝

- 在决策树生成过程中，对分别前和分裂后的就决策树整体损失(代价)进行计算，如果分裂后的损失更大，那么就停止分裂，反之才继续分裂

#### 后剪枝

- 在决策树完全生成后，使用自下而上的顺序考虑逐个剪掉内部节点，同时在剪枝前后，分别对决策树进行整体损失计算，如果剪枝后损失更小，那么就执行剪枝，否则，不执行

- 注意，在CART中，后剪枝是使用自下而上的方式剪枝，并生成一个子树序列，利用交叉验证方式，测试子树序列的各颗子树的均方误差或基尼指数，选取相应值最小的决策树子树评为最优的决策树
