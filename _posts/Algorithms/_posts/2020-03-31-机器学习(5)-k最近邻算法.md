---
layout: post
title: 机器学习(五)-k最近邻算法
tags: [knn]
category: ['algorithms']
date: 2020-03-31 00:00:05
toc: true
---


## KNN

k近邻算法(k-nearest neighbor, KNN)是一种基本分类与回归方法。

#### KNN算法的工作机制

- 给定测试样本，基于某种距离度量(如欧氏距离)找出测试集中与其最靠近的k个训练样本，然后基于这k个近邻样本的信息进行预测。
    - 给定的测试样本，其中的实例类别已定(分类任务)，或实例的真是标记值已定(回归任务)
    - 在分类任务中，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测
        对于分类问题，给定$m$个训练样本$(\boldsymbol{x}_i, y_i)$，设定参数$k$，假设类型数为$c$，待分类样本的特征向量是$\boldsymbol{x}$，那么预测的流程：
        - 在训练样本中找出离$\boldsymbol{x}$最近的$k$个样本，假设这些样本的集合为$N$
        - 统计集合$N$中每一类的个数$C_i, i=1,2,···,c$
        - 最终分类结果为$\max_i C_i$
    - 在回归任务中，可使用"平均法"，即将这k个样本的实值输出，标记的平均值作为预测结果

- 可见，KNN算法不具有显示的学习过程。
    - KNN实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”
    - KNN是“lazy learning”的著名代表，此类学习技术在训练阶段仅仅是吧样本保存起来，训练时间开销为0，等需要进行预测时再进行处理。与之相对的是“eager learning”

- KNN算法实现简单，但缺点是训练样本数大、特征向量维数很高时，计算复杂度高。
    - 每次预测时要计算待预测样本和每一个训练样本的距离。而且要对距离进行排序找到最近的k个样本。
        - 可以使用高效的部分排序算法，只找出最小的k个数
        - 或是利用kd数实现快速的近邻样本查找

- 变种：
    - 带权重的KNN
    - 模糊KNN

## KNN算法的基本要素

- k值的选择
- 距离度量
- 分类/回归决策规则

#### K值的选择
- k值的选择会对KNN算法的结果产生重大影响。它需要根据问题和数据的特点来确定，并注意：
    - 如果选择较小k值，由于只有与输入实例较近的训练实例才会对预测结果作用，估计误差会增大，因此将对这些训练实例非常敏感，从而容易造成过拟合
    - 如果选择较大的k值，估计误差虽然可以减少，但近似误差，即与输入实例较远的训练实例也会对预测起作用，会使预测发生错误。
    - k值的减小意味着整体模型变得复杂， k值的增大意味着整体模型变得简单

    实际中，k值取值一般比较小，并通常采用交叉验证法来选取最优的k值

##### 距离度量

- KNN算法模型的特征空间一般是n维实数向量空间$R^n$

##### 欧式距离

$$d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{\sum_{i=1}^n(u_i-v_i)^2}$$

##### 曼哈顿距离

$$d(\boldsymbol{u}, \boldsymbol{v}) = \sum_{i=1}^n \vert u_i-v_i \vert$$

##### Mahalanobis距离

- Mahalanobis距离是一种概率意义上的距离。它度量两个随机向量的相似度。

    $$d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{(\boldsymbol{u}-\boldsymbol{v})^TS(\boldsymbol{u}-\boldsymbol{v})}$$

    - 矩阵$S$是正定的(否则没有意义)
    - 当$S$是单位矩阵时，Mahalanobis距离退化为欧氏距离

    $S$矩阵可以通过计算训练样本集的协方差矩阵得到，也可以通过训练样本学习得到

- KNN算法的精度在很大程度上依赖于所使用的距离度量标准，“距离度量学习”是一种从带标签的样本集中学习，得到距离度量矩阵的方法

##### Bhattacharyya距离

- Bhattacharyya距离定义了离散型或连续型概率分布的相似性

    - 对于离散型随机变量的分布，定义为：

        $$d(\boldsymbol{u}, \boldsymbol{v}) =  -\ln (\sum_{i=1}^n\sqrt{u_i\cdot y_i})$$

        其中随机向量的分量值必须非负

#### 分类/回归决策规则

- KNN中分类决策规则往往是多数表决
- KNN中回归决策规则往往是“平均值”

## kd树

- KNN的实现，主要考虑的问题是如何对训练数据进行快速的k近邻搜索，尤其是在特征空间的维数大及训练数据容量大时。KNN最简单的实现方法是线性扫描，但显而易见，性能很低。

- kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构
    - kd树是一颗二叉树，表示对k维空间的一个划分
    - 构造kd树相当于不断对垂直于坐标轴超平面将k维空间划分，构成一系列的k维超矩形区域，kd树的每个节点对应于一个k维超矩形区域

#### kd树构造

- 通常依次选择坐标轴对空间划分
    - 选择训练实例点在所选定坐标轴上的**中位数**坐标作为切分点，进行二分操作，并将切分点所属的实例点保存在当前的子树的根节点，然后递归处理左右两部分子区域，
    直到两个子区域没有实例存在时停止，最终构建出kd树

通过中位数构建的kd树是平衡的

#### kd树搜索

- 首先从上往下，根据目标点依次在坐标轴上进行判断，直到叶节点
- 然后将此叶节点作为“当前最近点”，随即递归向上回退，并坐如下处理
    - 先判断当前所在节点比“当前最近点”距离更近，则更新当前所在节点为“当前最近点”
    - 搜索当前节点的另一子节点，在其中查找是否有更近的点，若有，则更新其为新的“当前最近点”
    - 直到回退到根节点，搜索结束

- 如果实例点是随机分布的，kd树搜索的平均计算复杂度是$O(\log N)$，$N$为训练样本数。

- kd树更适合于训练样本数远大于空间维数的k近邻搜索。当空间维数接近训练样本数时，它的效率会迅速下降，几乎接近线性扫描

